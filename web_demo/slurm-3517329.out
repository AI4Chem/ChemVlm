2024-05-29 20:39:19 | INFO | model_worker | args: Namespace(host='0.0.0.0', port=10014, worker_address='http://10.140.24.69:10014', controller_address='http://10.140.24.69:10010', model_path='/mnt/hwfile/ai4chem/share/multimodal-exam/internvl_chemllm-llm_lora', model_base=None, model_name='test', device='auto', multi_modal=False, limit_model_concurrency=5, stream_interval=1, no_register=False, load_8bit=False, load_4bit=False)
2024-05-29 20:39:19 | INFO | model_worker | Loading the model test on worker c3d11a ...
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-05-29 20:39:19 | INFO | internvl.model.internvl_chat.configuration_internvl_chat | vision_select_layer: -1
2024-05-29 20:39:19 | INFO | internvl.model.internvl_chat.configuration_internvl_chat | ps_version: v1
2024-05-29 20:39:19 | INFO | internvl.model.internvl_chat.configuration_internvl_chat | min_dynamic_patch: 1
2024-05-29 20:39:19 | INFO | internvl.model.internvl_chat.configuration_internvl_chat | max_dynamic_patch: 6
2024-05-29 20:39:19 | INFO | internvl.model.internvl_chat.modeling_internvl_chat | num_image_token: 256
2024-05-29 20:39:19 | INFO | internvl.model.internvl_chat.modeling_internvl_chat | ps_version: v1
2024-05-29 20:39:22 | INFO | stdout | trainable params: 72,351,744 || all params: 19,933,612,032 || trainable%: 0.3630
2024-05-29 20:39:25 | INFO | accelerate.utils.modeling | We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2024-05-29 20:39:29 | ERROR | stderr | Loading checkpoint shards:   0%|          | 0/11 [00:00<?, ?it/s]
2024-05-29 20:39:31 | ERROR | stderr | Loading checkpoint shards:   9%|▉         | 1/11 [00:02<00:23,  2.38s/it]
2024-05-29 20:39:33 | ERROR | stderr | Loading checkpoint shards:  18%|█▊        | 2/11 [00:04<00:20,  2.31s/it]
2024-05-29 20:39:36 | ERROR | stderr | Loading checkpoint shards:  27%|██▋       | 3/11 [00:06<00:18,  2.26s/it]
2024-05-29 20:39:38 | ERROR | stderr | Loading checkpoint shards:  36%|███▋      | 4/11 [00:09<00:15,  2.22s/it]
2024-05-29 20:39:40 | ERROR | stderr | Loading checkpoint shards:  45%|████▌     | 5/11 [00:11<00:13,  2.20s/it]
2024-05-29 20:39:42 | ERROR | stderr | Loading checkpoint shards:  55%|█████▍    | 6/11 [00:13<00:10,  2.17s/it]
2024-05-29 20:39:44 | ERROR | stderr | Loading checkpoint shards:  64%|██████▎   | 7/11 [00:15<00:08,  2.16s/it]
2024-05-29 20:39:46 | ERROR | stderr | Loading checkpoint shards:  73%|███████▎  | 8/11 [00:17<00:06,  2.13s/it]
2024-05-29 20:39:48 | ERROR | stderr | Loading checkpoint shards:  82%|████████▏ | 9/11 [00:19<00:04,  2.14s/it]
2024-05-29 20:39:51 | ERROR | stderr | Loading checkpoint shards:  91%|█████████ | 10/11 [00:21<00:02,  2.15s/it]
2024-05-29 20:39:52 | ERROR | stderr | Loading checkpoint shards: 100%|██████████| 11/11 [00:22<00:00,  1.84s/it]
2024-05-29 20:39:52 | ERROR | stderr | Loading checkpoint shards: 100%|██████████| 11/11 [00:22<00:00,  2.09s/it]
2024-05-29 20:39:52 | ERROR | stderr | 
2024-05-29 20:39:52 | INFO | model_worker | Register to controller
2024-05-29 20:39:52 | ERROR | stderr | INFO:     Started server process [259471]
2024-05-29 20:39:52 | ERROR | stderr | INFO:     Waiting for application startup.
2024-05-29 20:39:52 | ERROR | stderr | INFO:     Application startup complete.
2024-05-29 20:39:52 | ERROR | stderr | INFO:     Uvicorn running on http://0.0.0.0:10014 (Press CTRL+C to quit)
2024-05-29 20:39:57 | INFO | stdout | INFO:     10.140.24.69:47104 - "POST /worker_get_status HTTP/1.1" 200 OK
2024-05-29 20:40:07 | INFO | model_worker | Send heart beat. Models: ['test']. Semaphore: None. global_counter: 0
2024-05-29 20:40:08 | INFO | model_worker | Send heart beat. Models: ['test']. Semaphore: Semaphore(value=4, locked=False). global_counter: 1
2024-05-29 20:40:08 | INFO | stdout | INFO:     10.140.24.69:47118 - "POST /worker_generate_stream HTTP/1.1" 200 OK
2024-05-29 20:40:08 | INFO | model_worker | max_input_tiles: 12
2024-05-29 20:40:08 | INFO | model_worker | dynamic_image_size: False
2024-05-29 20:40:08 | INFO | model_worker | use_thumbnail: False
2024-05-29 20:40:08 | INFO | model_worker | Resize images to 448x448
2024-05-29 20:40:08 | INFO | model_worker | Split images to torch.Size([1, 3, 448, 448])
2024-05-29 20:40:08 | INFO | model_worker | A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: <img><image></img>
请描述这张图片 ASSISTANT:
2024-05-29 20:40:08 | INFO | model_worker | num_image_tokens: 256
2024-05-29 20:40:08 | INFO | model_worker | max_new_tokens: 1024
2024-05-29 20:40:14 | ERROR | stderr | /mnt/petrelfs/zhangdi1/lijunxian/InternVL/internvl_chat/internvl/model/internvl_chat/modeling_internvl_chat.py:198: UserWarning: In ps_version 'v1', the height and width have not been swapped back, which results in a transposed image.
2024-05-29 20:40:14 | ERROR | stderr |   warnings.warn("In ps_version 'v1', the height and width have not been swapped back, "
2024-05-29 20:40:22 | INFO | model_worker | Send heart beat. Models: ['test']. Semaphore: Semaphore(value=4, locked=False). global_counter: 1
2024-05-29 20:40:37 | INFO | model_worker | Send heart beat. Models: ['test']. Semaphore: Semaphore(value=4, locked=False). global_counter: 1
2024-05-29 20:40:52 | INFO | model_worker | Send heart beat. Models: ['test']. Semaphore: Semaphore(value=4, locked=False). global_counter: 1
2024-05-29 20:41:02 | INFO | model_worker | Send heart beat. Models: ['test']. Semaphore: Semaphore(value=5, locked=False). global_counter: 1
2024-05-29 20:41:02 | INFO | stdout | INFO:     10.140.24.69:47150 - "POST /worker_get_status HTTP/1.1" 200 OK
2024-05-29 20:41:07 | INFO | model_worker | Send heart beat. Models: ['test']. Semaphore: Semaphore(value=5, locked=False). global_counter: 1
2024-05-29 20:41:10 | INFO | model_worker | Send heart beat. Models: ['test']. Semaphore: Semaphore(value=4, locked=False). global_counter: 2
2024-05-29 20:41:10 | INFO | stdout | INFO:     10.140.24.69:47164 - "POST /worker_generate_stream HTTP/1.1" 200 OK
2024-05-29 20:41:10 | INFO | model_worker | max_input_tiles: 12
2024-05-29 20:41:10 | INFO | model_worker | dynamic_image_size: False
2024-05-29 20:41:10 | INFO | model_worker | use_thumbnail: False
2024-05-29 20:41:10 | INFO | model_worker | Resize images to 448x448
2024-05-29 20:41:10 | INFO | model_worker | Split images to torch.Size([1, 3, 448, 448])
2024-05-29 20:41:10 | INFO | model_worker | A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: <img><image></img>
请描述这张图片 ASSISTANT:
2024-05-29 20:41:10 | INFO | model_worker | num_image_tokens: 256
2024-05-29 20:41:10 | INFO | model_worker | max_new_tokens: 1024
2024-05-29 20:41:22 | INFO | model_worker | Send heart beat. Models: ['test']. Semaphore: Semaphore(value=4, locked=False). global_counter: 2
2024-05-29 20:41:37 | INFO | model_worker | Send heart beat. Models: ['test']. Semaphore: Semaphore(value=4, locked=False). global_counter: 2
2024-05-29 20:41:52 | INFO | model_worker | Send heart beat. Models: ['test']. Semaphore: Semaphore(value=4, locked=False). global_counter: 2
2024-05-29 20:42:08 | INFO | model_worker | Send heart beat. Models: ['test']. Semaphore: Semaphore(value=4, locked=False). global_counter: 2
2024-05-29 20:42:23 | INFO | model_worker | Send heart beat. Models: ['test']. Semaphore: Semaphore(value=4, locked=False). global_counter: 2
2024-05-29 20:42:38 | INFO | model_worker | Send heart beat. Models: ['test']. Semaphore: Semaphore(value=4, locked=False). global_counter: 2
2024-05-29 20:42:53 | INFO | model_worker | Send heart beat. Models: ['test']. Semaphore: Semaphore(value=4, locked=False). global_counter: 2
2024-05-29 20:43:08 | INFO | model_worker | Send heart beat. Models: ['test']. Semaphore: Semaphore(value=4, locked=False). global_counter: 2
slurmstepd: error: *** JOB 3517329 ON SH-IDC1-10-140-24-69 CANCELLED AT 2024-05-29T20:43:11 ***
