2024-06-24 17:23:52 | INFO | model_worker | args: Namespace(host='0.0.0.0', port=10061, worker_address='http://10.140.24.69:10061', controller_address='http://10.140.24.69:10050', model_path='/mnt/hwfile/ai4chem/CKPT/chemvl_ft_6_19_0_merged', model_base=None, model_name='chemvlm', device='auto', multi_modal=False, limit_model_concurrency=5, stream_interval=1, no_register=False, load_8bit=False, load_4bit=False)
2024-06-24 17:23:52 | INFO | model_worker | Loading the model chemvlm on worker c74374 ...
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2024-06-24 17:23:52 | INFO | internvl.model.internvl_chat.configuration_internvl_chat | vision_select_layer: -1
2024-06-24 17:23:52 | INFO | internvl.model.internvl_chat.configuration_internvl_chat | ps_version: v2
2024-06-24 17:23:52 | INFO | internvl.model.internvl_chat.configuration_internvl_chat | min_dynamic_patch: 1
2024-06-24 17:23:52 | INFO | internvl.model.internvl_chat.configuration_internvl_chat | max_dynamic_patch: 6
2024-06-24 17:23:52 | INFO | internvl.model.internvl_chat.modeling_internvl_chat | num_image_token: 256
2024-06-24 17:23:52 | INFO | internvl.model.internvl_chat.modeling_internvl_chat | ps_version: v2
2024-06-24 17:23:54 | INFO | stdout | trainable params: 73,728,000 || all params: 5,610,224,000 || trainable%: 1.3142
2024-06-24 17:23:54 | INFO | stdout | trainable params: 72,351,744 || all params: 19,933,612,032 || trainable%: 0.3630
2024-06-24 17:24:06 | ERROR | stderr | Loading checkpoint shards:   0%|          | 0/22 [00:00<?, ?it/s]
2024-06-24 17:24:06 | ERROR | stderr | Loading checkpoint shards:   5%|▍         | 1/22 [00:00<00:03,  6.43it/s]
2024-06-24 17:24:06 | ERROR | stderr | Loading checkpoint shards:   9%|▉         | 2/22 [00:00<00:02,  6.67it/s]
2024-06-24 17:24:06 | ERROR | stderr | Loading checkpoint shards:  14%|█▎        | 3/22 [00:00<00:02,  6.75it/s]
2024-06-24 17:24:06 | ERROR | stderr | Loading checkpoint shards:  18%|█▊        | 4/22 [00:00<00:02,  6.81it/s]
2024-06-24 17:24:06 | ERROR | stderr | Loading checkpoint shards:  23%|██▎       | 5/22 [00:00<00:02,  6.86it/s]
2024-06-24 17:24:07 | ERROR | stderr | Loading checkpoint shards:  27%|██▋       | 6/22 [00:00<00:02,  6.99it/s]
2024-06-24 17:24:07 | ERROR | stderr | Loading checkpoint shards:  32%|███▏      | 7/22 [00:01<00:02,  7.08it/s]
2024-06-24 17:24:07 | ERROR | stderr | Loading checkpoint shards:  36%|███▋      | 8/22 [00:01<00:01,  7.12it/s]
2024-06-24 17:24:07 | ERROR | stderr | Loading checkpoint shards:  41%|████      | 9/22 [00:01<00:01,  7.16it/s]
2024-06-24 17:24:07 | ERROR | stderr | Loading checkpoint shards:  45%|████▌     | 10/22 [00:01<00:01,  7.29it/s]
2024-06-24 17:24:07 | ERROR | stderr | Loading checkpoint shards:  50%|█████     | 11/22 [00:01<00:01,  7.42it/s]
2024-06-24 17:24:07 | ERROR | stderr | Loading checkpoint shards:  55%|█████▍    | 12/22 [00:01<00:01,  7.51it/s]
2024-06-24 17:24:07 | ERROR | stderr | Loading checkpoint shards:  59%|█████▉    | 13/22 [00:01<00:01,  7.56it/s]
2024-06-24 17:24:08 | ERROR | stderr | Loading checkpoint shards:  64%|██████▎   | 14/22 [00:01<00:01,  7.54it/s]
2024-06-24 17:24:08 | ERROR | stderr | Loading checkpoint shards:  68%|██████▊   | 15/22 [00:02<00:00,  7.60it/s]
2024-06-24 17:24:08 | ERROR | stderr | Loading checkpoint shards:  73%|███████▎  | 16/22 [00:02<00:00,  7.62it/s]
2024-06-24 17:24:08 | ERROR | stderr | Loading checkpoint shards:  77%|███████▋  | 17/22 [00:02<00:00,  7.66it/s]
2024-06-24 17:24:08 | ERROR | stderr | Loading checkpoint shards:  82%|████████▏ | 18/22 [00:02<00:00,  7.66it/s]
2024-06-24 17:24:08 | ERROR | stderr | Loading checkpoint shards:  86%|████████▋ | 19/22 [00:02<00:00,  7.70it/s]
2024-06-24 17:24:08 | ERROR | stderr | Loading checkpoint shards:  91%|█████████ | 20/22 [00:02<00:00,  7.71it/s]
2024-06-24 17:24:08 | ERROR | stderr | Loading checkpoint shards:  95%|█████████▌| 21/22 [00:02<00:00,  7.72it/s]
2024-06-24 17:24:09 | ERROR | stderr | Loading checkpoint shards: 100%|██████████| 22/22 [00:03<00:00,  5.72it/s]
2024-06-24 17:24:09 | ERROR | stderr | Loading checkpoint shards: 100%|██████████| 22/22 [00:03<00:00,  7.02it/s]
2024-06-24 17:24:09 | ERROR | stderr | 
Some weights of the model checkpoint at /mnt/hwfile/ai4chem/CKPT/chemvl_ft_6_19_0_merged were not used when initializing InternVLChatModel: ['language_model.model.layers.0.attention.wo.weight', 'language_model.model.layers.0.attention.wqkv.weight', 'language_model.model.layers.0.attention_norm.weight', 'language_model.model.layers.0.feed_forward.w1.weight', 'language_model.model.layers.0.feed_forward.w2.weight', 'language_model.model.layers.0.feed_forward.w3.weight', 'language_model.model.layers.0.ffn_norm.weight', 'language_model.model.layers.1.attention.wo.weight', 'language_model.model.layers.1.attention.wqkv.weight', 'language_model.model.layers.1.attention_norm.weight', 'language_model.model.layers.1.feed_forward.w1.weight', 'language_model.model.layers.1.feed_forward.w2.weight', 'language_model.model.layers.1.feed_forward.w3.weight', 'language_model.model.layers.1.ffn_norm.weight', 'language_model.model.layers.10.attention.wo.weight', 'language_model.model.layers.10.attention.wqkv.weight', 'language_model.model.layers.10.attention_norm.weight', 'language_model.model.layers.10.feed_forward.w1.weight', 'language_model.model.layers.10.feed_forward.w2.weight', 'language_model.model.layers.10.feed_forward.w3.weight', 'language_model.model.layers.10.ffn_norm.weight', 'language_model.model.layers.11.attention.wo.weight', 'language_model.model.layers.11.attention.wqkv.weight', 'language_model.model.layers.11.attention_norm.weight', 'language_model.model.layers.11.feed_forward.w1.weight', 'language_model.model.layers.11.feed_forward.w2.weight', 'language_model.model.layers.11.feed_forward.w3.weight', 'language_model.model.layers.11.ffn_norm.weight', 'language_model.model.layers.12.attention.wo.weight', 'language_model.model.layers.12.attention.wqkv.weight', 'language_model.model.layers.12.attention_norm.weight', 'language_model.model.layers.12.feed_forward.w1.weight', 'language_model.model.layers.12.feed_forward.w2.weight', 'language_model.model.layers.12.feed_forward.w3.weight', 'language_model.model.layers.12.ffn_norm.weight', 'language_model.model.layers.13.attention.wo.weight', 'language_model.model.layers.13.attention.wqkv.weight', 'language_model.model.layers.13.attention_norm.weight', 'language_model.model.layers.13.feed_forward.w1.weight', 'language_model.model.layers.13.feed_forward.w2.weight', 'language_model.model.layers.13.feed_forward.w3.weight', 'language_model.model.layers.13.ffn_norm.weight', 'language_model.model.layers.14.attention.wo.weight', 'language_model.model.layers.14.attention.wqkv.weight', 'language_model.model.layers.14.attention_norm.weight', 'language_model.model.layers.14.feed_forward.w1.weight', 'language_model.model.layers.14.feed_forward.w2.weight', 'language_model.model.layers.14.feed_forward.w3.weight', 'language_model.model.layers.14.ffn_norm.weight', 'language_model.model.layers.15.attention.wo.weight', 'language_model.model.layers.15.attention.wqkv.weight', 'language_model.model.layers.15.attention_norm.weight', 'language_model.model.layers.15.feed_forward.w1.weight', 'language_model.model.layers.15.feed_forward.w2.weight', 'language_model.model.layers.15.feed_forward.w3.weight', 'language_model.model.layers.15.ffn_norm.weight', 'language_model.model.layers.16.attention.wo.weight', 'language_model.model.layers.16.attention.wqkv.weight', 'language_model.model.layers.16.attention_norm.weight', 'language_model.model.layers.16.feed_forward.w1.weight', 'language_model.model.layers.16.feed_forward.w2.weight', 'language_model.model.layers.16.feed_forward.w3.weight', 'language_model.model.layers.16.ffn_norm.weight', 'language_model.model.layers.17.attention.wo.weight', 'language_model.model.layers.17.attention.wqkv.weight', 'language_model.model.layers.17.attention_norm.weight', 'language_model.model.layers.17.feed_forward.w1.weight', 'language_model.model.layers.17.feed_forward.w2.weight', 'language_model.model.layers.17.feed_forward.w3.weight', 'language_model.model.layers.17.ffn_norm.weight', 'language_model.model.layers.18.attention.wo.weight', 'language_model.model.layers.18.attention.wqkv.weight', 'language_model.model.layers.18.attention_norm.weight', 'language_model.model.layers.18.feed_forward.w1.weight', 'language_model.model.layers.18.feed_forward.w2.weight', 'language_model.model.layers.18.feed_forward.w3.weight', 'language_model.model.layers.18.ffn_norm.weight', 'language_model.model.layers.19.attention.wo.weight', 'language_model.model.layers.19.attention.wqkv.weight', 'language_model.model.layers.19.attention_norm.weight', 'language_model.model.layers.19.feed_forward.w1.weight', 'language_model.model.layers.19.feed_forward.w2.weight', 'language_model.model.layers.19.feed_forward.w3.weight', 'language_model.model.layers.19.ffn_norm.weight', 'language_model.model.layers.2.attention.wo.weight', 'language_model.model.layers.2.attention.wqkv.weight', 'language_model.model.layers.2.attention_norm.weight', 'language_model.model.layers.2.feed_forward.w1.weight', 'language_model.model.layers.2.feed_forward.w2.weight', 'language_model.model.layers.2.feed_forward.w3.weight', 'language_model.model.layers.2.ffn_norm.weight', 'language_model.model.layers.20.attention.wo.weight', 'language_model.model.layers.20.attention.wqkv.weight', 'language_model.model.layers.20.attention_norm.weight', 'language_model.model.layers.20.feed_forward.w1.weight', 'language_model.model.layers.20.feed_forward.w2.weight', 'language_model.model.layers.20.feed_forward.w3.weight', 'language_model.model.layers.20.ffn_norm.weight', 'language_model.model.layers.21.attention.wo.weight', 'language_model.model.layers.21.attention.wqkv.weight', 'language_model.model.layers.21.attention_norm.weight', 'language_model.model.layers.21.feed_forward.w1.weight', 'language_model.model.layers.21.feed_forward.w2.weight', 'language_model.model.layers.21.feed_forward.w3.weight', 'language_model.model.layers.21.ffn_norm.weight', 'language_model.model.layers.22.attention.wo.weight', 'language_model.model.layers.22.attention.wqkv.weight', 'language_model.model.layers.22.attention_norm.weight', 'language_model.model.layers.22.feed_forward.w1.weight', 'language_model.model.layers.22.feed_forward.w2.weight', 'language_model.model.layers.22.feed_forward.w3.weight', 'language_model.model.layers.22.ffn_norm.weight', 'language_model.model.layers.23.attention.wo.weight', 'language_model.model.layers.23.attention.wqkv.weight', 'language_model.model.layers.23.attention_norm.weight', 'language_model.model.layers.23.feed_forward.w1.weight', 'language_model.model.layers.23.feed_forward.w2.weight', 'language_model.model.layers.23.feed_forward.w3.weight', 'language_model.model.layers.23.ffn_norm.weight', 'language_model.model.layers.24.attention.wo.weight', 'language_model.model.layers.24.attention.wqkv.weight', 'language_model.model.layers.24.attention_norm.weight', 'language_model.model.layers.24.feed_forward.w1.weight', 'language_model.model.layers.24.feed_forward.w2.weight', 'language_model.model.layers.24.feed_forward.w3.weight', 'language_model.model.layers.24.ffn_norm.weight', 'language_model.model.layers.25.attention.wo.weight', 'language_model.model.layers.25.attention.wqkv.weight', 'language_model.model.layers.25.attention_norm.weight', 'language_model.model.layers.25.feed_forward.w1.weight', 'language_model.model.layers.25.feed_forward.w2.weight', 'language_model.model.layers.25.feed_forward.w3.weight', 'language_model.model.layers.25.ffn_norm.weight', 'language_model.model.layers.26.attention.wo.weight', 'language_model.model.layers.26.attention.wqkv.weight', 'language_model.model.layers.26.attention_norm.weight', 'language_model.model.layers.26.feed_forward.w1.weight', 'language_model.model.layers.26.feed_forward.w2.weight', 'language_model.model.layers.26.feed_forward.w3.weight', 'language_model.model.layers.26.ffn_norm.weight', 'language_model.model.layers.27.attention.wo.weight', 'language_model.model.layers.27.attention.wqkv.weight', 'language_model.model.layers.27.attention_norm.weight', 'language_model.model.layers.27.feed_forward.w1.weight', 'language_model.model.layers.27.feed_forward.w2.weight', 'language_model.model.layers.27.feed_forward.w3.weight', 'language_model.model.layers.27.ffn_norm.weight', 'language_model.model.layers.28.attention.wo.weight', 'language_model.model.layers.28.attention.wqkv.weight', 'language_model.model.layers.28.attention_norm.weight', 'language_model.model.layers.28.feed_forward.w1.weight', 'language_model.model.layers.28.feed_forward.w2.weight', 'language_model.model.layers.28.feed_forward.w3.weight', 'language_model.model.layers.28.ffn_norm.weight', 'language_model.model.layers.29.attention.wo.weight', 'language_model.model.layers.29.attention.wqkv.weight', 'language_model.model.layers.29.attention_norm.weight', 'language_model.model.layers.29.feed_forward.w1.weight', 'language_model.model.layers.29.feed_forward.w2.weight', 'language_model.model.layers.29.feed_forward.w3.weight', 'language_model.model.layers.29.ffn_norm.weight', 'language_model.model.layers.3.attention.wo.weight', 'language_model.model.layers.3.attention.wqkv.weight', 'language_model.model.layers.3.attention_norm.weight', 'language_model.model.layers.3.feed_forward.w1.weight', 'language_model.model.layers.3.feed_forward.w2.weight', 'language_model.model.layers.3.feed_forward.w3.weight', 'language_model.model.layers.3.ffn_norm.weight', 'language_model.model.layers.30.attention.wo.weight', 'language_model.model.layers.30.attention.wqkv.weight', 'language_model.model.layers.30.attention_norm.weight', 'language_model.model.layers.30.feed_forward.w1.weight', 'language_model.model.layers.30.feed_forward.w2.weight', 'language_model.model.layers.30.feed_forward.w3.weight', 'language_model.model.layers.30.ffn_norm.weight', 'language_model.model.layers.31.attention.wo.weight', 'language_model.model.layers.31.attention.wqkv.weight', 'language_model.model.layers.31.attention_norm.weight', 'language_model.model.layers.31.feed_forward.w1.weight', 'language_model.model.layers.31.feed_forward.w2.weight', 'language_model.model.layers.31.feed_forward.w3.weight', 'language_model.model.layers.31.ffn_norm.weight', 'language_model.model.layers.32.attention.wo.weight', 'language_model.model.layers.32.attention.wqkv.weight', 'language_model.model.layers.32.attention_norm.weight', 'language_model.model.layers.32.feed_forward.w1.weight', 'language_model.model.layers.32.feed_forward.w2.weight', 'language_model.model.layers.32.feed_forward.w3.weight', 'language_model.model.layers.32.ffn_norm.weight', 'language_model.model.layers.33.attention.wo.weight', 'language_model.model.layers.33.attention.wqkv.weight', 'language_model.model.layers.33.attention_norm.weight', 'language_model.model.layers.33.feed_forward.w1.weight', 'language_model.model.layers.33.feed_forward.w2.weight', 'language_model.model.layers.33.feed_forward.w3.weight', 'language_model.model.layers.33.ffn_norm.weight', 'language_model.model.layers.34.attention.wo.weight', 'language_model.model.layers.34.attention.wqkv.weight', 'language_model.model.layers.34.attention_norm.weight', 'language_model.model.layers.34.feed_forward.w1.weight', 'language_model.model.layers.34.feed_forward.w2.weight', 'language_model.model.layers.34.feed_forward.w3.weight', 'language_model.model.layers.34.ffn_norm.weight', 'language_model.model.layers.35.attention.wo.weight', 'language_model.model.layers.35.attention.wqkv.weight', 'language_model.model.layers.35.attention_norm.weight', 'language_model.model.layers.35.feed_forward.w1.weight', 'language_model.model.layers.35.feed_forward.w2.weight', 'language_model.model.layers.35.feed_forward.w3.weight', 'language_model.model.layers.35.ffn_norm.weight', 'language_model.model.layers.36.attention.wo.weight', 'language_model.model.layers.36.attention.wqkv.weight', 'language_model.model.layers.36.attention_norm.weight', 'language_model.model.layers.36.feed_forward.w1.weight', 'language_model.model.layers.36.feed_forward.w2.weight', 'language_model.model.layers.36.feed_forward.w3.weight', 'language_model.model.layers.36.ffn_norm.weight', 'language_model.model.layers.37.attention.wo.weight', 'language_model.model.layers.37.attention.wqkv.weight', 'language_model.model.layers.37.attention_norm.weight', 'language_model.model.layers.37.feed_forward.w1.weight', 'language_model.model.layers.37.feed_forward.w2.weight', 'language_model.model.layers.37.feed_forward.w3.weight', 'language_model.model.layers.37.ffn_norm.weight', 'language_model.model.layers.38.attention.wo.weight', 'language_model.model.layers.38.attention.wqkv.weight', 'language_model.model.layers.38.attention_norm.weight', 'language_model.model.layers.38.feed_forward.w1.weight', 'language_model.model.layers.38.feed_forward.w2.weight', 'language_model.model.layers.38.feed_forward.w3.weight', 'language_model.model.layers.38.ffn_norm.weight', 'language_model.model.layers.39.attention.wo.weight', 'language_model.model.layers.39.attention.wqkv.weight', 'language_model.model.layers.39.attention_norm.weight', 'language_model.model.layers.39.feed_forward.w1.weight', 'language_model.model.layers.39.feed_forward.w2.weight', 'language_model.model.layers.39.feed_forward.w3.weight', 'language_model.model.layers.39.ffn_norm.weight', 'language_model.model.layers.4.attention.wo.weight', 'language_model.model.layers.4.attention.wqkv.weight', 'language_model.model.layers.4.attention_norm.weight', 'language_model.model.layers.4.feed_forward.w1.weight', 'language_model.model.layers.4.feed_forward.w2.weight', 'language_model.model.layers.4.feed_forward.w3.weight', 'language_model.model.layers.4.ffn_norm.weight', 'language_model.model.layers.40.attention.wo.weight', 'language_model.model.layers.40.attention.wqkv.weight', 'language_model.model.layers.40.attention_norm.weight', 'language_model.model.layers.40.feed_forward.w1.weight', 'language_model.model.layers.40.feed_forward.w2.weight', 'language_model.model.layers.40.feed_forward.w3.weight', 'language_model.model.layers.40.ffn_norm.weight', 'language_model.model.layers.41.attention.wo.weight', 'language_model.model.layers.41.attention.wqkv.weight', 'language_model.model.layers.41.attention_norm.weight', 'language_model.model.layers.41.feed_forward.w1.weight', 'language_model.model.layers.41.feed_forward.w2.weight', 'language_model.model.layers.41.feed_forward.w3.weight', 'language_model.model.layers.41.ffn_norm.weight', 'language_model.model.layers.42.attention.wo.weight', 'language_model.model.layers.42.attention.wqkv.weight', 'language_model.model.layers.42.attention_norm.weight', 'language_model.model.layers.42.feed_forward.w1.weight', 'language_model.model.layers.42.feed_forward.w2.weight', 'language_model.model.layers.42.feed_forward.w3.weight', 'language_model.model.layers.42.ffn_norm.weight', 'language_model.model.layers.43.attention.wo.weight', 'language_model.model.layers.43.attention.wqkv.weight', 'language_model.model.layers.43.attention_norm.weight', 'language_model.model.layers.43.feed_forward.w1.weight', 'language_model.model.layers.43.feed_forward.w2.weight', 'language_model.model.layers.43.feed_forward.w3.weight', 'language_model.model.layers.43.ffn_norm.weight', 'language_model.model.layers.44.attention.wo.weight', 'language_model.model.layers.44.attention.wqkv.weight', 'language_model.model.layers.44.attention_norm.weight', 'language_model.model.layers.44.feed_forward.w1.weight', 'language_model.model.layers.44.feed_forward.w2.weight', 'language_model.model.layers.44.feed_forward.w3.weight', 'language_model.model.layers.44.ffn_norm.weight', 'language_model.model.layers.45.attention.wo.weight', 'language_model.model.layers.45.attention.wqkv.weight', 'language_model.model.layers.45.attention_norm.weight', 'language_model.model.layers.45.feed_forward.w1.weight', 'language_model.model.layers.45.feed_forward.w2.weight', 'language_model.model.layers.45.feed_forward.w3.weight', 'language_model.model.layers.45.ffn_norm.weight', 'language_model.model.layers.46.attention.wo.weight', 'language_model.model.layers.46.attention.wqkv.weight', 'language_model.model.layers.46.attention_norm.weight', 'language_model.model.layers.46.feed_forward.w1.weight', 'language_model.model.layers.46.feed_forward.w2.weight', 'language_model.model.layers.46.feed_forward.w3.weight', 'language_model.model.layers.46.ffn_norm.weight', 'language_model.model.layers.47.attention.wo.weight', 'language_model.model.layers.47.attention.wqkv.weight', 'language_model.model.layers.47.attention_norm.weight', 'language_model.model.layers.47.feed_forward.w1.weight', 'language_model.model.layers.47.feed_forward.w2.weight', 'language_model.model.layers.47.feed_forward.w3.weight', 'language_model.model.layers.47.ffn_norm.weight', 'language_model.model.layers.5.attention.wo.weight', 'language_model.model.layers.5.attention.wqkv.weight', 'language_model.model.layers.5.attention_norm.weight', 'language_model.model.layers.5.feed_forward.w1.weight', 'language_model.model.layers.5.feed_forward.w2.weight', 'language_model.model.layers.5.feed_forward.w3.weight', 'language_model.model.layers.5.ffn_norm.weight', 'language_model.model.layers.6.attention.wo.weight', 'language_model.model.layers.6.attention.wqkv.weight', 'language_model.model.layers.6.attention_norm.weight', 'language_model.model.layers.6.feed_forward.w1.weight', 'language_model.model.layers.6.feed_forward.w2.weight', 'language_model.model.layers.6.feed_forward.w3.weight', 'language_model.model.layers.6.ffn_norm.weight', 'language_model.model.layers.7.attention.wo.weight', 'language_model.model.layers.7.attention.wqkv.weight', 'language_model.model.layers.7.attention_norm.weight', 'language_model.model.layers.7.feed_forward.w1.weight', 'language_model.model.layers.7.feed_forward.w2.weight', 'language_model.model.layers.7.feed_forward.w3.weight', 'language_model.model.layers.7.ffn_norm.weight', 'language_model.model.layers.8.attention.wo.weight', 'language_model.model.layers.8.attention.wqkv.weight', 'language_model.model.layers.8.attention_norm.weight', 'language_model.model.layers.8.feed_forward.w1.weight', 'language_model.model.layers.8.feed_forward.w2.weight', 'language_model.model.layers.8.feed_forward.w3.weight', 'language_model.model.layers.8.ffn_norm.weight', 'language_model.model.layers.9.attention.wo.weight', 'language_model.model.layers.9.attention.wqkv.weight', 'language_model.model.layers.9.attention_norm.weight', 'language_model.model.layers.9.feed_forward.w1.weight', 'language_model.model.layers.9.feed_forward.w2.weight', 'language_model.model.layers.9.feed_forward.w3.weight', 'language_model.model.layers.9.ffn_norm.weight', 'language_model.model.norm.weight', 'language_model.model.tok_embeddings.weight', 'language_model.output.weight', 'vision_model.embeddings.class_embedding', 'vision_model.embeddings.patch_embedding.bias', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.embeddings.position_embedding', 'vision_model.encoder.layers.0.attn.k_norm.weight', 'vision_model.encoder.layers.0.attn.proj.bias', 'vision_model.encoder.layers.0.attn.proj.weight', 'vision_model.encoder.layers.0.attn.q_norm.weight', 'vision_model.encoder.layers.0.attn.qkv.weight', 'vision_model.encoder.layers.0.ls1', 'vision_model.encoder.layers.0.ls2', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.0.norm1.weight', 'vision_model.encoder.layers.0.norm2.weight', 'vision_model.encoder.layers.1.attn.k_norm.weight', 'vision_model.encoder.layers.1.attn.proj.bias', 'vision_model.encoder.layers.1.attn.proj.weight', 'vision_model.encoder.layers.1.attn.q_norm.weight', 'vision_model.encoder.layers.1.attn.qkv.weight', 'vision_model.encoder.layers.1.ls1', 'vision_model.encoder.layers.1.ls2', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.1.norm1.weight', 'vision_model.encoder.layers.1.norm2.weight', 'vision_model.encoder.layers.10.attn.k_norm.weight', 'vision_model.encoder.layers.10.attn.proj.bias', 'vision_model.encoder.layers.10.attn.proj.weight', 'vision_model.encoder.layers.10.attn.q_norm.weight', 'vision_model.encoder.layers.10.attn.qkv.weight', 'vision_model.encoder.layers.10.ls1', 'vision_model.encoder.layers.10.ls2', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.10.norm1.weight', 'vision_model.encoder.layers.10.norm2.weight', 'vision_model.encoder.layers.11.attn.k_norm.weight', 'vision_model.encoder.layers.11.attn.proj.bias', 'vision_model.encoder.layers.11.attn.proj.weight', 'vision_model.encoder.layers.11.attn.q_norm.weight', 'vision_model.encoder.layers.11.attn.qkv.weight', 'vision_model.encoder.layers.11.ls1', 'vision_model.encoder.layers.11.ls2', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.11.norm1.weight', 'vision_model.encoder.layers.11.norm2.weight', 'vision_model.encoder.layers.12.attn.k_norm.weight', 'vision_model.encoder.layers.12.attn.proj.bias', 'vision_model.encoder.layers.12.attn.proj.weight', 'vision_model.encoder.layers.12.attn.q_norm.weight', 'vision_model.encoder.layers.12.attn.qkv.weight', 'vision_model.encoder.layers.12.ls1', 'vision_model.encoder.layers.12.ls2', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.12.norm1.weight', 'vision_model.encoder.layers.12.norm2.weight', 'vision_model.encoder.layers.13.attn.k_norm.weight', 'vision_model.encoder.layers.13.attn.proj.bias', 'vision_model.encoder.layers.13.attn.proj.weight', 'vision_model.encoder.layers.13.attn.q_norm.weight', 'vision_model.encoder.layers.13.attn.qkv.weight', 'vision_model.encoder.layers.13.ls1', 'vision_model.encoder.layers.13.ls2', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.13.norm1.weight', 'vision_model.encoder.layers.13.norm2.weight', 'vision_model.encoder.layers.14.attn.k_norm.weight', 'vision_model.encoder.layers.14.attn.proj.bias', 'vision_model.encoder.layers.14.attn.proj.weight', 'vision_model.encoder.layers.14.attn.q_norm.weight', 'vision_model.encoder.layers.14.attn.qkv.weight', 'vision_model.encoder.layers.14.ls1', 'vision_model.encoder.layers.14.ls2', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.14.norm1.weight', 'vision_model.encoder.layers.14.norm2.weight', 'vision_model.encoder.layers.15.attn.k_norm.weight', 'vision_model.encoder.layers.15.attn.proj.bias', 'vision_model.encoder.layers.15.attn.proj.weight', 'vision_model.encoder.layers.15.attn.q_norm.weight', 'vision_model.encoder.layers.15.attn.qkv.weight', 'vision_model.encoder.layers.15.ls1', 'vision_model.encoder.layers.15.ls2', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.15.norm1.weight', 'vision_model.encoder.layers.15.norm2.weight', 'vision_model.encoder.layers.16.attn.k_norm.weight', 'vision_model.encoder.layers.16.attn.proj.bias', 'vision_model.encoder.layers.16.attn.proj.weight', 'vision_model.encoder.layers.16.attn.q_norm.weight', 'vision_model.encoder.layers.16.attn.qkv.weight', 'vision_model.encoder.layers.16.ls1', 'vision_model.encoder.layers.16.ls2', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.16.norm1.weight', 'vision_model.encoder.layers.16.norm2.weight', 'vision_model.encoder.layers.17.attn.k_norm.weight', 'vision_model.encoder.layers.17.attn.proj.bias', 'vision_model.encoder.layers.17.attn.proj.weight', 'vision_model.encoder.layers.17.attn.q_norm.weight', 'vision_model.encoder.layers.17.attn.qkv.weight', 'vision_model.encoder.layers.17.ls1', 'vision_model.encoder.layers.17.ls2', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.17.norm1.weight', 'vision_model.encoder.layers.17.norm2.weight', 'vision_model.encoder.layers.18.attn.k_norm.weight', 'vision_model.encoder.layers.18.attn.proj.bias', 'vision_model.encoder.layers.18.attn.proj.weight', 'vision_model.encoder.layers.18.attn.q_norm.weight', 'vision_model.encoder.layers.18.attn.qkv.weight', 'vision_model.encoder.layers.18.ls1', 'vision_model.encoder.layers.18.ls2', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.18.norm1.weight', 'vision_model.encoder.layers.18.norm2.weight', 'vision_model.encoder.layers.19.attn.k_norm.weight', 'vision_model.encoder.layers.19.attn.proj.bias', 'vision_model.encoder.layers.19.attn.proj.weight', 'vision_model.encoder.layers.19.attn.q_norm.weight', 'vision_model.encoder.layers.19.attn.qkv.weight', 'vision_model.encoder.layers.19.ls1', 'vision_model.encoder.layers.19.ls2', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.19.norm1.weight', 'vision_model.encoder.layers.19.norm2.weight', 'vision_model.encoder.layers.2.attn.k_norm.weight', 'vision_model.encoder.layers.2.attn.proj.bias', 'vision_model.encoder.layers.2.attn.proj.weight', 'vision_model.encoder.layers.2.attn.q_norm.weight', 'vision_model.encoder.layers.2.attn.qkv.weight', 'vision_model.encoder.layers.2.ls1', 'vision_model.encoder.layers.2.ls2', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.2.norm1.weight', 'vision_model.encoder.layers.2.norm2.weight', 'vision_model.encoder.layers.20.attn.k_norm.weight', 'vision_model.encoder.layers.20.attn.proj.bias', 'vision_model.encoder.layers.20.attn.proj.weight', 'vision_model.encoder.layers.20.attn.q_norm.weight', 'vision_model.encoder.layers.20.attn.qkv.weight', 'vision_model.encoder.layers.20.ls1', 'vision_model.encoder.layers.20.ls2', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.20.norm1.weight', 'vision_model.encoder.layers.20.norm2.weight', 'vision_model.encoder.layers.21.attn.k_norm.weight', 'vision_model.encoder.layers.21.attn.proj.bias', 'vision_model.encoder.layers.21.attn.proj.weight', 'vision_model.encoder.layers.21.attn.q_norm.weight', 'vision_model.encoder.layers.21.attn.qkv.weight', 'vision_model.encoder.layers.21.ls1', 'vision_model.encoder.layers.21.ls2', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.21.norm1.weight', 'vision_model.encoder.layers.21.norm2.weight', 'vision_model.encoder.layers.22.attn.k_norm.weight', 'vision_model.encoder.layers.22.attn.proj.bias', 'vision_model.encoder.layers.22.attn.proj.weight', 'vision_model.encoder.layers.22.attn.q_norm.weight', 'vision_model.encoder.layers.22.attn.qkv.weight', 'vision_model.encoder.layers.22.ls1', 'vision_model.encoder.layers.22.ls2', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.22.norm1.weight', 'vision_model.encoder.layers.22.norm2.weight', 'vision_model.encoder.layers.23.attn.k_norm.weight', 'vision_model.encoder.layers.23.attn.proj.bias', 'vision_model.encoder.layers.23.attn.proj.weight', 'vision_model.encoder.layers.23.attn.q_norm.weight', 'vision_model.encoder.layers.23.attn.qkv.weight', 'vision_model.encoder.layers.23.ls1', 'vision_model.encoder.layers.23.ls2', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.23.norm1.weight', 'vision_model.encoder.layers.23.norm2.weight', 'vision_model.encoder.layers.24.attn.k_norm.weight', 'vision_model.encoder.layers.24.attn.proj.bias', 'vision_model.encoder.layers.24.attn.proj.weight', 'vision_model.encoder.layers.24.attn.q_norm.weight', 'vision_model.encoder.layers.24.attn.qkv.weight', 'vision_model.encoder.layers.24.ls1', 'vision_model.encoder.layers.24.ls2', 'vision_model.encoder.layers.24.mlp.fc1.bias', 'vision_model.encoder.layers.24.mlp.fc1.weight', 'vision_model.encoder.layers.24.mlp.fc2.bias', 'vision_model.encoder.layers.24.mlp.fc2.weight', 'vision_model.encoder.layers.24.norm1.weight', 'vision_model.encoder.layers.24.norm2.weight', 'vision_model.encoder.layers.25.attn.k_norm.weight', 'vision_model.encoder.layers.25.attn.proj.bias', 'vision_model.encoder.layers.25.attn.proj.weight', 'vision_model.encoder.layers.25.attn.q_norm.weight', 'vision_model.encoder.layers.25.attn.qkv.weight', 'vision_model.encoder.layers.25.ls1', 'vision_model.encoder.layers.25.ls2', 'vision_model.encoder.layers.25.mlp.fc1.bias', 'vision_model.encoder.layers.25.mlp.fc1.weight', 'vision_model.encoder.layers.25.mlp.fc2.bias', 'vision_model.encoder.layers.25.mlp.fc2.weight', 'vision_model.encoder.layers.25.norm1.weight', 'vision_model.encoder.layers.25.norm2.weight', 'vision_model.encoder.layers.26.attn.k_norm.weight', 'vision_model.encoder.layers.26.attn.proj.bias', 'vision_model.encoder.layers.26.attn.proj.weight', 'vision_model.encoder.layers.26.attn.q_norm.weight', 'vision_model.encoder.layers.26.attn.qkv.weight', 'vision_model.encoder.layers.26.ls1', 'vision_model.encoder.layers.26.ls2', 'vision_model.encoder.layers.26.mlp.fc1.bias', 'vision_model.encoder.layers.26.mlp.fc1.weight', 'vision_model.encoder.layers.26.mlp.fc2.bias', 'vision_model.encoder.layers.26.mlp.fc2.weight', 'vision_model.encoder.layers.26.norm1.weight', 'vision_model.encoder.layers.26.norm2.weight', 'vision_model.encoder.layers.27.attn.k_norm.weight', 'vision_model.encoder.layers.27.attn.proj.bias', 'vision_model.encoder.layers.27.attn.proj.weight', 'vision_model.encoder.layers.27.attn.q_norm.weight', 'vision_model.encoder.layers.27.attn.qkv.weight', 'vision_model.encoder.layers.27.ls1', 'vision_model.encoder.layers.27.ls2', 'vision_model.encoder.layers.27.mlp.fc1.bias', 'vision_model.encoder.layers.27.mlp.fc1.weight', 'vision_model.encoder.layers.27.mlp.fc2.bias', 'vision_model.encoder.layers.27.mlp.fc2.weight', 'vision_model.encoder.layers.27.norm1.weight', 'vision_model.encoder.layers.27.norm2.weight', 'vision_model.encoder.layers.28.attn.k_norm.weight', 'vision_model.encoder.layers.28.attn.proj.bias', 'vision_model.encoder.layers.28.attn.proj.weight', 'vision_model.encoder.layers.28.attn.q_norm.weight', 'vision_model.encoder.layers.28.attn.qkv.weight', 'vision_model.encoder.layers.28.ls1', 'vision_model.encoder.layers.28.ls2', 'vision_model.encoder.layers.28.mlp.fc1.bias', 'vision_model.encoder.layers.28.mlp.fc1.weight', 'vision_model.encoder.layers.28.mlp.fc2.bias', 'vision_model.encoder.layers.28.mlp.fc2.weight', 'vision_model.encoder.layers.28.norm1.weight', 'vision_model.encoder.layers.28.norm2.weight', 'vision_model.encoder.layers.29.attn.k_norm.weight', 'vision_model.encoder.layers.29.attn.proj.bias', 'vision_model.encoder.layers.29.attn.proj.weight', 'vision_model.encoder.layers.29.attn.q_norm.weight', 'vision_model.encoder.layers.29.attn.qkv.weight', 'vision_model.encoder.layers.29.ls1', 'vision_model.encoder.layers.29.ls2', 'vision_model.encoder.layers.29.mlp.fc1.bias', 'vision_model.encoder.layers.29.mlp.fc1.weight', 'vision_model.encoder.layers.29.mlp.fc2.bias', 'vision_model.encoder.layers.29.mlp.fc2.weight', 'vision_model.encoder.layers.29.norm1.weight', 'vision_model.encoder.layers.29.norm2.weight', 'vision_model.encoder.layers.3.attn.k_norm.weight', 'vision_model.encoder.layers.3.attn.proj.bias', 'vision_model.encoder.layers.3.attn.proj.weight', 'vision_model.encoder.layers.3.attn.q_norm.weight', 'vision_model.encoder.layers.3.attn.qkv.weight', 'vision_model.encoder.layers.3.ls1', 'vision_model.encoder.layers.3.ls2', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.3.norm1.weight', 'vision_model.encoder.layers.3.norm2.weight', 'vision_model.encoder.layers.30.attn.k_norm.weight', 'vision_model.encoder.layers.30.attn.proj.bias', 'vision_model.encoder.layers.30.attn.proj.weight', 'vision_model.encoder.layers.30.attn.q_norm.weight', 'vision_model.encoder.layers.30.attn.qkv.weight', 'vision_model.encoder.layers.30.ls1', 'vision_model.encoder.layers.30.ls2', 'vision_model.encoder.layers.30.mlp.fc1.bias', 'vision_model.encoder.layers.30.mlp.fc1.weight', 'vision_model.encoder.layers.30.mlp.fc2.bias', 'vision_model.encoder.layers.30.mlp.fc2.weight', 'vision_model.encoder.layers.30.norm1.weight', 'vision_model.encoder.layers.30.norm2.weight', 'vision_model.encoder.layers.31.attn.k_norm.weight', 'vision_model.encoder.layers.31.attn.proj.bias', 'vision_model.encoder.layers.31.attn.proj.weight', 'vision_model.encoder.layers.31.attn.q_norm.weight', 'vision_model.encoder.layers.31.attn.qkv.weight', 'vision_model.encoder.layers.31.ls1', 'vision_model.encoder.layers.31.ls2', 'vision_model.encoder.layers.31.mlp.fc1.bias', 'vision_model.encoder.layers.31.mlp.fc1.weight', 'vision_model.encoder.layers.31.mlp.fc2.bias', 'vision_model.encoder.layers.31.mlp.fc2.weight', 'vision_model.encoder.layers.31.norm1.weight', 'vision_model.encoder.layers.31.norm2.weight', 'vision_model.encoder.layers.32.attn.k_norm.weight', 'vision_model.encoder.layers.32.attn.proj.bias', 'vision_model.encoder.layers.32.attn.proj.weight', 'vision_model.encoder.layers.32.attn.q_norm.weight', 'vision_model.encoder.layers.32.attn.qkv.weight', 'vision_model.encoder.layers.32.ls1', 'vision_model.encoder.layers.32.ls2', 'vision_model.encoder.layers.32.mlp.fc1.bias', 'vision_model.encoder.layers.32.mlp.fc1.weight', 'vision_model.encoder.layers.32.mlp.fc2.bias', 'vision_model.encoder.layers.32.mlp.fc2.weight', 'vision_model.encoder.layers.32.norm1.weight', 'vision_model.encoder.layers.32.norm2.weight', 'vision_model.encoder.layers.33.attn.k_norm.weight', 'vision_model.encoder.layers.33.attn.proj.bias', 'vision_model.encoder.layers.33.attn.proj.weight', 'vision_model.encoder.layers.33.attn.q_norm.weight', 'vision_model.encoder.layers.33.attn.qkv.weight', 'vision_model.encoder.layers.33.ls1', 'vision_model.encoder.layers.33.ls2', 'vision_model.encoder.layers.33.mlp.fc1.bias', 'vision_model.encoder.layers.33.mlp.fc1.weight', 'vision_model.encoder.layers.33.mlp.fc2.bias', 'vision_model.encoder.layers.33.mlp.fc2.weight', 'vision_model.encoder.layers.33.norm1.weight', 'vision_model.encoder.layers.33.norm2.weight', 'vision_model.encoder.layers.34.attn.k_norm.weight', 'vision_model.encoder.layers.34.attn.proj.bias', 'vision_model.encoder.layers.34.attn.proj.weight', 'vision_model.encoder.layers.34.attn.q_norm.weight', 'vision_model.encoder.layers.34.attn.qkv.weight', 'vision_model.encoder.layers.34.ls1', 'vision_model.encoder.layers.34.ls2', 'vision_model.encoder.layers.34.mlp.fc1.bias', 'vision_model.encoder.layers.34.mlp.fc1.weight', 'vision_model.encoder.layers.34.mlp.fc2.bias', 'vision_model.encoder.layers.34.mlp.fc2.weight', 'vision_model.encoder.layers.34.norm1.weight', 'vision_model.encoder.layers.34.norm2.weight', 'vision_model.encoder.layers.35.attn.k_norm.weight', 'vision_model.encoder.layers.35.attn.proj.bias', 'vision_model.encoder.layers.35.attn.proj.weight', 'vision_model.encoder.layers.35.attn.q_norm.weight', 'vision_model.encoder.layers.35.attn.qkv.weight', 'vision_model.encoder.layers.35.ls1', 'vision_model.encoder.layers.35.ls2', 'vision_model.encoder.layers.35.mlp.fc1.bias', 'vision_model.encoder.layers.35.mlp.fc1.weight', 'vision_model.encoder.layers.35.mlp.fc2.bias', 'vision_model.encoder.layers.35.mlp.fc2.weight', 'vision_model.encoder.layers.35.norm1.weight', 'vision_model.encoder.layers.35.norm2.weight', 'vision_model.encoder.layers.36.attn.k_norm.weight', 'vision_model.encoder.layers.36.attn.proj.bias', 'vision_model.encoder.layers.36.attn.proj.weight', 'vision_model.encoder.layers.36.attn.q_norm.weight', 'vision_model.encoder.layers.36.attn.qkv.weight', 'vision_model.encoder.layers.36.ls1', 'vision_model.encoder.layers.36.ls2', 'vision_model.encoder.layers.36.mlp.fc1.bias', 'vision_model.encoder.layers.36.mlp.fc1.weight', 'vision_model.encoder.layers.36.mlp.fc2.bias', 'vision_model.encoder.layers.36.mlp.fc2.weight', 'vision_model.encoder.layers.36.norm1.weight', 'vision_model.encoder.layers.36.norm2.weight', 'vision_model.encoder.layers.37.attn.k_norm.weight', 'vision_model.encoder.layers.37.attn.proj.bias', 'vision_model.encoder.layers.37.attn.proj.weight', 'vision_model.encoder.layers.37.attn.q_norm.weight', 'vision_model.encoder.layers.37.attn.qkv.weight', 'vision_model.encoder.layers.37.ls1', 'vision_model.encoder.layers.37.ls2', 'vision_model.encoder.layers.37.mlp.fc1.bias', 'vision_model.encoder.layers.37.mlp.fc1.weight', 'vision_model.encoder.layers.37.mlp.fc2.bias', 'vision_model.encoder.layers.37.mlp.fc2.weight', 'vision_model.encoder.layers.37.norm1.weight', 'vision_model.encoder.layers.37.norm2.weight', 'vision_model.encoder.layers.38.attn.k_norm.weight', 'vision_model.encoder.layers.38.attn.proj.bias', 'vision_model.encoder.layers.38.attn.proj.weight', 'vision_model.encoder.layers.38.attn.q_norm.weight', 'vision_model.encoder.layers.38.attn.qkv.weight', 'vision_model.encoder.layers.38.ls1', 'vision_model.encoder.layers.38.ls2', 'vision_model.encoder.layers.38.mlp.fc1.bias', 'vision_model.encoder.layers.38.mlp.fc1.weight', 'vision_model.encoder.layers.38.mlp.fc2.bias', 'vision_model.encoder.layers.38.mlp.fc2.weight', 'vision_model.encoder.layers.38.norm1.weight', 'vision_model.encoder.layers.38.norm2.weight', 'vision_model.encoder.layers.39.attn.k_norm.weight', 'vision_model.encoder.layers.39.attn.proj.bias', 'vision_model.encoder.layers.39.attn.proj.weight', 'vision_model.encoder.layers.39.attn.q_norm.weight', 'vision_model.encoder.layers.39.attn.qkv.weight', 'vision_model.encoder.layers.39.ls1', 'vision_model.encoder.layers.39.ls2', 'vision_model.encoder.layers.39.mlp.fc1.bias', 'vision_model.encoder.layers.39.mlp.fc1.weight', 'vision_model.encoder.layers.39.mlp.fc2.bias', 'vision_model.encoder.layers.39.mlp.fc2.weight', 'vision_model.encoder.layers.39.norm1.weight', 'vision_model.encoder.layers.39.norm2.weight', 'vision_model.encoder.layers.4.attn.k_norm.weight', 'vision_model.encoder.layers.4.attn.proj.bias', 'vision_model.encoder.layers.4.attn.proj.weight', 'vision_model.encoder.layers.4.attn.q_norm.weight', 'vision_model.encoder.layers.4.attn.qkv.weight', 'vision_model.encoder.layers.4.ls1', 'vision_model.encoder.layers.4.ls2', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.4.norm1.weight', 'vision_model.encoder.layers.4.norm2.weight', 'vision_model.encoder.layers.40.attn.k_norm.weight', 'vision_model.encoder.layers.40.attn.proj.bias', 'vision_model.encoder.layers.40.attn.proj.weight', 'vision_model.encoder.layers.40.attn.q_norm.weight', 'vision_model.encoder.layers.40.attn.qkv.weight', 'vision_model.encoder.layers.40.ls1', 'vision_model.encoder.layers.40.ls2', 'vision_model.encoder.layers.40.mlp.fc1.bias', 'vision_model.encoder.layers.40.mlp.fc1.weight', 'vision_model.encoder.layers.40.mlp.fc2.bias', 'vision_model.encoder.layers.40.mlp.fc2.weight', 'vision_model.encoder.layers.40.norm1.weight', 'vision_model.encoder.layers.40.norm2.weight', 'vision_model.encoder.layers.41.attn.k_norm.weight', 'vision_model.encoder.layers.41.attn.proj.bias', 'vision_model.encoder.layers.41.attn.proj.weight', 'vision_model.encoder.layers.41.attn.q_norm.weight', 'vision_model.encoder.layers.41.attn.qkv.weight', 'vision_model.encoder.layers.41.ls1', 'vision_model.encoder.layers.41.ls2', 'vision_model.encoder.layers.41.mlp.fc1.bias', 'vision_model.encoder.layers.41.mlp.fc1.weight', 'vision_model.encoder.layers.41.mlp.fc2.bias', 'vision_model.encoder.layers.41.mlp.fc2.weight', 'vision_model.encoder.layers.41.norm1.weight', 'vision_model.encoder.layers.41.norm2.weight', 'vision_model.encoder.layers.42.attn.k_norm.weight', 'vision_model.encoder.layers.42.attn.proj.bias', 'vision_model.encoder.layers.42.attn.proj.weight', 'vision_model.encoder.layers.42.attn.q_norm.weight', 'vision_model.encoder.layers.42.attn.qkv.weight', 'vision_model.encoder.layers.42.ls1', 'vision_model.encoder.layers.42.ls2', 'vision_model.encoder.layers.42.mlp.fc1.bias', 'vision_model.encoder.layers.42.mlp.fc1.weight', 'vision_model.encoder.layers.42.mlp.fc2.bias', 'vision_model.encoder.layers.42.mlp.fc2.weight', 'vision_model.encoder.layers.42.norm1.weight', 'vision_model.encoder.layers.42.norm2.weight', 'vision_model.encoder.layers.43.attn.k_norm.weight', 'vision_model.encoder.layers.43.attn.proj.bias', 'vision_model.encoder.layers.43.attn.proj.weight', 'vision_model.encoder.layers.43.attn.q_norm.weight', 'vision_model.encoder.layers.43.attn.qkv.weight', 'vision_model.encoder.layers.43.ls1', 'vision_model.encoder.layers.43.ls2', 'vision_model.encoder.layers.43.mlp.fc1.bias', 'vision_model.encoder.layers.43.mlp.fc1.weight', 'vision_model.encoder.layers.43.mlp.fc2.bias', 'vision_model.encoder.layers.43.mlp.fc2.weight', 'vision_model.encoder.layers.43.norm1.weight', 'vision_model.encoder.layers.43.norm2.weight', 'vision_model.encoder.layers.44.attn.k_norm.weight', 'vision_model.encoder.layers.44.attn.proj.bias', 'vision_model.encoder.layers.44.attn.proj.weight', 'vision_model.encoder.layers.44.attn.q_norm.weight', 'vision_model.encoder.layers.44.attn.qkv.weight', 'vision_model.encoder.layers.44.ls1', 'vision_model.encoder.layers.44.ls2', 'vision_model.encoder.layers.44.mlp.fc1.bias', 'vision_model.encoder.layers.44.mlp.fc1.weight', 'vision_model.encoder.layers.44.mlp.fc2.bias', 'vision_model.encoder.layers.44.mlp.fc2.weight', 'vision_model.encoder.layers.44.norm1.weight', 'vision_model.encoder.layers.44.norm2.weight', 'vision_model.encoder.layers.5.attn.k_norm.weight', 'vision_model.encoder.layers.5.attn.proj.bias', 'vision_model.encoder.layers.5.attn.proj.weight', 'vision_model.encoder.layers.5.attn.q_norm.weight', 'vision_model.encoder.layers.5.attn.qkv.weight', 'vision_model.encoder.layers.5.ls1', 'vision_model.encoder.layers.5.ls2', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.5.norm1.weight', 'vision_model.encoder.layers.5.norm2.weight', 'vision_model.encoder.layers.6.attn.k_norm.weight', 'vision_model.encoder.layers.6.attn.proj.bias', 'vision_model.encoder.layers.6.attn.proj.weight', 'vision_model.encoder.layers.6.attn.q_norm.weight', 'vision_model.encoder.layers.6.attn.qkv.weight', 'vision_model.encoder.layers.6.ls1', 'vision_model.encoder.layers.6.ls2', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.6.norm1.weight', 'vision_model.encoder.layers.6.norm2.weight', 'vision_model.encoder.layers.7.attn.k_norm.weight', 'vision_model.encoder.layers.7.attn.proj.bias', 'vision_model.encoder.layers.7.attn.proj.weight', 'vision_model.encoder.layers.7.attn.q_norm.weight', 'vision_model.encoder.layers.7.attn.qkv.weight', 'vision_model.encoder.layers.7.ls1', 'vision_model.encoder.layers.7.ls2', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.7.norm1.weight', 'vision_model.encoder.layers.7.norm2.weight', 'vision_model.encoder.layers.8.attn.k_norm.weight', 'vision_model.encoder.layers.8.attn.proj.bias', 'vision_model.encoder.layers.8.attn.proj.weight', 'vision_model.encoder.layers.8.attn.q_norm.weight', 'vision_model.encoder.layers.8.attn.qkv.weight', 'vision_model.encoder.layers.8.ls1', 'vision_model.encoder.layers.8.ls2', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.8.norm1.weight', 'vision_model.encoder.layers.8.norm2.weight', 'vision_model.encoder.layers.9.attn.k_norm.weight', 'vision_model.encoder.layers.9.attn.proj.bias', 'vision_model.encoder.layers.9.attn.proj.weight', 'vision_model.encoder.layers.9.attn.q_norm.weight', 'vision_model.encoder.layers.9.attn.qkv.weight', 'vision_model.encoder.layers.9.ls1', 'vision_model.encoder.layers.9.ls2', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.9.norm1.weight', 'vision_model.encoder.layers.9.norm2.weight']
- This IS expected if you are initializing InternVLChatModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing InternVLChatModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of InternVLChatModel were not initialized from the model checkpoint at /mnt/hwfile/ai4chem/CKPT/chemvl_ft_6_19_0_merged and are newly initialized: ['language_model.base_model.model.model.layers.0.attention.wo.base_layer.weight', 'language_model.base_model.model.model.layers.0.attention.wo.lora_A.default.weight', 'language_model.base_model.model.model.layers.0.attention.wo.lora_B.default.weight', 'language_model.base_model.model.model.layers.0.attention.wqkv.base_layer.weight', 'language_model.base_model.model.model.layers.0.attention.wqkv.lora_A.default.weight', 'language_model.base_model.model.model.layers.0.attention.wqkv.lora_B.default.weight', 'language_model.base_model.model.model.layers.0.attention_norm.weight', 'language_model.base_model.model.model.layers.0.feed_forward.w1.base_layer.weight', 'language_model.base_model.model.model.layers.0.feed_forward.w1.lora_A.default.weight', 'language_model.base_model.model.model.layers.0.feed_forward.w1.lora_B.default.weight', 'language_model.base_model.model.model.layers.0.feed_forward.w2.base_layer.weight', 'language_model.base_model.model.model.layers.0.feed_forward.w2.lora_A.default.weight', 'language_model.base_model.model.model.layers.0.feed_forward.w2.lora_B.default.weight', 'language_model.base_model.model.model.layers.0.feed_forward.w3.base_layer.weight', 'language_model.base_model.model.model.layers.0.feed_forward.w3.lora_A.default.weight', 'language_model.base_model.model.model.layers.0.feed_forward.w3.lora_B.default.weight', 'language_model.base_model.model.model.layers.0.ffn_norm.weight', 'language_model.base_model.model.model.layers.1.attention.wo.base_layer.weight', 'language_model.base_model.model.model.layers.1.attention.wo.lora_A.default.weight', 'language_model.base_model.model.model.layers.1.attention.wo.lora_B.default.weight', 'language_model.base_model.model.model.layers.1.attention.wqkv.base_layer.weight', 'language_model.base_model.model.model.layers.1.attention.wqkv.lora_A.default.weight', 'language_model.base_model.model.model.layers.1.attention.wqkv.lora_B.default.weight', 'language_model.base_model.model.model.layers.1.attention_norm.weight', 'language_model.base_model.model.model.layers.1.feed_forward.w1.base_layer.weight', 'language_model.base_model.model.model.layers.1.feed_forward.w1.lora_A.default.weight', 'language_model.base_model.model.model.layers.1.feed_forward.w1.lora_B.default.weight', 'language_model.base_model.model.model.layers.1.feed_forward.w2.base_layer.weight', 'language_model.base_model.model.model.layers.1.feed_forward.w2.lora_A.default.weight', 'language_model.base_model.model.model.layers.1.feed_forward.w2.lora_B.default.weight', 'language_model.base_model.model.model.layers.1.feed_forward.w3.base_layer.weight', 'language_model.base_model.model.model.layers.1.feed_forward.w3.lora_A.default.weight', 'language_model.base_model.model.model.layers.1.feed_forward.w3.lora_B.default.weight', 'language_model.base_model.model.model.layers.1.ffn_norm.weight', 'language_model.base_model.model.model.layers.10.attention.wo.base_layer.weight', 'language_model.base_model.model.model.layers.10.attention.wo.lora_A.default.weight', 'language_model.base_model.model.model.layers.10.attention.wo.lora_B.default.weight', 'language_model.base_model.model.model.layers.10.attention.wqkv.base_layer.weight', 'language_model.base_model.model.model.layers.10.attention.wqkv.lora_A.default.weight', 'language_model.base_model.model.model.layers.10.attention.wqkv.lora_B.default.weight', 'language_model.base_model.model.model.layers.10.attention_norm.weight', 'language_model.base_model.model.model.layers.10.feed_forward.w1.base_layer.weight', 'language_model.base_model.model.model.layers.10.feed_forward.w1.lora_A.default.weight', 'language_model.base_model.model.model.layers.10.feed_forward.w1.lora_B.default.weight', 'language_model.base_model.model.model.layers.10.feed_forward.w2.base_layer.weight', 'language_model.base_model.model.model.layers.10.feed_forward.w2.lora_A.default.weight', 'language_model.base_model.model.model.layers.10.feed_forward.w2.lora_B.default.weight', 'language_model.base_model.model.model.layers.10.feed_forward.w3.base_layer.weight', 'language_model.base_model.model.model.layers.10.feed_forward.w3.lora_A.default.weight', 'language_model.base_model.model.model.layers.10.feed_forward.w3.lora_B.default.weight', 'language_model.base_model.model.model.layers.10.ffn_norm.weight', 'language_model.base_model.model.model.layers.11.attention.wo.base_layer.weight', 'language_model.base_model.model.model.layers.11.attention.wo.lora_A.default.weight', 'language_model.base_model.model.model.layers.11.attention.wo.lora_B.default.weight', 'language_model.base_model.model.model.layers.11.attention.wqkv.base_layer.weight', 'language_model.base_model.model.model.layers.11.attention.wqkv.lora_A.default.weight', 'language_model.base_model.model.model.layers.11.attention.wqkv.lora_B.default.weight', 'language_model.base_model.model.model.layers.11.attention_norm.weight', 'language_model.base_model.model.model.layers.11.feed_forward.w1.base_layer.weight', 'language_model.base_model.model.model.layers.11.feed_forward.w1.lora_A.default.weight', 'language_model.base_model.model.model.layers.11.feed_forward.w1.lora_B.default.weight', 'language_model.base_model.model.model.layers.11.feed_forward.w2.base_layer.weight', 'language_model.base_model.model.model.layers.11.feed_forward.w2.lora_A.default.weight', 'language_model.base_model.model.model.layers.11.feed_forward.w2.lora_B.default.weight', 'language_model.base_model.model.model.layers.11.feed_forward.w3.base_layer.weight', 'language_model.base_model.model.model.layers.11.feed_forward.w3.lora_A.default.weight', 'language_model.base_model.model.model.layers.11.feed_forward.w3.lora_B.default.weight', 'language_model.base_model.model.model.layers.11.ffn_norm.weight', 'language_model.base_model.model.model.layers.12.attention.wo.base_layer.weight', 'language_model.base_model.model.model.layers.12.attention.wo.lora_A.default.weight', 'language_model.base_model.model.model.layers.12.attention.wo.lora_B.default.weight', 'language_model.base_model.model.model.layers.12.attention.wqkv.base_layer.weight', 'language_model.base_model.model.model.layers.12.attention.wqkv.lora_A.default.weight', 'language_model.base_model.model.model.layers.12.attention.wqkv.lora_B.default.weight', 'language_model.base_model.model.model.layers.12.attention_norm.weight', 'language_model.base_model.model.model.layers.12.feed_forward.w1.base_layer.weight', 'language_model.base_model.model.model.layers.12.feed_forward.w1.lora_A.default.weight', 'language_model.base_model.model.model.layers.12.feed_forward.w1.lora_B.default.weight', 'language_model.base_model.model.model.layers.12.feed_forward.w2.base_layer.weight', 'language_model.base_model.model.model.layers.12.feed_forward.w2.lora_A.default.weight', 'language_model.base_model.model.model.layers.12.feed_forward.w2.lora_B.default.weight', 'language_model.base_model.model.model.layers.12.feed_forward.w3.base_layer.weight', 'language_model.base_model.model.model.layers.12.feed_forward.w3.lora_A.default.weight', 'language_model.base_model.model.model.layers.12.feed_forward.w3.lora_B.default.weight', 'language_model.base_model.model.model.layers.12.ffn_norm.weight', 'language_model.base_model.model.model.layers.13.attention.wo.base_layer.weight', 'language_model.base_model.model.model.layers.13.attention.wo.lora_A.default.weight', 'language_model.base_model.model.model.layers.13.attention.wo.lora_B.default.weight', 'language_model.base_model.model.model.layers.13.attention.wqkv.base_layer.weight', 'language_model.base_model.model.model.layers.13.attention.wqkv.lora_A.default.weight', 'language_model.base_model.model.model.layers.13.attention.wqkv.lora_B.default.weight', 'language_model.base_model.model.model.layers.13.attention_norm.weight', 'language_model.base_model.model.model.layers.13.feed_forward.w1.base_layer.weight', 'language_model.base_model.model.model.layers.13.feed_forward.w1.lora_A.default.weight', 'language_model.base_model.model.model.layers.13.feed_forward.w1.lora_B.default.weight', 'language_model.base_model.model.model.layers.13.feed_forward.w2.base_layer.weight', 'language_model.base_model.model.model.layers.13.feed_forward.w2.lora_A.default.weight', 'language_model.base_model.model.model.layers.13.feed_forward.w2.lora_B.default.weight', 'language_model.base_model.model.model.layers.13.feed_forward.w3.base_layer.weight', 'language_model.base_model.model.model.layers.13.feed_forward.w3.lora_A.default.weight', 'language_model.base_model.model.model.layers.13.feed_forward.w3.lora_B.default.weight', 'language_model.base_model.model.model.layers.13.ffn_norm.weight', 'language_model.base_model.model.model.layers.14.attention.wo.base_layer.weight', 'language_model.base_model.model.model.layers.14.attention.wo.lora_A.default.weight', 'language_model.base_model.model.model.layers.14.attention.wo.lora_B.default.weight', 'language_model.base_model.model.model.layers.14.attention.wqkv.base_layer.weight', 'language_model.base_model.model.model.layers.14.attention.wqkv.lora_A.default.weight', 'language_model.base_model.model.model.layers.14.attention.wqkv.lora_B.default.weight', 'language_model.base_model.model.model.layers.14.attention_norm.weight', 'language_model.base_model.model.model.layers.14.feed_forward.w1.base_layer.weight', 'language_model.base_model.model.model.layers.14.feed_forward.w1.lora_A.default.weight', 'language_model.base_model.model.model.layers.14.feed_forward.w1.lora_B.default.weight', 'language_model.base_model.model.model.layers.14.feed_forward.w2.base_layer.weight', 'language_model.base_model.model.model.layers.14.feed_forward.w2.lora_A.default.weight', 'language_model.base_model.model.model.layers.14.feed_forward.w2.lora_B.default.weight', 'language_model.base_model.model.model.layers.14.feed_forward.w3.base_layer.weight', 'language_model.base_model.model.model.layers.14.feed_forward.w3.lora_A.default.weight', 'language_model.base_model.model.model.layers.14.feed_forward.w3.lora_B.default.weight', 'language_model.base_model.model.model.layers.14.ffn_norm.weight', 'language_model.base_model.model.model.layers.15.attention.wo.base_layer.weight', 'language_model.base_model.model.model.layers.15.attention.wo.lora_A.default.weight', 'language_model.base_model.model.model.layers.15.attention.wo.lora_B.default.weight', 'language_model.base_model.model.model.layers.15.attention.wqkv.base_layer.weight', 'language_model.base_model.model.model.layers.15.attention.wqkv.lora_A.default.weight', 'language_model.base_model.model.model.layers.15.attention.wqkv.lora_B.default.weight', 'language_model.base_model.model.model.layers.15.attention_norm.weight', 'language_model.base_model.model.model.layers.15.feed_forward.w1.base_layer.weight', 'language_model.base_model.model.model.layers.15.feed_forward.w1.lora_A.default.weight', 'language_model.base_model.model.model.layers.15.feed_forward.w1.lora_B.default.weight', 'language_model.base_model.model.model.layers.15.feed_forward.w2.base_layer.weight', 'language_model.base_model.model.model.layers.15.feed_forward.w2.lora_A.default.weight', 'language_model.base_model.model.model.layers.15.feed_forward.w2.lora_B.default.weight', 'language_model.base_model.model.model.layers.15.feed_forward.w3.base_layer.weight', 'language_model.base_model.model.model.layers.15.feed_forward.w3.lora_A.default.weight', 'language_model.base_model.model.model.layers.15.feed_forward.w3.lora_B.default.weight', 'language_model.base_model.model.model.layers.15.ffn_norm.weight', 'language_model.base_model.model.model.layers.16.attention.wo.base_layer.weight', 'language_model.base_model.model.model.layers.16.attention.wo.lora_A.default.weight', 'language_model.base_model.model.model.layers.16.attention.wo.lora_B.default.weight', 'language_model.base_model.model.model.layers.16.attention.wqkv.base_layer.weight', 'language_model.base_model.model.model.layers.16.attention.wqkv.lora_A.default.weight', 'language_model.base_model.model.model.layers.16.attention.wqkv.lora_B.default.weight', 'language_model.base_model.model.model.layers.16.attention_norm.weight', 'language_model.base_model.model.model.layers.16.feed_forward.w1.base_layer.weight', 'language_model.base_model.model.model.layers.16.feed_forward.w1.lora_A.default.weight', 'language_model.base_model.model.model.layers.16.feed_forward.w1.lora_B.default.weight', 'language_model.base_model.model.model.layers.16.feed_forward.w2.base_layer.weight', 'language_model.base_model.model.model.layers.16.feed_forward.w2.lora_A.default.weight', 'language_model.base_model.model.model.layers.16.feed_forward.w2.lora_B.default.weight', 'language_model.base_model.model.model.layers.16.feed_forward.w3.base_layer.weight', 'language_model.base_model.model.model.layers.16.feed_forward.w3.lora_A.default.weight', 'language_model.base_model.model.model.layers.16.feed_forward.w3.lora_B.default.weight', 'language_model.base_model.model.model.layers.16.ffn_norm.weight', 'language_model.base_model.model.model.layers.17.attention.wo.base_layer.weight', 'language_model.base_model.model.model.layers.17.attention.wo.lora_A.default.weight', 'language_model.base_model.model.model.layers.17.attention.wo.lora_B.default.weight', 'language_model.base_model.model.model.layers.17.attention.wqkv.base_layer.weight', 'language_model.base_model.model.model.layers.17.attention.wqkv.lora_A.default.weight', 'language_model.base_model.model.model.layers.17.attention.wqkv.lora_B.default.weight', 'language_model.base_model.model.model.layers.17.attention_norm.weight', 'language_model.base_model.model.model.layers.17.feed_forward.w1.base_layer.weight', 'language_model.base_model.model.model.layers.17.feed_forward.w1.lora_A.default.weight', 'language_model.base_model.model.model.layers.17.feed_forward.w1.lora_B.default.weight', 'language_model.base_model.model.model.layers.17.feed_forward.w2.base_layer.weight', 'language_model.base_model.model.model.layers.17.feed_forward.w2.lora_A.default.weight', 'language_model.base_model.model.model.layers.17.feed_forward.w2.lora_B.default.weight', 'language_model.base_model.model.model.layers.17.feed_forward.w3.base_layer.weight', 'language_model.base_model.model.model.layers.17.feed_forward.w3.lora_A.default.weight', 'language_model.base_model.model.model.layers.17.feed_forward.w3.lora_B.default.weight', 'language_model.base_model.model.model.layers.17.ffn_norm.weight', 'language_model.base_model.model.model.layers.18.attention.wo.base_layer.weight', 'language_model.base_model.model.model.layers.18.attention.wo.lora_A.default.weight', 'language_model.base_model.model.model.layers.18.attention.wo.lora_B.default.weight', 'language_model.base_model.model.model.layers.18.attention.wqkv.base_layer.weight', 'language_model.base_model.model.model.layers.18.attention.wqkv.lora_A.default.weight', 'language_model.base_model.model.model.layers.18.attention.wqkv.lora_B.default.weight', 'language_model.base_model.model.model.layers.18.attention_norm.weight', 'language_model.base_model.model.model.layers.18.feed_forward.w1.base_layer.weight', 'language_model.base_model.model.model.layers.18.feed_forward.w1.lora_A.default.weight', 'language_model.base_model.model.model.layers.18.feed_forward.w1.lora_B.default.weight', 'language_model.base_model.model.model.layers.18.feed_forward.w2.base_layer.weight', 'language_model.base_model.model.model.layers.18.feed_forward.w2.lora_A.default.weight', 'language_model.base_model.model.model.layers.18.feed_forward.w2.lora_B.default.weight', 'language_model.base_model.model.model.layers.18.feed_forward.w3.base_layer.weight', 'language_model.base_model.model.model.layers.18.feed_forward.w3.lora_A.default.weight', 'language_model.base_model.model.model.layers.18.feed_forward.w3.lora_B.default.weight', 'language_model.base_model.model.model.layers.18.ffn_norm.weight', 'language_model.base_model.model.model.layers.19.attention.wo.base_layer.weight', 'language_model.base_model.model.model.layers.19.attention.wo.lora_A.default.weight', 'language_model.base_model.model.model.layers.19.attention.wo.lora_B.default.weight', 'language_model.base_model.model.model.layers.19.attention.wqkv.base_layer.weight', 'language_model.base_model.model.model.layers.19.attention.wqkv.lora_A.default.weight', 'language_model.base_model.model.model.layers.19.attention.wqkv.lora_B.default.weight', 'language_model.base_model.model.model.layers.19.attention_norm.weight', 'language_model.base_model.model.model.layers.19.feed_forward.w1.base_layer.weight', 'language_model.base_model.model.model.layers.19.feed_forward.w1.lora_A.default.weight', 'language_model.base_model.model.model.layers.19.feed_forward.w1.lora_B.default.weight', 'language_model.base_model.model.model.layers.19.feed_forward.w2.base_layer.weight', 'language_model.base_model.model.model.layers.19.feed_forward.w2.lora_A.default.weight', 'language_model.base_model.model.model.layers.19.feed_forward.w2.lora_B.default.weight', 'language_model.base_model.model.model.layers.19.feed_forward.w3.base_layer.weight', 'language_model.base_model.model.model.layers.19.feed_forward.w3.lora_A.default.weight', 'language_model.base_model.model.model.layers.19.feed_forward.w3.lora_B.default.weight', 'language_model.base_model.model.model.layers.19.ffn_norm.weight', 'language_model.base_model.model.model.layers.2.attention.wo.base_layer.weight', 'language_model.base_model.model.model.layers.2.attention.wo.lora_A.default.weight', 'language_model.base_model.model.model.layers.2.attention.wo.lora_B.default.weight', 'language_model.base_model.model.model.layers.2.attention.wqkv.base_layer.weight', 'language_model.base_model.model.model.layers.2.attention.wqkv.lora_A.default.weight', 'language_model.base_model.model.model.layers.2.attention.wqkv.lora_B.default.weight', 'language_model.base_model.model.model.layers.2.attention_norm.weight', 'language_model.base_model.model.model.layers.2.feed_forward.w1.base_layer.weight', 'language_model.base_model.model.model.layers.2.feed_forward.w1.lora_A.default.weight', 'language_model.base_model.model.model.layers.2.feed_forward.w1.lora_B.default.weight', 'language_model.base_model.model.model.layers.2.feed_forward.w2.base_layer.weight', 'language_model.base_model.model.model.layers.2.feed_forward.w2.lora_A.default.weight', 'language_model.base_model.model.model.layers.2.feed_forward.w2.lora_B.default.weight', 'language_model.base_model.model.model.layers.2.feed_forward.w3.base_layer.weight', 'language_model.base_model.model.model.layers.2.feed_forward.w3.lora_A.default.weight', 'language_model.base_model.model.model.layers.2.feed_forward.w3.lora_B.default.weight', 'language_model.base_model.model.model.layers.2.ffn_norm.weight', 'language_model.base_model.model.model.layers.20.attention.wo.base_layer.weight', 'language_model.base_model.model.model.layers.20.attention.wo.lora_A.default.weight', 'language_model.base_model.model.model.layers.20.attention.wo.lora_B.default.weight', 'language_model.base_model.model.model.layers.20.attention.wqkv.base_layer.weight', 'language_model.base_model.model.model.layers.20.attention.wqkv.lora_A.default.weight', 'language_model.base_model.model.model.layers.20.attention.wqkv.lora_B.default.weight', 'language_model.base_model.model.model.layers.20.attention_norm.weight', 'language_model.base_model.model.model.layers.20.feed_forward.w1.base_layer.weight', 'language_model.base_model.model.model.layers.20.feed_forward.w1.lora_A.default.weight', 'language_model.base_model.model.model.layers.20.feed_forward.w1.lora_B.default.weight', 'language_model.base_model.model.model.layers.20.feed_forward.w2.base_layer.weight', 'language_model.base_model.model.model.layers.20.feed_forward.w2.lora_A.default.weight', 'language_model.base_model.model.model.layers.20.feed_forward.w2.lora_B.default.weight', 'language_model.base_model.model.model.layers.20.feed_forward.w3.base_layer.weight', 'language_model.base_model.model.model.layers.20.feed_forward.w3.lora_A.default.weight', 'language_model.base_model.model.model.layers.20.feed_forward.w3.lora_B.default.weight', 'language_model.base_model.model.model.layers.20.ffn_norm.weight', 'language_model.base_model.model.model.layers.21.attention.wo.base_layer.weight', 'language_model.base_model.model.model.layers.21.attention.wo.lora_A.default.weight', 'language_model.base_model.model.model.layers.21.attention.wo.lora_B.default.weight', 'language_model.base_model.model.model.layers.21.attention.wqkv.base_layer.weight', 'language_model.base_model.model.model.layers.21.attention.wqkv.lora_A.default.weight', 'language_model.base_model.model.model.layers.21.attention.wqkv.lora_B.default.weight', 'language_model.base_model.model.model.layers.21.attention_norm.weight', 'language_model.base_model.model.model.layers.21.feed_forward.w1.base_layer.weight', 'language_model.base_model.model.model.layers.21.feed_forward.w1.lora_A.default.weight', 'language_model.base_model.model.model.layers.21.feed_forward.w1.lora_B.default.weight', 'language_model.base_model.model.model.layers.21.feed_forward.w2.base_layer.weight', 'language_model.base_model.model.model.layers.21.feed_forward.w2.lora_A.default.weight', 'language_model.base_model.model.model.layers.21.feed_forward.w2.lora_B.default.weight', 'language_model.base_model.model.model.layers.21.feed_forward.w3.base_layer.weight', 'language_model.base_model.model.model.layers.21.feed_forward.w3.lora_A.default.weight', 'language_model.base_model.model.model.layers.21.feed_forward.w3.lora_B.default.weight', 'language_model.base_model.model.model.layers.21.ffn_norm.weight', 'language_model.base_model.model.model.layers.22.attention.wo.base_layer.weight', 'language_model.base_model.model.model.layers.22.attention.wo.lora_A.default.weight', 'language_model.base_model.model.model.layers.22.attention.wo.lora_B.default.weight', 'language_model.base_model.model.model.layers.22.attention.wqkv.base_layer.weight', 'language_model.base_model.model.model.layers.22.attention.wqkv.lora_A.default.weight', 'language_model.base_model.model.model.layers.22.attention.wqkv.lora_B.default.weight', 'language_model.base_model.model.model.layers.22.attention_norm.weight', 'language_model.base_model.model.model.layers.22.feed_forward.w1.base_layer.weight', 'language_model.base_model.model.model.layers.22.feed_forward.w1.lora_A.default.weight', 'language_model.base_model.model.model.layers.22.feed_forward.w1.lora_B.default.weight', 'language_model.base_model.model.model.layers.22.feed_forward.w2.base_layer.weight', 'language_model.base_model.model.model.layers.22.feed_forward.w2.lora_A.default.weight', 'language_model.base_model.model.model.layers.22.feed_forward.w2.lora_B.default.weight', 'language_model.base_model.model.model.layers.22.feed_forward.w3.base_layer.weight', 'language_model.base_model.model.model.layers.22.feed_forward.w3.lora_A.default.weight', 'language_model.base_model.model.model.layers.22.feed_forward.w3.lora_B.default.weight', 'language_model.base_model.model.model.layers.22.ffn_norm.weight', 'language_model.base_model.model.model.layers.23.attention.wo.base_layer.weight', 'language_model.base_model.model.model.layers.23.attention.wo.lora_A.default.weight', 'language_model.base_model.model.model.layers.23.attention.wo.lora_B.default.weight', 'language_model.base_model.model.model.layers.23.attention.wqkv.base_layer.weight', 'language_model.base_model.model.model.layers.23.attention.wqkv.lora_A.default.weight', 'language_model.base_model.model.model.layers.23.attention.wqkv.lora_B.default.weight', 'language_model.base_model.model.model.layers.23.attention_norm.weight', 'language_model.base_model.model.model.layers.23.feed_forward.w1.base_layer.weight', 'language_model.base_model.model.model.layers.23.feed_forward.w1.lora_A.default.weight', 'language_model.base_model.model.model.layers.23.feed_forward.w1.lora_B.default.weight', 'language_model.base_model.model.model.layers.23.feed_forward.w2.base_layer.weight', 'language_model.base_model.model.model.layers.23.feed_forward.w2.lora_A.default.weight', 'language_model.base_model.model.model.layers.23.feed_forward.w2.lora_B.default.weight', 'language_model.base_model.model.model.layers.23.feed_forward.w3.base_layer.weight', 'language_model.base_model.model.model.layers.23.feed_forward.w3.lora_A.default.weight', 'language_model.base_model.model.model.layers.23.feed_forward.w3.lora_B.default.weight', 'language_model.base_model.model.model.layers.23.ffn_norm.weight', 'language_model.base_model.model.model.layers.24.attention.wo.base_layer.weight', 'language_model.base_model.model.model.layers.24.attention.wo.lora_A.default.weight', 'language_model.base_model.model.model.layers.24.attention.wo.lora_B.default.weight', 'language_model.base_model.model.model.layers.24.attention.wqkv.base_layer.weight', 'language_model.base_model.model.model.layers.24.attention.wqkv.lora_A.default.weight', 'language_model.base_model.model.model.layers.24.attention.wqkv.lora_B.default.weight', 'language_model.base_model.model.model.layers.24.attention_norm.weight', 'language_model.base_model.model.model.layers.24.feed_forward.w1.base_layer.weight', 'language_model.base_model.model.model.layers.24.feed_forward.w1.lora_A.default.weight', 'language_model.base_model.model.model.layers.24.feed_forward.w1.lora_B.default.weight', 'language_model.base_model.model.model.layers.24.feed_forward.w2.base_layer.weight', 'language_model.base_model.model.model.layers.24.feed_forward.w2.lora_A.default.weight', 'language_model.base_model.model.model.layers.24.feed_forward.w2.lora_B.default.weight', 'language_model.base_model.model.model.layers.24.feed_forward.w3.base_layer.weight', 'language_model.base_model.model.model.layers.24.feed_forward.w3.lora_A.default.weight', 'language_model.base_model.model.model.layers.24.feed_forward.w3.lora_B.default.weight', 'language_model.base_model.model.model.layers.24.ffn_norm.weight', 'language_model.base_model.model.model.layers.25.attention.wo.base_layer.weight', 'language_model.base_model.model.model.layers.25.attention.wo.lora_A.default.weight', 'language_model.base_model.model.model.layers.25.attention.wo.lora_B.default.weight', 'language_model.base_model.model.model.layers.25.attention.wqkv.base_layer.weight', 'language_model.base_model.model.model.layers.25.attention.wqkv.lora_A.default.weight', 'language_model.base_model.model.model.layers.25.attention.wqkv.lora_B.default.weight', 'language_model.base_model.model.model.layers.25.attention_norm.weight', 'language_model.base_model.model.model.layers.25.feed_forward.w1.base_layer.weight', 'language_model.base_model.model.model.layers.25.feed_forward.w1.lora_A.default.weight', 'language_model.base_model.model.model.layers.25.feed_forward.w1.lora_B.default.weight', 'language_model.base_model.model.model.layers.25.feed_forward.w2.base_layer.weight', 'language_model.base_model.model.model.layers.25.feed_forward.w2.lora_A.default.weight', 'language_model.base_model.model.model.layers.25.feed_forward.w2.lora_B.default.weight', 'language_model.base_model.model.model.layers.25.feed_forward.w3.base_layer.weight', 'language_model.base_model.model.model.layers.25.feed_forward.w3.lora_A.default.weight', 'language_model.base_model.model.model.layers.25.feed_forward.w3.lora_B.default.weight', 'language_model.base_model.model.model.layers.25.ffn_norm.weight', 'language_model.base_model.model.model.layers.26.attention.wo.base_layer.weight', 'language_model.base_model.model.model.layers.26.attention.wo.lora_A.default.weight', 'language_model.base_model.model.model.layers.26.attention.wo.lora_B.default.weight', 'language_model.base_model.model.model.layers.26.attention.wqkv.base_layer.weight', 'language_model.base_model.model.model.layers.26.attention.wqkv.lora_A.default.weight', 'language_model.base_model.model.model.layers.26.attention.wqkv.lora_B.default.weight', 'language_model.base_model.model.model.layers.26.attention_norm.weight', 'language_model.base_model.model.model.layers.26.feed_forward.w1.base_layer.weight', 'language_model.base_model.model.model.layers.26.feed_forward.w1.lora_A.default.weight', 'language_model.base_model.model.model.layers.26.feed_forward.w1.lora_B.default.weight', 'language_model.base_model.model.model.layers.26.feed_forward.w2.base_layer.weight', 'language_model.base_model.model.model.layers.26.feed_forward.w2.lora_A.default.weight', 'language_model.base_model.model.model.layers.26.feed_forward.w2.lora_B.default.weight', 'language_model.base_model.model.model.layers.26.feed_forward.w3.base_layer.weight', 'language_model.base_model.model.model.layers.26.feed_forward.w3.lora_A.default.weight', 'language_model.base_model.model.model.layers.26.feed_forward.w3.lora_B.default.weight', 'language_model.base_model.model.model.layers.26.ffn_norm.weight', 'language_model.base_model.model.model.layers.27.attention.wo.base_layer.weight', 'language_model.base_model.model.model.layers.27.attention.wo.lora_A.default.weight', 'language_model.base_model.model.model.layers.27.attention.wo.lora_B.default.weight', 'language_model.base_model.model.model.layers.27.attention.wqkv.base_layer.weight', 'language_model.base_model.model.model.layers.27.attention.wqkv.lora_A.default.weight', 'language_model.base_model.model.model.layers.27.attention.wqkv.lora_B.default.weight', 'language_model.base_model.model.model.layers.27.attention_norm.weight', 'language_model.base_model.model.model.layers.27.feed_forward.w1.base_layer.weight', 'language_model.base_model.model.model.layers.27.feed_forward.w1.lora_A.default.weight', 'language_model.base_model.model.model.layers.27.feed_forward.w1.lora_B.default.weight', 'language_model.base_model.model.model.layers.27.feed_forward.w2.base_layer.weight', 'language_model.base_model.model.model.layers.27.feed_forward.w2.lora_A.default.weight', 'language_model.base_model.model.model.layers.27.feed_forward.w2.lora_B.default.weight', 'language_model.base_model.model.model.layers.27.feed_forward.w3.base_layer.weight', 'language_model.base_model.model.model.layers.27.feed_forward.w3.lora_A.default.weight', 'language_model.base_model.model.model.layers.27.feed_forward.w3.lora_B.default.weight', 'language_model.base_model.model.model.layers.27.ffn_norm.weight', 'language_model.base_model.model.model.layers.28.attention.wo.base_layer.weight', 'language_model.base_model.model.model.layers.28.attention.wo.lora_A.default.weight', 'language_model.base_model.model.model.layers.28.attention.wo.lora_B.default.weight', 'language_model.base_model.model.model.layers.28.attention.wqkv.base_layer.weight', 'language_model.base_model.model.model.layers.28.attention.wqkv.lora_A.default.weight', 'language_model.base_model.model.model.layers.28.attention.wqkv.lora_B.default.weight', 'language_model.base_model.model.model.layers.28.attention_norm.weight', 'language_model.base_model.model.model.layers.28.feed_forward.w1.base_layer.weight', 'language_model.base_model.model.model.layers.28.feed_forward.w1.lora_A.default.weight', 'language_model.base_model.model.model.layers.28.feed_forward.w1.lora_B.default.weight', 'language_model.base_model.model.model.layers.28.feed_forward.w2.base_layer.weight', 'language_model.base_model.model.model.layers.28.feed_forward.w2.lora_A.default.weight', 'language_model.base_model.model.model.layers.28.feed_forward.w2.lora_B.default.weight', 'language_model.base_model.model.model.layers.28.feed_forward.w3.base_layer.weight', 'language_model.base_model.model.model.layers.28.feed_forward.w3.lora_A.default.weight', 'language_model.base_model.model.model.layers.28.feed_forward.w3.lora_B.default.weight', 'language_model.base_model.model.model.layers.28.ffn_norm.weight', 'language_model.base_model.model.model.layers.29.attention.wo.base_layer.weight', 'language_model.base_model.model.model.layers.29.attention.wo.lora_A.default.weight', 'language_model.base_model.model.model.layers.29.attention.wo.lora_B.default.weight', 'language_model.base_model.model.model.layers.29.attention.wqkv.base_layer.weight', 'language_model.base_model.model.model.layers.29.attention.wqkv.lora_A.default.weight', 'language_model.base_model.model.model.layers.29.attention.wqkv.lora_B.default.weight', 'language_model.base_model.model.model.layers.29.attention_norm.weight', 'language_model.base_model.model.model.layers.29.feed_forward.w1.base_layer.weight', 'language_model.base_model.model.model.layers.29.feed_forward.w1.lora_A.default.weight', 'language_model.base_model.model.model.layers.29.feed_forward.w1.lora_B.default.weight', 'language_model.base_model.model.model.layers.29.feed_forward.w2.base_layer.weight', 'language_model.base_model.model.model.layers.29.feed_forward.w2.lora_A.default.weight', 'language_model.base_model.model.model.layers.29.feed_forward.w2.lora_B.default.weight', 'language_model.base_model.model.model.layers.29.feed_forward.w3.base_layer.weight', 'language_model.base_model.model.model.layers.29.feed_forward.w3.lora_A.default.weight', 'language_model.base_model.model.model.layers.29.feed_forward.w3.lora_B.default.weight', 'language_model.base_model.model.model.layers.29.ffn_norm.weight', 'language_model.base_model.model.model.layers.3.attention.wo.base_layer.weight', 'language_model.base_model.model.model.layers.3.attention.wo.lora_A.default.weight', 'language_model.base_model.model.model.layers.3.attention.wo.lora_B.default.weight', 'language_model.base_model.model.model.layers.3.attention.wqkv.base_layer.weight', 'language_model.base_model.model.model.layers.3.attention.wqkv.lora_A.default.weight', 'language_model.base_model.model.model.layers.3.attention.wqkv.lora_B.default.weight', 'language_model.base_model.model.model.layers.3.attention_norm.weight', 'language_model.base_model.model.model.layers.3.feed_forward.w1.base_layer.weight', 'language_model.base_model.model.model.layers.3.feed_forward.w1.lora_A.default.weight', 'language_model.base_model.model.model.layers.3.feed_forward.w1.lora_B.default.weight', 'language_model.base_model.model.model.layers.3.feed_forward.w2.base_layer.weight', 'language_model.base_model.model.model.layers.3.feed_forward.w2.lora_A.default.weight', 'language_model.base_model.model.model.layers.3.feed_forward.w2.lora_B.default.weight', 'language_model.base_model.model.model.layers.3.feed_forward.w3.base_layer.weight', 'language_model.base_model.model.model.layers.3.feed_forward.w3.lora_A.default.weight', 'language_model.base_model.model.model.layers.3.feed_forward.w3.lora_B.default.weight', 'language_model.base_model.model.model.layers.3.ffn_norm.weight', 'language_model.base_model.model.model.layers.30.attention.wo.base_layer.weight', 'language_model.base_model.model.model.layers.30.attention.wo.lora_A.default.weight', 'language_model.base_model.model.model.layers.30.attention.wo.lora_B.default.weight', 'language_model.base_model.model.model.layers.30.attention.wqkv.base_layer.weight', 'language_model.base_model.model.model.layers.30.attention.wqkv.lora_A.default.weight', 'language_model.base_model.model.model.layers.30.attention.wqkv.lora_B.default.weight', 'language_model.base_model.model.model.layers.30.attention_norm.weight', 'language_model.base_model.model.model.layers.30.feed_forward.w1.base_layer.weight', 'language_model.base_model.model.model.layers.30.feed_forward.w1.lora_A.default.weight', 'language_model.base_model.model.model.layers.30.feed_forward.w1.lora_B.default.weight', 'language_model.base_model.model.model.layers.30.feed_forward.w2.base_layer.weight', 'language_model.base_model.model.model.layers.30.feed_forward.w2.lora_A.default.weight', 'language_model.base_model.model.model.layers.30.feed_forward.w2.lora_B.default.weight', 'language_model.base_model.model.model.layers.30.feed_forward.w3.base_layer.weight', 'language_model.base_model.model.model.layers.30.feed_forward.w3.lora_A.default.weight', 'language_model.base_model.model.model.layers.30.feed_forward.w3.lora_B.default.weight', 'language_model.base_model.model.model.layers.30.ffn_norm.weight', 'language_model.base_model.model.model.layers.31.attention.wo.base_layer.weight', 'language_model.base_model.model.model.layers.31.attention.wo.lora_A.default.weight', 'language_model.base_model.model.model.layers.31.attention.wo.lora_B.default.weight', 'language_model.base_model.model.model.layers.31.attention.wqkv.base_layer.weight', 'language_model.base_model.model.model.layers.31.attention.wqkv.lora_A.default.weight', 'language_model.base_model.model.model.layers.31.attention.wqkv.lora_B.default.weight', 'language_model.base_model.model.model.layers.31.attention_norm.weight', 'language_model.base_model.model.model.layers.31.feed_forward.w1.base_layer.weight', 'language_model.base_model.model.model.layers.31.feed_forward.w1.lora_A.default.weight', 'language_model.base_model.model.model.layers.31.feed_forward.w1.lora_B.default.weight', 'language_model.base_model.model.model.layers.31.feed_forward.w2.base_layer.weight', 'language_model.base_model.model.model.layers.31.feed_forward.w2.lora_A.default.weight', 'language_model.base_model.model.model.layers.31.feed_forward.w2.lora_B.default.weight', 'language_model.base_model.model.model.layers.31.feed_forward.w3.base_layer.weight', 'language_model.base_model.model.model.layers.31.feed_forward.w3.lora_A.default.weight', 'language_model.base_model.model.model.layers.31.feed_forward.w3.lora_B.default.weight', 'language_model.base_model.model.model.layers.31.ffn_norm.weight', 'language_model.base_model.model.model.layers.32.attention.wo.base_layer.weight', 'language_model.base_model.model.model.layers.32.attention.wo.lora_A.default.weight', 'language_model.base_model.model.model.layers.32.attention.wo.lora_B.default.weight', 'language_model.base_model.model.model.layers.32.attention.wqkv.base_layer.weight', 'language_model.base_model.model.model.layers.32.attention.wqkv.lora_A.default.weight', 'language_model.base_model.model.model.layers.32.attention.wqkv.lora_B.default.weight', 'language_model.base_model.model.model.layers.32.attention_norm.weight', 'language_model.base_model.model.model.layers.32.feed_forward.w1.base_layer.weight', 'language_model.base_model.model.model.layers.32.feed_forward.w1.lora_A.default.weight', 'language_model.base_model.model.model.layers.32.feed_forward.w1.lora_B.default.weight', 'language_model.base_model.model.model.layers.32.feed_forward.w2.base_layer.weight', 'language_model.base_model.model.model.layers.32.feed_forward.w2.lora_A.default.weight', 'language_model.base_model.model.model.layers.32.feed_forward.w2.lora_B.default.weight', 'language_model.base_model.model.model.layers.32.feed_forward.w3.base_layer.weight', 'language_model.base_model.model.model.layers.32.feed_forward.w3.lora_A.default.weight', 'language_model.base_model.model.model.layers.32.feed_forward.w3.lora_B.default.weight', 'language_model.base_model.model.model.layers.32.ffn_norm.weight', 'language_model.base_model.model.model.layers.33.attention.wo.base_layer.weight', 'language_model.base_model.model.model.layers.33.attention.wo.lora_A.default.weight', 'language_model.base_model.model.model.layers.33.attention.wo.lora_B.default.weight', 'language_model.base_model.model.model.layers.33.attention.wqkv.base_layer.weight', 'language_model.base_model.model.model.layers.33.attention.wqkv.lora_A.default.weight', 'language_model.base_model.model.model.layers.33.attention.wqkv.lora_B.default.weight', 'language_model.base_model.model.model.layers.33.attention_norm.weight', 'language_model.base_model.model.model.layers.33.feed_forward.w1.base_layer.weight', 'language_model.base_model.model.model.layers.33.feed_forward.w1.lora_A.default.weight', 'language_model.base_model.model.model.layers.33.feed_forward.w1.lora_B.default.weight', 'language_model.base_model.model.model.layers.33.feed_forward.w2.base_layer.weight', 'language_model.base_model.model.model.layers.33.feed_forward.w2.lora_A.default.weight', 'language_model.base_model.model.model.layers.33.feed_forward.w2.lora_B.default.weight', 'language_model.base_model.model.model.layers.33.feed_forward.w3.base_layer.weight', 'language_model.base_model.model.model.layers.33.feed_forward.w3.lora_A.default.weight', 'language_model.base_model.model.model.layers.33.feed_forward.w3.lora_B.default.weight', 'language_model.base_model.model.model.layers.33.ffn_norm.weight', 'language_model.base_model.model.model.layers.34.attention.wo.base_layer.weight', 'language_model.base_model.model.model.layers.34.attention.wo.lora_A.default.weight', 'language_model.base_model.model.model.layers.34.attention.wo.lora_B.default.weight', 'language_model.base_model.model.model.layers.34.attention.wqkv.base_layer.weight', 'language_model.base_model.model.model.layers.34.attention.wqkv.lora_A.default.weight', 'language_model.base_model.model.model.layers.34.attention.wqkv.lora_B.default.weight', 'language_model.base_model.model.model.layers.34.attention_norm.weight', 'language_model.base_model.model.model.layers.34.feed_forward.w1.base_layer.weight', 'language_model.base_model.model.model.layers.34.feed_forward.w1.lora_A.default.weight', 'language_model.base_model.model.model.layers.34.feed_forward.w1.lora_B.default.weight', 'language_model.base_model.model.model.layers.34.feed_forward.w2.base_layer.weight', 'language_model.base_model.model.model.layers.34.feed_forward.w2.lora_A.default.weight', 'language_model.base_model.model.model.layers.34.feed_forward.w2.lora_B.default.weight', 'language_model.base_model.model.model.layers.34.feed_forward.w3.base_layer.weight', 'language_model.base_model.model.model.layers.34.feed_forward.w3.lora_A.default.weight', 'language_model.base_model.model.model.layers.34.feed_forward.w3.lora_B.default.weight', 'language_model.base_model.model.model.layers.34.ffn_norm.weight', 'language_model.base_model.model.model.layers.35.attention.wo.base_layer.weight', 'language_model.base_model.model.model.layers.35.attention.wo.lora_A.default.weight', 'language_model.base_model.model.model.layers.35.attention.wo.lora_B.default.weight', 'language_model.base_model.model.model.layers.35.attention.wqkv.base_layer.weight', 'language_model.base_model.model.model.layers.35.attention.wqkv.lora_A.default.weight', 'language_model.base_model.model.model.layers.35.attention.wqkv.lora_B.default.weight', 'language_model.base_model.model.model.layers.35.attention_norm.weight', 'language_model.base_model.model.model.layers.35.feed_forward.w1.base_layer.weight', 'language_model.base_model.model.model.layers.35.feed_forward.w1.lora_A.default.weight', 'language_model.base_model.model.model.layers.35.feed_forward.w1.lora_B.default.weight', 'language_model.base_model.model.model.layers.35.feed_forward.w2.base_layer.weight', 'language_model.base_model.model.model.layers.35.feed_forward.w2.lora_A.default.weight', 'language_model.base_model.model.model.layers.35.feed_forward.w2.lora_B.default.weight', 'language_model.base_model.model.model.layers.35.feed_forward.w3.base_layer.weight', 'language_model.base_model.model.model.layers.35.feed_forward.w3.lora_A.default.weight', 'language_model.base_model.model.model.layers.35.feed_forward.w3.lora_B.default.weight', 'language_model.base_model.model.model.layers.35.ffn_norm.weight', 'language_model.base_model.model.model.layers.36.attention.wo.base_layer.weight', 'language_model.base_model.model.model.layers.36.attention.wo.lora_A.default.weight', 'language_model.base_model.model.model.layers.36.attention.wo.lora_B.default.weight', 'language_model.base_model.model.model.layers.36.attention.wqkv.base_layer.weight', 'language_model.base_model.model.model.layers.36.attention.wqkv.lora_A.default.weight', 'language_model.base_model.model.model.layers.36.attention.wqkv.lora_B.default.weight', 'language_model.base_model.model.model.layers.36.attention_norm.weight', 'language_model.base_model.model.model.layers.36.feed_forward.w1.base_layer.weight', 'language_model.base_model.model.model.layers.36.feed_forward.w1.lora_A.default.weight', 'language_model.base_model.model.model.layers.36.feed_forward.w1.lora_B.default.weight', 'language_model.base_model.model.model.layers.36.feed_forward.w2.base_layer.weight', 'language_model.base_model.model.model.layers.36.feed_forward.w2.lora_A.default.weight', 'language_model.base_model.model.model.layers.36.feed_forward.w2.lora_B.default.weight', 'language_model.base_model.model.model.layers.36.feed_forward.w3.base_layer.weight', 'language_model.base_model.model.model.layers.36.feed_forward.w3.lora_A.default.weight', 'language_model.base_model.model.model.layers.36.feed_forward.w3.lora_B.default.weight', 'language_model.base_model.model.model.layers.36.ffn_norm.weight', 'language_model.base_model.model.model.layers.37.attention.wo.base_layer.weight', 'language_model.base_model.model.model.layers.37.attention.wo.lora_A.default.weight', 'language_model.base_model.model.model.layers.37.attention.wo.lora_B.default.weight', 'language_model.base_model.model.model.layers.37.attention.wqkv.base_layer.weight', 'language_model.base_model.model.model.layers.37.attention.wqkv.lora_A.default.weight', 'language_model.base_model.model.model.layers.37.attention.wqkv.lora_B.default.weight', 'language_model.base_model.model.model.layers.37.attention_norm.weight', 'language_model.base_model.model.model.layers.37.feed_forward.w1.base_layer.weight', 'language_model.base_model.model.model.layers.37.feed_forward.w1.lora_A.default.weight', 'language_model.base_model.model.model.layers.37.feed_forward.w1.lora_B.default.weight', 'language_model.base_model.model.model.layers.37.feed_forward.w2.base_layer.weight', 'language_model.base_model.model.model.layers.37.feed_forward.w2.lora_A.default.weight', 'language_model.base_model.model.model.layers.37.feed_forward.w2.lora_B.default.weight', 'language_model.base_model.model.model.layers.37.feed_forward.w3.base_layer.weight', 'language_model.base_model.model.model.layers.37.feed_forward.w3.lora_A.default.weight', 'language_model.base_model.model.model.layers.37.feed_forward.w3.lora_B.default.weight', 'language_model.base_model.model.model.layers.37.ffn_norm.weight', 'language_model.base_model.model.model.layers.38.attention.wo.base_layer.weight', 'language_model.base_model.model.model.layers.38.attention.wo.lora_A.default.weight', 'language_model.base_model.model.model.layers.38.attention.wo.lora_B.default.weight', 'language_model.base_model.model.model.layers.38.attention.wqkv.base_layer.weight', 'language_model.base_model.model.model.layers.38.attention.wqkv.lora_A.default.weight', 'language_model.base_model.model.model.layers.38.attention.wqkv.lora_B.default.weight', 'language_model.base_model.model.model.layers.38.attention_norm.weight', 'language_model.base_model.model.model.layers.38.feed_forward.w1.base_layer.weight', 'language_model.base_model.model.model.layers.38.feed_forward.w1.lora_A.default.weight', 'language_model.base_model.model.model.layers.38.feed_forward.w1.lora_B.default.weight', 'language_model.base_model.model.model.layers.38.feed_forward.w2.base_layer.weight', 'language_model.base_model.model.model.layers.38.feed_forward.w2.lora_A.default.weight', 'language_model.base_model.model.model.layers.38.feed_forward.w2.lora_B.default.weight', 'language_model.base_model.model.model.layers.38.feed_forward.w3.base_layer.weight', 'language_model.base_model.model.model.layers.38.feed_forward.w3.lora_A.default.weight', 'language_model.base_model.model.model.layers.38.feed_forward.w3.lora_B.default.weight', 'language_model.base_model.model.model.layers.38.ffn_norm.weight', 'language_model.base_model.model.model.layers.39.attention.wo.base_layer.weight', 'language_model.base_model.model.model.layers.39.attention.wo.lora_A.default.weight', 'language_model.base_model.model.model.layers.39.attention.wo.lora_B.default.weight', 'language_model.base_model.model.model.layers.39.attention.wqkv.base_layer.weight', 'language_model.base_model.model.model.layers.39.attention.wqkv.lora_A.default.weight', 'language_model.base_model.model.model.layers.39.attention.wqkv.lora_B.default.weight', 'language_model.base_model.model.model.layers.39.attention_norm.weight', 'language_model.base_model.model.model.layers.39.feed_forward.w1.base_layer.weight', 'language_model.base_model.model.model.layers.39.feed_forward.w1.lora_A.default.weight', 'language_model.base_model.model.model.layers.39.feed_forward.w1.lora_B.default.weight', 'language_model.base_model.model.model.layers.39.feed_forward.w2.base_layer.weight', 'language_model.base_model.model.model.layers.39.feed_forward.w2.lora_A.default.weight', 'language_model.base_model.model.model.layers.39.feed_forward.w2.lora_B.default.weight', 'language_model.base_model.model.model.layers.39.feed_forward.w3.base_layer.weight', 'language_model.base_model.model.model.layers.39.feed_forward.w3.lora_A.default.weight', 'language_model.base_model.model.model.layers.39.feed_forward.w3.lora_B.default.weight', 'language_model.base_model.model.model.layers.39.ffn_norm.weight', 'language_model.base_model.model.model.layers.4.attention.wo.base_layer.weight', 'language_model.base_model.model.model.layers.4.attention.wo.lora_A.default.weight', 'language_model.base_model.model.model.layers.4.attention.wo.lora_B.default.weight', 'language_model.base_model.model.model.layers.4.attention.wqkv.base_layer.weight', 'language_model.base_model.model.model.layers.4.attention.wqkv.lora_A.default.weight', 'language_model.base_model.model.model.layers.4.attention.wqkv.lora_B.default.weight', 'language_model.base_model.model.model.layers.4.attention_norm.weight', 'language_model.base_model.model.model.layers.4.feed_forward.w1.base_layer.weight', 'language_model.base_model.model.model.layers.4.feed_forward.w1.lora_A.default.weight', 'language_model.base_model.model.model.layers.4.feed_forward.w1.lora_B.default.weight', 'language_model.base_model.model.model.layers.4.feed_forward.w2.base_layer.weight', 'language_model.base_model.model.model.layers.4.feed_forward.w2.lora_A.default.weight', 'language_model.base_model.model.model.layers.4.feed_forward.w2.lora_B.default.weight', 'language_model.base_model.model.model.layers.4.feed_forward.w3.base_layer.weight', 'language_model.base_model.model.model.layers.4.feed_forward.w3.lora_A.default.weight', 'language_model.base_model.model.model.layers.4.feed_forward.w3.lora_B.default.weight', 'language_model.base_model.model.model.layers.4.ffn_norm.weight', 'language_model.base_model.model.model.layers.40.attention.wo.base_layer.weight', 'language_model.base_model.model.model.layers.40.attention.wo.lora_A.default.weight', 'language_model.base_model.model.model.layers.40.attention.wo.lora_B.default.weight', 'language_model.base_model.model.model.layers.40.attention.wqkv.base_layer.weight', 'language_model.base_model.model.model.layers.40.attention.wqkv.lora_A.default.weight', 'language_model.base_model.model.model.layers.40.attention.wqkv.lora_B.default.weight', 'language_model.base_model.model.model.layers.40.attention_norm.weight', 'language_model.base_model.model.model.layers.40.feed_forward.w1.base_layer.weight', 'language_model.base_model.model.model.layers.40.feed_forward.w1.lora_A.default.weight', 'language_model.base_model.model.model.layers.40.feed_forward.w1.lora_B.default.weight', 'language_model.base_model.model.model.layers.40.feed_forward.w2.base_layer.weight', 'language_model.base_model.model.model.layers.40.feed_forward.w2.lora_A.default.weight', 'language_model.base_model.model.model.layers.40.feed_forward.w2.lora_B.default.weight', 'language_model.base_model.model.model.layers.40.feed_forward.w3.base_layer.weight', 'language_model.base_model.model.model.layers.40.feed_forward.w3.lora_A.default.weight', 'language_model.base_model.model.model.layers.40.feed_forward.w3.lora_B.default.weight', 'language_model.base_model.model.model.layers.40.ffn_norm.weight', 'language_model.base_model.model.model.layers.41.attention.wo.base_layer.weight', 'language_model.base_model.model.model.layers.41.attention.wo.lora_A.default.weight', 'language_model.base_model.model.model.layers.41.attention.wo.lora_B.default.weight', 'language_model.base_model.model.model.layers.41.attention.wqkv.base_layer.weight', 'language_model.base_model.model.model.layers.41.attention.wqkv.lora_A.default.weight', 'language_model.base_model.model.model.layers.41.attention.wqkv.lora_B.default.weight', 'language_model.base_model.model.model.layers.41.attention_norm.weight', 'language_model.base_model.model.model.layers.41.feed_forward.w1.base_layer.weight', 'language_model.base_model.model.model.layers.41.feed_forward.w1.lora_A.default.weight', 'language_model.base_model.model.model.layers.41.feed_forward.w1.lora_B.default.weight', 'language_model.base_model.model.model.layers.41.feed_forward.w2.base_layer.weight', 'language_model.base_model.model.model.layers.41.feed_forward.w2.lora_A.default.weight', 'language_model.base_model.model.model.layers.41.feed_forward.w2.lora_B.default.weight', 'language_model.base_model.model.model.layers.41.feed_forward.w3.base_layer.weight', 'language_model.base_model.model.model.layers.41.feed_forward.w3.lora_A.default.weight', 'language_model.base_model.model.model.layers.41.feed_forward.w3.lora_B.default.weight', 'language_model.base_model.model.model.layers.41.ffn_norm.weight', 'language_model.base_model.model.model.layers.42.attention.wo.base_layer.weight', 'language_model.base_model.model.model.layers.42.attention.wo.lora_A.default.weight', 'language_model.base_model.model.model.layers.42.attention.wo.lora_B.default.weight', 'language_model.base_model.model.model.layers.42.attention.wqkv.base_layer.weight', 'language_model.base_model.model.model.layers.42.attention.wqkv.lora_A.default.weight', 'language_model.base_model.model.model.layers.42.attention.wqkv.lora_B.default.weight', 'language_model.base_model.model.model.layers.42.attention_norm.weight', 'language_model.base_model.model.model.layers.42.feed_forward.w1.base_layer.weight', 'language_model.base_model.model.model.layers.42.feed_forward.w1.lora_A.default.weight', 'language_model.base_model.model.model.layers.42.feed_forward.w1.lora_B.default.weight', 'language_model.base_model.model.model.layers.42.feed_forward.w2.base_layer.weight', 'language_model.base_model.model.model.layers.42.feed_forward.w2.lora_A.default.weight', 'language_model.base_model.model.model.layers.42.feed_forward.w2.lora_B.default.weight', 'language_model.base_model.model.model.layers.42.feed_forward.w3.base_layer.weight', 'language_model.base_model.model.model.layers.42.feed_forward.w3.lora_A.default.weight', 'language_model.base_model.model.model.layers.42.feed_forward.w3.lora_B.default.weight', 'language_model.base_model.model.model.layers.42.ffn_norm.weight', 'language_model.base_model.model.model.layers.43.attention.wo.base_layer.weight', 'language_model.base_model.model.model.layers.43.attention.wo.lora_A.default.weight', 'language_model.base_model.model.model.layers.43.attention.wo.lora_B.default.weight', 'language_model.base_model.model.model.layers.43.attention.wqkv.base_layer.weight', 'language_model.base_model.model.model.layers.43.attention.wqkv.lora_A.default.weight', 'language_model.base_model.model.model.layers.43.attention.wqkv.lora_B.default.weight', 'language_model.base_model.model.model.layers.43.attention_norm.weight', 'language_model.base_model.model.model.layers.43.feed_forward.w1.base_layer.weight', 'language_model.base_model.model.model.layers.43.feed_forward.w1.lora_A.default.weight', 'language_model.base_model.model.model.layers.43.feed_forward.w1.lora_B.default.weight', 'language_model.base_model.model.model.layers.43.feed_forward.w2.base_layer.weight', 'language_model.base_model.model.model.layers.43.feed_forward.w2.lora_A.default.weight', 'language_model.base_model.model.model.layers.43.feed_forward.w2.lora_B.default.weight', 'language_model.base_model.model.model.layers.43.feed_forward.w3.base_layer.weight', 'language_model.base_model.model.model.layers.43.feed_forward.w3.lora_A.default.weight', 'language_model.base_model.model.model.layers.43.feed_forward.w3.lora_B.default.weight', 'language_model.base_model.model.model.layers.43.ffn_norm.weight', 'language_model.base_model.model.model.layers.44.attention.wo.base_layer.weight', 'language_model.base_model.model.model.layers.44.attention.wo.lora_A.default.weight', 'language_model.base_model.model.model.layers.44.attention.wo.lora_B.default.weight', 'language_model.base_model.model.model.layers.44.attention.wqkv.base_layer.weight', 'language_model.base_model.model.model.layers.44.attention.wqkv.lora_A.default.weight', 'language_model.base_model.model.model.layers.44.attention.wqkv.lora_B.default.weight', 'language_model.base_model.model.model.layers.44.attention_norm.weight', 'language_model.base_model.model.model.layers.44.feed_forward.w1.base_layer.weight', 'language_model.base_model.model.model.layers.44.feed_forward.w1.lora_A.default.weight', 'language_model.base_model.model.model.layers.44.feed_forward.w1.lora_B.default.weight', 'language_model.base_model.model.model.layers.44.feed_forward.w2.base_layer.weight', 'language_model.base_model.model.model.layers.44.feed_forward.w2.lora_A.default.weight', 'language_model.base_model.model.model.layers.44.feed_forward.w2.lora_B.default.weight', 'language_model.base_model.model.model.layers.44.feed_forward.w3.base_layer.weight', 'language_model.base_model.model.model.layers.44.feed_forward.w3.lora_A.default.weight', 'language_model.base_model.model.model.layers.44.feed_forward.w3.lora_B.default.weight', 'language_model.base_model.model.model.layers.44.ffn_norm.weight', 'language_model.base_model.model.model.layers.45.attention.wo.base_layer.weight', 'language_model.base_model.model.model.layers.45.attention.wo.lora_A.default.weight', 'language_model.base_model.model.model.layers.45.attention.wo.lora_B.default.weight', 'language_model.base_model.model.model.layers.45.attention.wqkv.base_layer.weight', 'language_model.base_model.model.model.layers.45.attention.wqkv.lora_A.default.weight', 'language_model.base_model.model.model.layers.45.attention.wqkv.lora_B.default.weight', 'language_model.base_model.model.model.layers.45.attention_norm.weight', 'language_model.base_model.model.model.layers.45.feed_forward.w1.base_layer.weight', 'language_model.base_model.model.model.layers.45.feed_forward.w1.lora_A.default.weight', 'language_model.base_model.model.model.layers.45.feed_forward.w1.lora_B.default.weight', 'language_model.base_model.model.model.layers.45.feed_forward.w2.base_layer.weight', 'language_model.base_model.model.model.layers.45.feed_forward.w2.lora_A.default.weight', 'language_model.base_model.model.model.layers.45.feed_forward.w2.lora_B.default.weight', 'language_model.base_model.model.model.layers.45.feed_forward.w3.base_layer.weight', 'language_model.base_model.model.model.layers.45.feed_forward.w3.lora_A.default.weight', 'language_model.base_model.model.model.layers.45.feed_forward.w3.lora_B.default.weight', 'language_model.base_model.model.model.layers.45.ffn_norm.weight', 'language_model.base_model.model.model.layers.46.attention.wo.base_layer.weight', 'language_model.base_model.model.model.layers.46.attention.wo.lora_A.default.weight', 'language_model.base_model.model.model.layers.46.attention.wo.lora_B.default.weight', 'language_model.base_model.model.model.layers.46.attention.wqkv.base_layer.weight', 'language_model.base_model.model.model.layers.46.attention.wqkv.lora_A.default.weight', 'language_model.base_model.model.model.layers.46.attention.wqkv.lora_B.default.weight', 'language_model.base_model.model.model.layers.46.attention_norm.weight', 'language_model.base_model.model.model.layers.46.feed_forward.w1.base_layer.weight', 'language_model.base_model.model.model.layers.46.feed_forward.w1.lora_A.default.weight', 'language_model.base_model.model.model.layers.46.feed_forward.w1.lora_B.default.weight', 'language_model.base_model.model.model.layers.46.feed_forward.w2.base_layer.weight', 'language_model.base_model.model.model.layers.46.feed_forward.w2.lora_A.default.weight', 'language_model.base_model.model.model.layers.46.feed_forward.w2.lora_B.default.weight', 'language_model.base_model.model.model.layers.46.feed_forward.w3.base_layer.weight', 'language_model.base_model.model.model.layers.46.feed_forward.w3.lora_A.default.weight', 'language_model.base_model.model.model.layers.46.feed_forward.w3.lora_B.default.weight', 'language_model.base_model.model.model.layers.46.ffn_norm.weight', 'language_model.base_model.model.model.layers.47.attention.wo.base_layer.weight', 'language_model.base_model.model.model.layers.47.attention.wo.lora_A.default.weight', 'language_model.base_model.model.model.layers.47.attention.wo.lora_B.default.weight', 'language_model.base_model.model.model.layers.47.attention.wqkv.base_layer.weight', 'language_model.base_model.model.model.layers.47.attention.wqkv.lora_A.default.weight', 'language_model.base_model.model.model.layers.47.attention.wqkv.lora_B.default.weight', 'language_model.base_model.model.model.layers.47.attention_norm.weight', 'language_model.base_model.model.model.layers.47.feed_forward.w1.base_layer.weight', 'language_model.base_model.model.model.layers.47.feed_forward.w1.lora_A.default.weight', 'language_model.base_model.model.model.layers.47.feed_forward.w1.lora_B.default.weight', 'language_model.base_model.model.model.layers.47.feed_forward.w2.base_layer.weight', 'language_model.base_model.model.model.layers.47.feed_forward.w2.lora_A.default.weight', 'language_model.base_model.model.model.layers.47.feed_forward.w2.lora_B.default.weight', 'language_model.base_model.model.model.layers.47.feed_forward.w3.base_layer.weight', 'language_model.base_model.model.model.layers.47.feed_forward.w3.lora_A.default.weight', 'language_model.base_model.model.model.layers.47.feed_forward.w3.lora_B.default.weight', 'language_model.base_model.model.model.layers.47.ffn_norm.weight', 'language_model.base_model.model.model.layers.5.attention.wo.base_layer.weight', 'language_model.base_model.model.model.layers.5.attention.wo.lora_A.default.weight', 'language_model.base_model.model.model.layers.5.attention.wo.lora_B.default.weight', 'language_model.base_model.model.model.layers.5.attention.wqkv.base_layer.weight', 'language_model.base_model.model.model.layers.5.attention.wqkv.lora_A.default.weight', 'language_model.base_model.model.model.layers.5.attention.wqkv.lora_B.default.weight', 'language_model.base_model.model.model.layers.5.attention_norm.weight', 'language_model.base_model.model.model.layers.5.feed_forward.w1.base_layer.weight', 'language_model.base_model.model.model.layers.5.feed_forward.w1.lora_A.default.weight', 'language_model.base_model.model.model.layers.5.feed_forward.w1.lora_B.default.weight', 'language_model.base_model.model.model.layers.5.feed_forward.w2.base_layer.weight', 'language_model.base_model.model.model.layers.5.feed_forward.w2.lora_A.default.weight', 'language_model.base_model.model.model.layers.5.feed_forward.w2.lora_B.default.weight', 'language_model.base_model.model.model.layers.5.feed_forward.w3.base_layer.weight', 'language_model.base_model.model.model.layers.5.feed_forward.w3.lora_A.default.weight', 'language_model.base_model.model.model.layers.5.feed_forward.w3.lora_B.default.weight', 'language_model.base_model.model.model.layers.5.ffn_norm.weight', 'language_model.base_model.model.model.layers.6.attention.wo.base_layer.weight', 'language_model.base_model.model.model.layers.6.attention.wo.lora_A.default.weight', 'language_model.base_model.model.model.layers.6.attention.wo.lora_B.default.weight', 'language_model.base_model.model.model.layers.6.attention.wqkv.base_layer.weight', 'language_model.base_model.model.model.layers.6.attention.wqkv.lora_A.default.weight', 'language_model.base_model.model.model.layers.6.attention.wqkv.lora_B.default.weight', 'language_model.base_model.model.model.layers.6.attention_norm.weight', 'language_model.base_model.model.model.layers.6.feed_forward.w1.base_layer.weight', 'language_model.base_model.model.model.layers.6.feed_forward.w1.lora_A.default.weight', 'language_model.base_model.model.model.layers.6.feed_forward.w1.lora_B.default.weight', 'language_model.base_model.model.model.layers.6.feed_forward.w2.base_layer.weight', 'language_model.base_model.model.model.layers.6.feed_forward.w2.lora_A.default.weight', 'language_model.base_model.model.model.layers.6.feed_forward.w2.lora_B.default.weight', 'language_model.base_model.model.model.layers.6.feed_forward.w3.base_layer.weight', 'language_model.base_model.model.model.layers.6.feed_forward.w3.lora_A.default.weight', 'language_model.base_model.model.model.layers.6.feed_forward.w3.lora_B.default.weight', 'language_model.base_model.model.model.layers.6.ffn_norm.weight', 'language_model.base_model.model.model.layers.7.attention.wo.base_layer.weight', 'language_model.base_model.model.model.layers.7.attention.wo.lora_A.default.weight', 'language_model.base_model.model.model.layers.7.attention.wo.lora_B.default.weight', 'language_model.base_model.model.model.layers.7.attention.wqkv.base_layer.weight', 'language_model.base_model.model.model.layers.7.attention.wqkv.lora_A.default.weight', 'language_model.base_model.model.model.layers.7.attention.wqkv.lora_B.default.weight', 'language_model.base_model.model.model.layers.7.attention_norm.weight', 'language_model.base_model.model.model.layers.7.feed_forward.w1.base_layer.weight', 'language_model.base_model.model.model.layers.7.feed_forward.w1.lora_A.default.weight', 'language_model.base_model.model.model.layers.7.feed_forward.w1.lora_B.default.weight', 'language_model.base_model.model.model.layers.7.feed_forward.w2.base_layer.weight', 'language_model.base_model.model.model.layers.7.feed_forward.w2.lora_A.default.weight', 'language_model.base_model.model.model.layers.7.feed_forward.w2.lora_B.default.weight', 'language_model.base_model.model.model.layers.7.feed_forward.w3.base_layer.weight', 'language_model.base_model.model.model.layers.7.feed_forward.w3.lora_A.default.weight', 'language_model.base_model.model.model.layers.7.feed_forward.w3.lora_B.default.weight', 'language_model.base_model.model.model.layers.7.ffn_norm.weight', 'language_model.base_model.model.model.layers.8.attention.wo.base_layer.weight', 'language_model.base_model.model.model.layers.8.attention.wo.lora_A.default.weight', 'language_model.base_model.model.model.layers.8.attention.wo.lora_B.default.weight', 'language_model.base_model.model.model.layers.8.attention.wqkv.base_layer.weight', 'language_model.base_model.model.model.layers.8.attention.wqkv.lora_A.default.weight', 'language_model.base_model.model.model.layers.8.attention.wqkv.lora_B.default.weight', 'language_model.base_model.model.model.layers.8.attention_norm.weight', 'language_model.base_model.model.model.layers.8.feed_forward.w1.base_layer.weight', 'language_model.base_model.model.model.layers.8.feed_forward.w1.lora_A.default.weight', 'language_model.base_model.model.model.layers.8.feed_forward.w1.lora_B.default.weight', 'language_model.base_model.model.model.layers.8.feed_forward.w2.base_layer.weight', 'language_model.base_model.model.model.layers.8.feed_forward.w2.lora_A.default.weight', 'language_model.base_model.model.model.layers.8.feed_forward.w2.lora_B.default.weight', 'language_model.base_model.model.model.layers.8.feed_forward.w3.base_layer.weight', 'language_model.base_model.model.model.layers.8.feed_forward.w3.lora_A.default.weight', 'language_model.base_model.model.model.layers.8.feed_forward.w3.lora_B.default.weight', 'language_model.base_model.model.model.layers.8.ffn_norm.weight', 'language_model.base_model.model.model.layers.9.attention.wo.base_layer.weight', 'language_model.base_model.model.model.layers.9.attention.wo.lora_A.default.weight', 'language_model.base_model.model.model.layers.9.attention.wo.lora_B.default.weight', 'language_model.base_model.model.model.layers.9.attention.wqkv.base_layer.weight', 'language_model.base_model.model.model.layers.9.attention.wqkv.lora_A.default.weight', 'language_model.base_model.model.model.layers.9.attention.wqkv.lora_B.default.weight', 'language_model.base_model.model.model.layers.9.attention_norm.weight', 'language_model.base_model.model.model.layers.9.feed_forward.w1.base_layer.weight', 'language_model.base_model.model.model.layers.9.feed_forward.w1.lora_A.default.weight', 'language_model.base_model.model.model.layers.9.feed_forward.w1.lora_B.default.weight', 'language_model.base_model.model.model.layers.9.feed_forward.w2.base_layer.weight', 'language_model.base_model.model.model.layers.9.feed_forward.w2.lora_A.default.weight', 'language_model.base_model.model.model.layers.9.feed_forward.w2.lora_B.default.weight', 'language_model.base_model.model.model.layers.9.feed_forward.w3.base_layer.weight', 'language_model.base_model.model.model.layers.9.feed_forward.w3.lora_A.default.weight', 'language_model.base_model.model.model.layers.9.feed_forward.w3.lora_B.default.weight', 'language_model.base_model.model.model.layers.9.ffn_norm.weight', 'language_model.base_model.model.model.norm.weight', 'language_model.base_model.model.model.tok_embeddings.weight', 'language_model.base_model.model.output.weight', 'vision_model.base_model.model.embeddings.class_embedding', 'vision_model.base_model.model.embeddings.patch_embedding.bias', 'vision_model.base_model.model.embeddings.patch_embedding.weight', 'vision_model.base_model.model.embeddings.position_embedding', 'vision_model.base_model.model.encoder.layers.0.attn.k_norm.weight', 'vision_model.base_model.model.encoder.layers.0.attn.proj.base_layer.bias', 'vision_model.base_model.model.encoder.layers.0.attn.proj.base_layer.weight', 'vision_model.base_model.model.encoder.layers.0.attn.proj.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.0.attn.proj.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.0.attn.q_norm.weight', 'vision_model.base_model.model.encoder.layers.0.attn.qkv.base_layer.weight', 'vision_model.base_model.model.encoder.layers.0.attn.qkv.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.0.attn.qkv.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.0.ls1', 'vision_model.base_model.model.encoder.layers.0.ls2', 'vision_model.base_model.model.encoder.layers.0.mlp.fc1.base_layer.bias', 'vision_model.base_model.model.encoder.layers.0.mlp.fc1.base_layer.weight', 'vision_model.base_model.model.encoder.layers.0.mlp.fc1.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.0.mlp.fc1.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.0.mlp.fc2.base_layer.bias', 'vision_model.base_model.model.encoder.layers.0.mlp.fc2.base_layer.weight', 'vision_model.base_model.model.encoder.layers.0.mlp.fc2.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.0.mlp.fc2.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.0.norm1.weight', 'vision_model.base_model.model.encoder.layers.0.norm2.weight', 'vision_model.base_model.model.encoder.layers.1.attn.k_norm.weight', 'vision_model.base_model.model.encoder.layers.1.attn.proj.base_layer.bias', 'vision_model.base_model.model.encoder.layers.1.attn.proj.base_layer.weight', 'vision_model.base_model.model.encoder.layers.1.attn.proj.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.1.attn.proj.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.1.attn.q_norm.weight', 'vision_model.base_model.model.encoder.layers.1.attn.qkv.base_layer.weight', 'vision_model.base_model.model.encoder.layers.1.attn.qkv.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.1.attn.qkv.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.1.ls1', 'vision_model.base_model.model.encoder.layers.1.ls2', 'vision_model.base_model.model.encoder.layers.1.mlp.fc1.base_layer.bias', 'vision_model.base_model.model.encoder.layers.1.mlp.fc1.base_layer.weight', 'vision_model.base_model.model.encoder.layers.1.mlp.fc1.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.1.mlp.fc1.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.1.mlp.fc2.base_layer.bias', 'vision_model.base_model.model.encoder.layers.1.mlp.fc2.base_layer.weight', 'vision_model.base_model.model.encoder.layers.1.mlp.fc2.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.1.mlp.fc2.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.1.norm1.weight', 'vision_model.base_model.model.encoder.layers.1.norm2.weight', 'vision_model.base_model.model.encoder.layers.10.attn.k_norm.weight', 'vision_model.base_model.model.encoder.layers.10.attn.proj.base_layer.bias', 'vision_model.base_model.model.encoder.layers.10.attn.proj.base_layer.weight', 'vision_model.base_model.model.encoder.layers.10.attn.proj.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.10.attn.proj.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.10.attn.q_norm.weight', 'vision_model.base_model.model.encoder.layers.10.attn.qkv.base_layer.weight', 'vision_model.base_model.model.encoder.layers.10.attn.qkv.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.10.attn.qkv.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.10.ls1', 'vision_model.base_model.model.encoder.layers.10.ls2', 'vision_model.base_model.model.encoder.layers.10.mlp.fc1.base_layer.bias', 'vision_model.base_model.model.encoder.layers.10.mlp.fc1.base_layer.weight', 'vision_model.base_model.model.encoder.layers.10.mlp.fc1.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.10.mlp.fc1.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.10.mlp.fc2.base_layer.bias', 'vision_model.base_model.model.encoder.layers.10.mlp.fc2.base_layer.weight', 'vision_model.base_model.model.encoder.layers.10.mlp.fc2.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.10.mlp.fc2.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.10.norm1.weight', 'vision_model.base_model.model.encoder.layers.10.norm2.weight', 'vision_model.base_model.model.encoder.layers.11.attn.k_norm.weight', 'vision_model.base_model.model.encoder.layers.11.attn.proj.base_layer.bias', 'vision_model.base_model.model.encoder.layers.11.attn.proj.base_layer.weight', 'vision_model.base_model.model.encoder.layers.11.attn.proj.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.11.attn.proj.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.11.attn.q_norm.weight', 'vision_model.base_model.model.encoder.layers.11.attn.qkv.base_layer.weight', 'vision_model.base_model.model.encoder.layers.11.attn.qkv.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.11.attn.qkv.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.11.ls1', 'vision_model.base_model.model.encoder.layers.11.ls2', 'vision_model.base_model.model.encoder.layers.11.mlp.fc1.base_layer.bias', 'vision_model.base_model.model.encoder.layers.11.mlp.fc1.base_layer.weight', 'vision_model.base_model.model.encoder.layers.11.mlp.fc1.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.11.mlp.fc1.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.11.mlp.fc2.base_layer.bias', 'vision_model.base_model.model.encoder.layers.11.mlp.fc2.base_layer.weight', 'vision_model.base_model.model.encoder.layers.11.mlp.fc2.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.11.mlp.fc2.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.11.norm1.weight', 'vision_model.base_model.model.encoder.layers.11.norm2.weight', 'vision_model.base_model.model.encoder.layers.12.attn.k_norm.weight', 'vision_model.base_model.model.encoder.layers.12.attn.proj.base_layer.bias', 'vision_model.base_model.model.encoder.layers.12.attn.proj.base_layer.weight', 'vision_model.base_model.model.encoder.layers.12.attn.proj.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.12.attn.proj.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.12.attn.q_norm.weight', 'vision_model.base_model.model.encoder.layers.12.attn.qkv.base_layer.weight', 'vision_model.base_model.model.encoder.layers.12.attn.qkv.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.12.attn.qkv.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.12.ls1', 'vision_model.base_model.model.encoder.layers.12.ls2', 'vision_model.base_model.model.encoder.layers.12.mlp.fc1.base_layer.bias', 'vision_model.base_model.model.encoder.layers.12.mlp.fc1.base_layer.weight', 'vision_model.base_model.model.encoder.layers.12.mlp.fc1.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.12.mlp.fc1.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.12.mlp.fc2.base_layer.bias', 'vision_model.base_model.model.encoder.layers.12.mlp.fc2.base_layer.weight', 'vision_model.base_model.model.encoder.layers.12.mlp.fc2.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.12.mlp.fc2.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.12.norm1.weight', 'vision_model.base_model.model.encoder.layers.12.norm2.weight', 'vision_model.base_model.model.encoder.layers.13.attn.k_norm.weight', 'vision_model.base_model.model.encoder.layers.13.attn.proj.base_layer.bias', 'vision_model.base_model.model.encoder.layers.13.attn.proj.base_layer.weight', 'vision_model.base_model.model.encoder.layers.13.attn.proj.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.13.attn.proj.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.13.attn.q_norm.weight', 'vision_model.base_model.model.encoder.layers.13.attn.qkv.base_layer.weight', 'vision_model.base_model.model.encoder.layers.13.attn.qkv.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.13.attn.qkv.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.13.ls1', 'vision_model.base_model.model.encoder.layers.13.ls2', 'vision_model.base_model.model.encoder.layers.13.mlp.fc1.base_layer.bias', 'vision_model.base_model.model.encoder.layers.13.mlp.fc1.base_layer.weight', 'vision_model.base_model.model.encoder.layers.13.mlp.fc1.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.13.mlp.fc1.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.13.mlp.fc2.base_layer.bias', 'vision_model.base_model.model.encoder.layers.13.mlp.fc2.base_layer.weight', 'vision_model.base_model.model.encoder.layers.13.mlp.fc2.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.13.mlp.fc2.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.13.norm1.weight', 'vision_model.base_model.model.encoder.layers.13.norm2.weight', 'vision_model.base_model.model.encoder.layers.14.attn.k_norm.weight', 'vision_model.base_model.model.encoder.layers.14.attn.proj.base_layer.bias', 'vision_model.base_model.model.encoder.layers.14.attn.proj.base_layer.weight', 'vision_model.base_model.model.encoder.layers.14.attn.proj.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.14.attn.proj.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.14.attn.q_norm.weight', 'vision_model.base_model.model.encoder.layers.14.attn.qkv.base_layer.weight', 'vision_model.base_model.model.encoder.layers.14.attn.qkv.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.14.attn.qkv.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.14.ls1', 'vision_model.base_model.model.encoder.layers.14.ls2', 'vision_model.base_model.model.encoder.layers.14.mlp.fc1.base_layer.bias', 'vision_model.base_model.model.encoder.layers.14.mlp.fc1.base_layer.weight', 'vision_model.base_model.model.encoder.layers.14.mlp.fc1.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.14.mlp.fc1.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.14.mlp.fc2.base_layer.bias', 'vision_model.base_model.model.encoder.layers.14.mlp.fc2.base_layer.weight', 'vision_model.base_model.model.encoder.layers.14.mlp.fc2.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.14.mlp.fc2.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.14.norm1.weight', 'vision_model.base_model.model.encoder.layers.14.norm2.weight', 'vision_model.base_model.model.encoder.layers.15.attn.k_norm.weight', 'vision_model.base_model.model.encoder.layers.15.attn.proj.base_layer.bias', 'vision_model.base_model.model.encoder.layers.15.attn.proj.base_layer.weight', 'vision_model.base_model.model.encoder.layers.15.attn.proj.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.15.attn.proj.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.15.attn.q_norm.weight', 'vision_model.base_model.model.encoder.layers.15.attn.qkv.base_layer.weight', 'vision_model.base_model.model.encoder.layers.15.attn.qkv.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.15.attn.qkv.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.15.ls1', 'vision_model.base_model.model.encoder.layers.15.ls2', 'vision_model.base_model.model.encoder.layers.15.mlp.fc1.base_layer.bias', 'vision_model.base_model.model.encoder.layers.15.mlp.fc1.base_layer.weight', 'vision_model.base_model.model.encoder.layers.15.mlp.fc1.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.15.mlp.fc1.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.15.mlp.fc2.base_layer.bias', 'vision_model.base_model.model.encoder.layers.15.mlp.fc2.base_layer.weight', 'vision_model.base_model.model.encoder.layers.15.mlp.fc2.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.15.mlp.fc2.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.15.norm1.weight', 'vision_model.base_model.model.encoder.layers.15.norm2.weight', 'vision_model.base_model.model.encoder.layers.16.attn.k_norm.weight', 'vision_model.base_model.model.encoder.layers.16.attn.proj.base_layer.bias', 'vision_model.base_model.model.encoder.layers.16.attn.proj.base_layer.weight', 'vision_model.base_model.model.encoder.layers.16.attn.proj.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.16.attn.proj.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.16.attn.q_norm.weight', 'vision_model.base_model.model.encoder.layers.16.attn.qkv.base_layer.weight', 'vision_model.base_model.model.encoder.layers.16.attn.qkv.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.16.attn.qkv.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.16.ls1', 'vision_model.base_model.model.encoder.layers.16.ls2', 'vision_model.base_model.model.encoder.layers.16.mlp.fc1.base_layer.bias', 'vision_model.base_model.model.encoder.layers.16.mlp.fc1.base_layer.weight', 'vision_model.base_model.model.encoder.layers.16.mlp.fc1.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.16.mlp.fc1.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.16.mlp.fc2.base_layer.bias', 'vision_model.base_model.model.encoder.layers.16.mlp.fc2.base_layer.weight', 'vision_model.base_model.model.encoder.layers.16.mlp.fc2.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.16.mlp.fc2.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.16.norm1.weight', 'vision_model.base_model.model.encoder.layers.16.norm2.weight', 'vision_model.base_model.model.encoder.layers.17.attn.k_norm.weight', 'vision_model.base_model.model.encoder.layers.17.attn.proj.base_layer.bias', 'vision_model.base_model.model.encoder.layers.17.attn.proj.base_layer.weight', 'vision_model.base_model.model.encoder.layers.17.attn.proj.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.17.attn.proj.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.17.attn.q_norm.weight', 'vision_model.base_model.model.encoder.layers.17.attn.qkv.base_layer.weight', 'vision_model.base_model.model.encoder.layers.17.attn.qkv.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.17.attn.qkv.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.17.ls1', 'vision_model.base_model.model.encoder.layers.17.ls2', 'vision_model.base_model.model.encoder.layers.17.mlp.fc1.base_layer.bias', 'vision_model.base_model.model.encoder.layers.17.mlp.fc1.base_layer.weight', 'vision_model.base_model.model.encoder.layers.17.mlp.fc1.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.17.mlp.fc1.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.17.mlp.fc2.base_layer.bias', 'vision_model.base_model.model.encoder.layers.17.mlp.fc2.base_layer.weight', 'vision_model.base_model.model.encoder.layers.17.mlp.fc2.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.17.mlp.fc2.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.17.norm1.weight', 'vision_model.base_model.model.encoder.layers.17.norm2.weight', 'vision_model.base_model.model.encoder.layers.18.attn.k_norm.weight', 'vision_model.base_model.model.encoder.layers.18.attn.proj.base_layer.bias', 'vision_model.base_model.model.encoder.layers.18.attn.proj.base_layer.weight', 'vision_model.base_model.model.encoder.layers.18.attn.proj.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.18.attn.proj.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.18.attn.q_norm.weight', 'vision_model.base_model.model.encoder.layers.18.attn.qkv.base_layer.weight', 'vision_model.base_model.model.encoder.layers.18.attn.qkv.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.18.attn.qkv.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.18.ls1', 'vision_model.base_model.model.encoder.layers.18.ls2', 'vision_model.base_model.model.encoder.layers.18.mlp.fc1.base_layer.bias', 'vision_model.base_model.model.encoder.layers.18.mlp.fc1.base_layer.weight', 'vision_model.base_model.model.encoder.layers.18.mlp.fc1.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.18.mlp.fc1.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.18.mlp.fc2.base_layer.bias', 'vision_model.base_model.model.encoder.layers.18.mlp.fc2.base_layer.weight', 'vision_model.base_model.model.encoder.layers.18.mlp.fc2.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.18.mlp.fc2.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.18.norm1.weight', 'vision_model.base_model.model.encoder.layers.18.norm2.weight', 'vision_model.base_model.model.encoder.layers.19.attn.k_norm.weight', 'vision_model.base_model.model.encoder.layers.19.attn.proj.base_layer.bias', 'vision_model.base_model.model.encoder.layers.19.attn.proj.base_layer.weight', 'vision_model.base_model.model.encoder.layers.19.attn.proj.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.19.attn.proj.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.19.attn.q_norm.weight', 'vision_model.base_model.model.encoder.layers.19.attn.qkv.base_layer.weight', 'vision_model.base_model.model.encoder.layers.19.attn.qkv.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.19.attn.qkv.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.19.ls1', 'vision_model.base_model.model.encoder.layers.19.ls2', 'vision_model.base_model.model.encoder.layers.19.mlp.fc1.base_layer.bias', 'vision_model.base_model.model.encoder.layers.19.mlp.fc1.base_layer.weight', 'vision_model.base_model.model.encoder.layers.19.mlp.fc1.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.19.mlp.fc1.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.19.mlp.fc2.base_layer.bias', 'vision_model.base_model.model.encoder.layers.19.mlp.fc2.base_layer.weight', 'vision_model.base_model.model.encoder.layers.19.mlp.fc2.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.19.mlp.fc2.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.19.norm1.weight', 'vision_model.base_model.model.encoder.layers.19.norm2.weight', 'vision_model.base_model.model.encoder.layers.2.attn.k_norm.weight', 'vision_model.base_model.model.encoder.layers.2.attn.proj.base_layer.bias', 'vision_model.base_model.model.encoder.layers.2.attn.proj.base_layer.weight', 'vision_model.base_model.model.encoder.layers.2.attn.proj.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.2.attn.proj.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.2.attn.q_norm.weight', 'vision_model.base_model.model.encoder.layers.2.attn.qkv.base_layer.weight', 'vision_model.base_model.model.encoder.layers.2.attn.qkv.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.2.attn.qkv.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.2.ls1', 'vision_model.base_model.model.encoder.layers.2.ls2', 'vision_model.base_model.model.encoder.layers.2.mlp.fc1.base_layer.bias', 'vision_model.base_model.model.encoder.layers.2.mlp.fc1.base_layer.weight', 'vision_model.base_model.model.encoder.layers.2.mlp.fc1.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.2.mlp.fc1.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.2.mlp.fc2.base_layer.bias', 'vision_model.base_model.model.encoder.layers.2.mlp.fc2.base_layer.weight', 'vision_model.base_model.model.encoder.layers.2.mlp.fc2.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.2.mlp.fc2.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.2.norm1.weight', 'vision_model.base_model.model.encoder.layers.2.norm2.weight', 'vision_model.base_model.model.encoder.layers.20.attn.k_norm.weight', 'vision_model.base_model.model.encoder.layers.20.attn.proj.base_layer.bias', 'vision_model.base_model.model.encoder.layers.20.attn.proj.base_layer.weight', 'vision_model.base_model.model.encoder.layers.20.attn.proj.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.20.attn.proj.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.20.attn.q_norm.weight', 'vision_model.base_model.model.encoder.layers.20.attn.qkv.base_layer.weight', 'vision_model.base_model.model.encoder.layers.20.attn.qkv.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.20.attn.qkv.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.20.ls1', 'vision_model.base_model.model.encoder.layers.20.ls2', 'vision_model.base_model.model.encoder.layers.20.mlp.fc1.base_layer.bias', 'vision_model.base_model.model.encoder.layers.20.mlp.fc1.base_layer.weight', 'vision_model.base_model.model.encoder.layers.20.mlp.fc1.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.20.mlp.fc1.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.20.mlp.fc2.base_layer.bias', 'vision_model.base_model.model.encoder.layers.20.mlp.fc2.base_layer.weight', 'vision_model.base_model.model.encoder.layers.20.mlp.fc2.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.20.mlp.fc2.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.20.norm1.weight', 'vision_model.base_model.model.encoder.layers.20.norm2.weight', 'vision_model.base_model.model.encoder.layers.21.attn.k_norm.weight', 'vision_model.base_model.model.encoder.layers.21.attn.proj.base_layer.bias', 'vision_model.base_model.model.encoder.layers.21.attn.proj.base_layer.weight', 'vision_model.base_model.model.encoder.layers.21.attn.proj.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.21.attn.proj.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.21.attn.q_norm.weight', 'vision_model.base_model.model.encoder.layers.21.attn.qkv.base_layer.weight', 'vision_model.base_model.model.encoder.layers.21.attn.qkv.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.21.attn.qkv.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.21.ls1', 'vision_model.base_model.model.encoder.layers.21.ls2', 'vision_model.base_model.model.encoder.layers.21.mlp.fc1.base_layer.bias', 'vision_model.base_model.model.encoder.layers.21.mlp.fc1.base_layer.weight', 'vision_model.base_model.model.encoder.layers.21.mlp.fc1.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.21.mlp.fc1.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.21.mlp.fc2.base_layer.bias', 'vision_model.base_model.model.encoder.layers.21.mlp.fc2.base_layer.weight', 'vision_model.base_model.model.encoder.layers.21.mlp.fc2.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.21.mlp.fc2.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.21.norm1.weight', 'vision_model.base_model.model.encoder.layers.21.norm2.weight', 'vision_model.base_model.model.encoder.layers.22.attn.k_norm.weight', 'vision_model.base_model.model.encoder.layers.22.attn.proj.base_layer.bias', 'vision_model.base_model.model.encoder.layers.22.attn.proj.base_layer.weight', 'vision_model.base_model.model.encoder.layers.22.attn.proj.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.22.attn.proj.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.22.attn.q_norm.weight', 'vision_model.base_model.model.encoder.layers.22.attn.qkv.base_layer.weight', 'vision_model.base_model.model.encoder.layers.22.attn.qkv.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.22.attn.qkv.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.22.ls1', 'vision_model.base_model.model.encoder.layers.22.ls2', 'vision_model.base_model.model.encoder.layers.22.mlp.fc1.base_layer.bias', 'vision_model.base_model.model.encoder.layers.22.mlp.fc1.base_layer.weight', 'vision_model.base_model.model.encoder.layers.22.mlp.fc1.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.22.mlp.fc1.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.22.mlp.fc2.base_layer.bias', 'vision_model.base_model.model.encoder.layers.22.mlp.fc2.base_layer.weight', 'vision_model.base_model.model.encoder.layers.22.mlp.fc2.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.22.mlp.fc2.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.22.norm1.weight', 'vision_model.base_model.model.encoder.layers.22.norm2.weight', 'vision_model.base_model.model.encoder.layers.23.attn.k_norm.weight', 'vision_model.base_model.model.encoder.layers.23.attn.proj.base_layer.bias', 'vision_model.base_model.model.encoder.layers.23.attn.proj.base_layer.weight', 'vision_model.base_model.model.encoder.layers.23.attn.proj.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.23.attn.proj.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.23.attn.q_norm.weight', 'vision_model.base_model.model.encoder.layers.23.attn.qkv.base_layer.weight', 'vision_model.base_model.model.encoder.layers.23.attn.qkv.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.23.attn.qkv.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.23.ls1', 'vision_model.base_model.model.encoder.layers.23.ls2', 'vision_model.base_model.model.encoder.layers.23.mlp.fc1.base_layer.bias', 'vision_model.base_model.model.encoder.layers.23.mlp.fc1.base_layer.weight', 'vision_model.base_model.model.encoder.layers.23.mlp.fc1.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.23.mlp.fc1.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.23.mlp.fc2.base_layer.bias', 'vision_model.base_model.model.encoder.layers.23.mlp.fc2.base_layer.weight', 'vision_model.base_model.model.encoder.layers.23.mlp.fc2.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.23.mlp.fc2.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.23.norm1.weight', 'vision_model.base_model.model.encoder.layers.23.norm2.weight', 'vision_model.base_model.model.encoder.layers.24.attn.k_norm.weight', 'vision_model.base_model.model.encoder.layers.24.attn.proj.base_layer.bias', 'vision_model.base_model.model.encoder.layers.24.attn.proj.base_layer.weight', 'vision_model.base_model.model.encoder.layers.24.attn.proj.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.24.attn.proj.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.24.attn.q_norm.weight', 'vision_model.base_model.model.encoder.layers.24.attn.qkv.base_layer.weight', 'vision_model.base_model.model.encoder.layers.24.attn.qkv.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.24.attn.qkv.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.24.ls1', 'vision_model.base_model.model.encoder.layers.24.ls2', 'vision_model.base_model.model.encoder.layers.24.mlp.fc1.base_layer.bias', 'vision_model.base_model.model.encoder.layers.24.mlp.fc1.base_layer.weight', 'vision_model.base_model.model.encoder.layers.24.mlp.fc1.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.24.mlp.fc1.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.24.mlp.fc2.base_layer.bias', 'vision_model.base_model.model.encoder.layers.24.mlp.fc2.base_layer.weight', 'vision_model.base_model.model.encoder.layers.24.mlp.fc2.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.24.mlp.fc2.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.24.norm1.weight', 'vision_model.base_model.model.encoder.layers.24.norm2.weight', 'vision_model.base_model.model.encoder.layers.25.attn.k_norm.weight', 'vision_model.base_model.model.encoder.layers.25.attn.proj.base_layer.bias', 'vision_model.base_model.model.encoder.layers.25.attn.proj.base_layer.weight', 'vision_model.base_model.model.encoder.layers.25.attn.proj.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.25.attn.proj.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.25.attn.q_norm.weight', 'vision_model.base_model.model.encoder.layers.25.attn.qkv.base_layer.weight', 'vision_model.base_model.model.encoder.layers.25.attn.qkv.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.25.attn.qkv.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.25.ls1', 'vision_model.base_model.model.encoder.layers.25.ls2', 'vision_model.base_model.model.encoder.layers.25.mlp.fc1.base_layer.bias', 'vision_model.base_model.model.encoder.layers.25.mlp.fc1.base_layer.weight', 'vision_model.base_model.model.encoder.layers.25.mlp.fc1.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.25.mlp.fc1.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.25.mlp.fc2.base_layer.bias', 'vision_model.base_model.model.encoder.layers.25.mlp.fc2.base_layer.weight', 'vision_model.base_model.model.encoder.layers.25.mlp.fc2.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.25.mlp.fc2.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.25.norm1.weight', 'vision_model.base_model.model.encoder.layers.25.norm2.weight', 'vision_model.base_model.model.encoder.layers.26.attn.k_norm.weight', 'vision_model.base_model.model.encoder.layers.26.attn.proj.base_layer.bias', 'vision_model.base_model.model.encoder.layers.26.attn.proj.base_layer.weight', 'vision_model.base_model.model.encoder.layers.26.attn.proj.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.26.attn.proj.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.26.attn.q_norm.weight', 'vision_model.base_model.model.encoder.layers.26.attn.qkv.base_layer.weight', 'vision_model.base_model.model.encoder.layers.26.attn.qkv.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.26.attn.qkv.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.26.ls1', 'vision_model.base_model.model.encoder.layers.26.ls2', 'vision_model.base_model.model.encoder.layers.26.mlp.fc1.base_layer.bias', 'vision_model.base_model.model.encoder.layers.26.mlp.fc1.base_layer.weight', 'vision_model.base_model.model.encoder.layers.26.mlp.fc1.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.26.mlp.fc1.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.26.mlp.fc2.base_layer.bias', 'vision_model.base_model.model.encoder.layers.26.mlp.fc2.base_layer.weight', 'vision_model.base_model.model.encoder.layers.26.mlp.fc2.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.26.mlp.fc2.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.26.norm1.weight', 'vision_model.base_model.model.encoder.layers.26.norm2.weight', 'vision_model.base_model.model.encoder.layers.27.attn.k_norm.weight', 'vision_model.base_model.model.encoder.layers.27.attn.proj.base_layer.bias', 'vision_model.base_model.model.encoder.layers.27.attn.proj.base_layer.weight', 'vision_model.base_model.model.encoder.layers.27.attn.proj.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.27.attn.proj.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.27.attn.q_norm.weight', 'vision_model.base_model.model.encoder.layers.27.attn.qkv.base_layer.weight', 'vision_model.base_model.model.encoder.layers.27.attn.qkv.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.27.attn.qkv.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.27.ls1', 'vision_model.base_model.model.encoder.layers.27.ls2', 'vision_model.base_model.model.encoder.layers.27.mlp.fc1.base_layer.bias', 'vision_model.base_model.model.encoder.layers.27.mlp.fc1.base_layer.weight', 'vision_model.base_model.model.encoder.layers.27.mlp.fc1.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.27.mlp.fc1.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.27.mlp.fc2.base_layer.bias', 'vision_model.base_model.model.encoder.layers.27.mlp.fc2.base_layer.weight', 'vision_model.base_model.model.encoder.layers.27.mlp.fc2.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.27.mlp.fc2.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.27.norm1.weight', 'vision_model.base_model.model.encoder.layers.27.norm2.weight', 'vision_model.base_model.model.encoder.layers.28.attn.k_norm.weight', 'vision_model.base_model.model.encoder.layers.28.attn.proj.base_layer.bias', 'vision_model.base_model.model.encoder.layers.28.attn.proj.base_layer.weight', 'vision_model.base_model.model.encoder.layers.28.attn.proj.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.28.attn.proj.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.28.attn.q_norm.weight', 'vision_model.base_model.model.encoder.layers.28.attn.qkv.base_layer.weight', 'vision_model.base_model.model.encoder.layers.28.attn.qkv.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.28.attn.qkv.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.28.ls1', 'vision_model.base_model.model.encoder.layers.28.ls2', 'vision_model.base_model.model.encoder.layers.28.mlp.fc1.base_layer.bias', 'vision_model.base_model.model.encoder.layers.28.mlp.fc1.base_layer.weight', 'vision_model.base_model.model.encoder.layers.28.mlp.fc1.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.28.mlp.fc1.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.28.mlp.fc2.base_layer.bias', 'vision_model.base_model.model.encoder.layers.28.mlp.fc2.base_layer.weight', 'vision_model.base_model.model.encoder.layers.28.mlp.fc2.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.28.mlp.fc2.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.28.norm1.weight', 'vision_model.base_model.model.encoder.layers.28.norm2.weight', 'vision_model.base_model.model.encoder.layers.29.attn.k_norm.weight', 'vision_model.base_model.model.encoder.layers.29.attn.proj.base_layer.bias', 'vision_model.base_model.model.encoder.layers.29.attn.proj.base_layer.weight', 'vision_model.base_model.model.encoder.layers.29.attn.proj.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.29.attn.proj.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.29.attn.q_norm.weight', 'vision_model.base_model.model.encoder.layers.29.attn.qkv.base_layer.weight', 'vision_model.base_model.model.encoder.layers.29.attn.qkv.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.29.attn.qkv.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.29.ls1', 'vision_model.base_model.model.encoder.layers.29.ls2', 'vision_model.base_model.model.encoder.layers.29.mlp.fc1.base_layer.bias', 'vision_model.base_model.model.encoder.layers.29.mlp.fc1.base_layer.weight', 'vision_model.base_model.model.encoder.layers.29.mlp.fc1.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.29.mlp.fc1.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.29.mlp.fc2.base_layer.bias', 'vision_model.base_model.model.encoder.layers.29.mlp.fc2.base_layer.weight', 'vision_model.base_model.model.encoder.layers.29.mlp.fc2.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.29.mlp.fc2.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.29.norm1.weight', 'vision_model.base_model.model.encoder.layers.29.norm2.weight', 'vision_model.base_model.model.encoder.layers.3.attn.k_norm.weight', 'vision_model.base_model.model.encoder.layers.3.attn.proj.base_layer.bias', 'vision_model.base_model.model.encoder.layers.3.attn.proj.base_layer.weight', 'vision_model.base_model.model.encoder.layers.3.attn.proj.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.3.attn.proj.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.3.attn.q_norm.weight', 'vision_model.base_model.model.encoder.layers.3.attn.qkv.base_layer.weight', 'vision_model.base_model.model.encoder.layers.3.attn.qkv.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.3.attn.qkv.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.3.ls1', 'vision_model.base_model.model.encoder.layers.3.ls2', 'vision_model.base_model.model.encoder.layers.3.mlp.fc1.base_layer.bias', 'vision_model.base_model.model.encoder.layers.3.mlp.fc1.base_layer.weight', 'vision_model.base_model.model.encoder.layers.3.mlp.fc1.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.3.mlp.fc1.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.3.mlp.fc2.base_layer.bias', 'vision_model.base_model.model.encoder.layers.3.mlp.fc2.base_layer.weight', 'vision_model.base_model.model.encoder.layers.3.mlp.fc2.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.3.mlp.fc2.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.3.norm1.weight', 'vision_model.base_model.model.encoder.layers.3.norm2.weight', 'vision_model.base_model.model.encoder.layers.30.attn.k_norm.weight', 'vision_model.base_model.model.encoder.layers.30.attn.proj.base_layer.bias', 'vision_model.base_model.model.encoder.layers.30.attn.proj.base_layer.weight', 'vision_model.base_model.model.encoder.layers.30.attn.proj.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.30.attn.proj.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.30.attn.q_norm.weight', 'vision_model.base_model.model.encoder.layers.30.attn.qkv.base_layer.weight', 'vision_model.base_model.model.encoder.layers.30.attn.qkv.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.30.attn.qkv.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.30.ls1', 'vision_model.base_model.model.encoder.layers.30.ls2', 'vision_model.base_model.model.encoder.layers.30.mlp.fc1.base_layer.bias', 'vision_model.base_model.model.encoder.layers.30.mlp.fc1.base_layer.weight', 'vision_model.base_model.model.encoder.layers.30.mlp.fc1.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.30.mlp.fc1.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.30.mlp.fc2.base_layer.bias', 'vision_model.base_model.model.encoder.layers.30.mlp.fc2.base_layer.weight', 'vision_model.base_model.model.encoder.layers.30.mlp.fc2.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.30.mlp.fc2.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.30.norm1.weight', 'vision_model.base_model.model.encoder.layers.30.norm2.weight', 'vision_model.base_model.model.encoder.layers.31.attn.k_norm.weight', 'vision_model.base_model.model.encoder.layers.31.attn.proj.base_layer.bias', 'vision_model.base_model.model.encoder.layers.31.attn.proj.base_layer.weight', 'vision_model.base_model.model.encoder.layers.31.attn.proj.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.31.attn.proj.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.31.attn.q_norm.weight', 'vision_model.base_model.model.encoder.layers.31.attn.qkv.base_layer.weight', 'vision_model.base_model.model.encoder.layers.31.attn.qkv.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.31.attn.qkv.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.31.ls1', 'vision_model.base_model.model.encoder.layers.31.ls2', 'vision_model.base_model.model.encoder.layers.31.mlp.fc1.base_layer.bias', 'vision_model.base_model.model.encoder.layers.31.mlp.fc1.base_layer.weight', 'vision_model.base_model.model.encoder.layers.31.mlp.fc1.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.31.mlp.fc1.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.31.mlp.fc2.base_layer.bias', 'vision_model.base_model.model.encoder.layers.31.mlp.fc2.base_layer.weight', 'vision_model.base_model.model.encoder.layers.31.mlp.fc2.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.31.mlp.fc2.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.31.norm1.weight', 'vision_model.base_model.model.encoder.layers.31.norm2.weight', 'vision_model.base_model.model.encoder.layers.32.attn.k_norm.weight', 'vision_model.base_model.model.encoder.layers.32.attn.proj.base_layer.bias', 'vision_model.base_model.model.encoder.layers.32.attn.proj.base_layer.weight', 'vision_model.base_model.model.encoder.layers.32.attn.proj.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.32.attn.proj.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.32.attn.q_norm.weight', 'vision_model.base_model.model.encoder.layers.32.attn.qkv.base_layer.weight', 'vision_model.base_model.model.encoder.layers.32.attn.qkv.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.32.attn.qkv.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.32.ls1', 'vision_model.base_model.model.encoder.layers.32.ls2', 'vision_model.base_model.model.encoder.layers.32.mlp.fc1.base_layer.bias', 'vision_model.base_model.model.encoder.layers.32.mlp.fc1.base_layer.weight', 'vision_model.base_model.model.encoder.layers.32.mlp.fc1.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.32.mlp.fc1.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.32.mlp.fc2.base_layer.bias', 'vision_model.base_model.model.encoder.layers.32.mlp.fc2.base_layer.weight', 'vision_model.base_model.model.encoder.layers.32.mlp.fc2.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.32.mlp.fc2.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.32.norm1.weight', 'vision_model.base_model.model.encoder.layers.32.norm2.weight', 'vision_model.base_model.model.encoder.layers.33.attn.k_norm.weight', 'vision_model.base_model.model.encoder.layers.33.attn.proj.base_layer.bias', 'vision_model.base_model.model.encoder.layers.33.attn.proj.base_layer.weight', 'vision_model.base_model.model.encoder.layers.33.attn.proj.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.33.attn.proj.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.33.attn.q_norm.weight', 'vision_model.base_model.model.encoder.layers.33.attn.qkv.base_layer.weight', 'vision_model.base_model.model.encoder.layers.33.attn.qkv.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.33.attn.qkv.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.33.ls1', 'vision_model.base_model.model.encoder.layers.33.ls2', 'vision_model.base_model.model.encoder.layers.33.mlp.fc1.base_layer.bias', 'vision_model.base_model.model.encoder.layers.33.mlp.fc1.base_layer.weight', 'vision_model.base_model.model.encoder.layers.33.mlp.fc1.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.33.mlp.fc1.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.33.mlp.fc2.base_layer.bias', 'vision_model.base_model.model.encoder.layers.33.mlp.fc2.base_layer.weight', 'vision_model.base_model.model.encoder.layers.33.mlp.fc2.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.33.mlp.fc2.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.33.norm1.weight', 'vision_model.base_model.model.encoder.layers.33.norm2.weight', 'vision_model.base_model.model.encoder.layers.34.attn.k_norm.weight', 'vision_model.base_model.model.encoder.layers.34.attn.proj.base_layer.bias', 'vision_model.base_model.model.encoder.layers.34.attn.proj.base_layer.weight', 'vision_model.base_model.model.encoder.layers.34.attn.proj.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.34.attn.proj.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.34.attn.q_norm.weight', 'vision_model.base_model.model.encoder.layers.34.attn.qkv.base_layer.weight', 'vision_model.base_model.model.encoder.layers.34.attn.qkv.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.34.attn.qkv.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.34.ls1', 'vision_model.base_model.model.encoder.layers.34.ls2', 'vision_model.base_model.model.encoder.layers.34.mlp.fc1.base_layer.bias', 'vision_model.base_model.model.encoder.layers.34.mlp.fc1.base_layer.weight', 'vision_model.base_model.model.encoder.layers.34.mlp.fc1.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.34.mlp.fc1.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.34.mlp.fc2.base_layer.bias', 'vision_model.base_model.model.encoder.layers.34.mlp.fc2.base_layer.weight', 'vision_model.base_model.model.encoder.layers.34.mlp.fc2.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.34.mlp.fc2.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.34.norm1.weight', 'vision_model.base_model.model.encoder.layers.34.norm2.weight', 'vision_model.base_model.model.encoder.layers.35.attn.k_norm.weight', 'vision_model.base_model.model.encoder.layers.35.attn.proj.base_layer.bias', 'vision_model.base_model.model.encoder.layers.35.attn.proj.base_layer.weight', 'vision_model.base_model.model.encoder.layers.35.attn.proj.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.35.attn.proj.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.35.attn.q_norm.weight', 'vision_model.base_model.model.encoder.layers.35.attn.qkv.base_layer.weight', 'vision_model.base_model.model.encoder.layers.35.attn.qkv.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.35.attn.qkv.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.35.ls1', 'vision_model.base_model.model.encoder.layers.35.ls2', 'vision_model.base_model.model.encoder.layers.35.mlp.fc1.base_layer.bias', 'vision_model.base_model.model.encoder.layers.35.mlp.fc1.base_layer.weight', 'vision_model.base_model.model.encoder.layers.35.mlp.fc1.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.35.mlp.fc1.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.35.mlp.fc2.base_layer.bias', 'vision_model.base_model.model.encoder.layers.35.mlp.fc2.base_layer.weight', 'vision_model.base_model.model.encoder.layers.35.mlp.fc2.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.35.mlp.fc2.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.35.norm1.weight', 'vision_model.base_model.model.encoder.layers.35.norm2.weight', 'vision_model.base_model.model.encoder.layers.36.attn.k_norm.weight', 'vision_model.base_model.model.encoder.layers.36.attn.proj.base_layer.bias', 'vision_model.base_model.model.encoder.layers.36.attn.proj.base_layer.weight', 'vision_model.base_model.model.encoder.layers.36.attn.proj.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.36.attn.proj.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.36.attn.q_norm.weight', 'vision_model.base_model.model.encoder.layers.36.attn.qkv.base_layer.weight', 'vision_model.base_model.model.encoder.layers.36.attn.qkv.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.36.attn.qkv.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.36.ls1', 'vision_model.base_model.model.encoder.layers.36.ls2', 'vision_model.base_model.model.encoder.layers.36.mlp.fc1.base_layer.bias', 'vision_model.base_model.model.encoder.layers.36.mlp.fc1.base_layer.weight', 'vision_model.base_model.model.encoder.layers.36.mlp.fc1.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.36.mlp.fc1.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.36.mlp.fc2.base_layer.bias', 'vision_model.base_model.model.encoder.layers.36.mlp.fc2.base_layer.weight', 'vision_model.base_model.model.encoder.layers.36.mlp.fc2.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.36.mlp.fc2.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.36.norm1.weight', 'vision_model.base_model.model.encoder.layers.36.norm2.weight', 'vision_model.base_model.model.encoder.layers.37.attn.k_norm.weight', 'vision_model.base_model.model.encoder.layers.37.attn.proj.base_layer.bias', 'vision_model.base_model.model.encoder.layers.37.attn.proj.base_layer.weight', 'vision_model.base_model.model.encoder.layers.37.attn.proj.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.37.attn.proj.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.37.attn.q_norm.weight', 'vision_model.base_model.model.encoder.layers.37.attn.qkv.base_layer.weight', 'vision_model.base_model.model.encoder.layers.37.attn.qkv.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.37.attn.qkv.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.37.ls1', 'vision_model.base_model.model.encoder.layers.37.ls2', 'vision_model.base_model.model.encoder.layers.37.mlp.fc1.base_layer.bias', 'vision_model.base_model.model.encoder.layers.37.mlp.fc1.base_layer.weight', 'vision_model.base_model.model.encoder.layers.37.mlp.fc1.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.37.mlp.fc1.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.37.mlp.fc2.base_layer.bias', 'vision_model.base_model.model.encoder.layers.37.mlp.fc2.base_layer.weight', 'vision_model.base_model.model.encoder.layers.37.mlp.fc2.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.37.mlp.fc2.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.37.norm1.weight', 'vision_model.base_model.model.encoder.layers.37.norm2.weight', 'vision_model.base_model.model.encoder.layers.38.attn.k_norm.weight', 'vision_model.base_model.model.encoder.layers.38.attn.proj.base_layer.bias', 'vision_model.base_model.model.encoder.layers.38.attn.proj.base_layer.weight', 'vision_model.base_model.model.encoder.layers.38.attn.proj.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.38.attn.proj.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.38.attn.q_norm.weight', 'vision_model.base_model.model.encoder.layers.38.attn.qkv.base_layer.weight', 'vision_model.base_model.model.encoder.layers.38.attn.qkv.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.38.attn.qkv.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.38.ls1', 'vision_model.base_model.model.encoder.layers.38.ls2', 'vision_model.base_model.model.encoder.layers.38.mlp.fc1.base_layer.bias', 'vision_model.base_model.model.encoder.layers.38.mlp.fc1.base_layer.weight', 'vision_model.base_model.model.encoder.layers.38.mlp.fc1.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.38.mlp.fc1.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.38.mlp.fc2.base_layer.bias', 'vision_model.base_model.model.encoder.layers.38.mlp.fc2.base_layer.weight', 'vision_model.base_model.model.encoder.layers.38.mlp.fc2.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.38.mlp.fc2.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.38.norm1.weight', 'vision_model.base_model.model.encoder.layers.38.norm2.weight', 'vision_model.base_model.model.encoder.layers.39.attn.k_norm.weight', 'vision_model.base_model.model.encoder.layers.39.attn.proj.base_layer.bias', 'vision_model.base_model.model.encoder.layers.39.attn.proj.base_layer.weight', 'vision_model.base_model.model.encoder.layers.39.attn.proj.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.39.attn.proj.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.39.attn.q_norm.weight', 'vision_model.base_model.model.encoder.layers.39.attn.qkv.base_layer.weight', 'vision_model.base_model.model.encoder.layers.39.attn.qkv.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.39.attn.qkv.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.39.ls1', 'vision_model.base_model.model.encoder.layers.39.ls2', 'vision_model.base_model.model.encoder.layers.39.mlp.fc1.base_layer.bias', 'vision_model.base_model.model.encoder.layers.39.mlp.fc1.base_layer.weight', 'vision_model.base_model.model.encoder.layers.39.mlp.fc1.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.39.mlp.fc1.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.39.mlp.fc2.base_layer.bias', 'vision_model.base_model.model.encoder.layers.39.mlp.fc2.base_layer.weight', 'vision_model.base_model.model.encoder.layers.39.mlp.fc2.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.39.mlp.fc2.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.39.norm1.weight', 'vision_model.base_model.model.encoder.layers.39.norm2.weight', 'vision_model.base_model.model.encoder.layers.4.attn.k_norm.weight', 'vision_model.base_model.model.encoder.layers.4.attn.proj.base_layer.bias', 'vision_model.base_model.model.encoder.layers.4.attn.proj.base_layer.weight', 'vision_model.base_model.model.encoder.layers.4.attn.proj.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.4.attn.proj.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.4.attn.q_norm.weight', 'vision_model.base_model.model.encoder.layers.4.attn.qkv.base_layer.weight', 'vision_model.base_model.model.encoder.layers.4.attn.qkv.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.4.attn.qkv.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.4.ls1', 'vision_model.base_model.model.encoder.layers.4.ls2', 'vision_model.base_model.model.encoder.layers.4.mlp.fc1.base_layer.bias', 'vision_model.base_model.model.encoder.layers.4.mlp.fc1.base_layer.weight', 'vision_model.base_model.model.encoder.layers.4.mlp.fc1.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.4.mlp.fc1.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.4.mlp.fc2.base_layer.bias', 'vision_model.base_model.model.encoder.layers.4.mlp.fc2.base_layer.weight', 'vision_model.base_model.model.encoder.layers.4.mlp.fc2.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.4.mlp.fc2.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.4.norm1.weight', 'vision_model.base_model.model.encoder.layers.4.norm2.weight', 'vision_model.base_model.model.encoder.layers.40.attn.k_norm.weight', 'vision_model.base_model.model.encoder.layers.40.attn.proj.base_layer.bias', 'vision_model.base_model.model.encoder.layers.40.attn.proj.base_layer.weight', 'vision_model.base_model.model.encoder.layers.40.attn.proj.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.40.attn.proj.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.40.attn.q_norm.weight', 'vision_model.base_model.model.encoder.layers.40.attn.qkv.base_layer.weight', 'vision_model.base_model.model.encoder.layers.40.attn.qkv.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.40.attn.qkv.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.40.ls1', 'vision_model.base_model.model.encoder.layers.40.ls2', 'vision_model.base_model.model.encoder.layers.40.mlp.fc1.base_layer.bias', 'vision_model.base_model.model.encoder.layers.40.mlp.fc1.base_layer.weight', 'vision_model.base_model.model.encoder.layers.40.mlp.fc1.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.40.mlp.fc1.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.40.mlp.fc2.base_layer.bias', 'vision_model.base_model.model.encoder.layers.40.mlp.fc2.base_layer.weight', 'vision_model.base_model.model.encoder.layers.40.mlp.fc2.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.40.mlp.fc2.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.40.norm1.weight', 'vision_model.base_model.model.encoder.layers.40.norm2.weight', 'vision_model.base_model.model.encoder.layers.41.attn.k_norm.weight', 'vision_model.base_model.model.encoder.layers.41.attn.proj.base_layer.bias', 'vision_model.base_model.model.encoder.layers.41.attn.proj.base_layer.weight', 'vision_model.base_model.model.encoder.layers.41.attn.proj.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.41.attn.proj.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.41.attn.q_norm.weight', 'vision_model.base_model.model.encoder.layers.41.attn.qkv.base_layer.weight', 'vision_model.base_model.model.encoder.layers.41.attn.qkv.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.41.attn.qkv.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.41.ls1', 'vision_model.base_model.model.encoder.layers.41.ls2', 'vision_model.base_model.model.encoder.layers.41.mlp.fc1.base_layer.bias', 'vision_model.base_model.model.encoder.layers.41.mlp.fc1.base_layer.weight', 'vision_model.base_model.model.encoder.layers.41.mlp.fc1.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.41.mlp.fc1.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.41.mlp.fc2.base_layer.bias', 'vision_model.base_model.model.encoder.layers.41.mlp.fc2.base_layer.weight', 'vision_model.base_model.model.encoder.layers.41.mlp.fc2.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.41.mlp.fc2.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.41.norm1.weight', 'vision_model.base_model.model.encoder.layers.41.norm2.weight', 'vision_model.base_model.model.encoder.layers.42.attn.k_norm.weight', 'vision_model.base_model.model.encoder.layers.42.attn.proj.base_layer.bias', 'vision_model.base_model.model.encoder.layers.42.attn.proj.base_layer.weight', 'vision_model.base_model.model.encoder.layers.42.attn.proj.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.42.attn.proj.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.42.attn.q_norm.weight', 'vision_model.base_model.model.encoder.layers.42.attn.qkv.base_layer.weight', 'vision_model.base_model.model.encoder.layers.42.attn.qkv.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.42.attn.qkv.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.42.ls1', 'vision_model.base_model.model.encoder.layers.42.ls2', 'vision_model.base_model.model.encoder.layers.42.mlp.fc1.base_layer.bias', 'vision_model.base_model.model.encoder.layers.42.mlp.fc1.base_layer.weight', 'vision_model.base_model.model.encoder.layers.42.mlp.fc1.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.42.mlp.fc1.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.42.mlp.fc2.base_layer.bias', 'vision_model.base_model.model.encoder.layers.42.mlp.fc2.base_layer.weight', 'vision_model.base_model.model.encoder.layers.42.mlp.fc2.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.42.mlp.fc2.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.42.norm1.weight', 'vision_model.base_model.model.encoder.layers.42.norm2.weight', 'vision_model.base_model.model.encoder.layers.43.attn.k_norm.weight', 'vision_model.base_model.model.encoder.layers.43.attn.proj.base_layer.bias', 'vision_model.base_model.model.encoder.layers.43.attn.proj.base_layer.weight', 'vision_model.base_model.model.encoder.layers.43.attn.proj.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.43.attn.proj.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.43.attn.q_norm.weight', 'vision_model.base_model.model.encoder.layers.43.attn.qkv.base_layer.weight', 'vision_model.base_model.model.encoder.layers.43.attn.qkv.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.43.attn.qkv.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.43.ls1', 'vision_model.base_model.model.encoder.layers.43.ls2', 'vision_model.base_model.model.encoder.layers.43.mlp.fc1.base_layer.bias', 'vision_model.base_model.model.encoder.layers.43.mlp.fc1.base_layer.weight', 'vision_model.base_model.model.encoder.layers.43.mlp.fc1.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.43.mlp.fc1.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.43.mlp.fc2.base_layer.bias', 'vision_model.base_model.model.encoder.layers.43.mlp.fc2.base_layer.weight', 'vision_model.base_model.model.encoder.layers.43.mlp.fc2.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.43.mlp.fc2.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.43.norm1.weight', 'vision_model.base_model.model.encoder.layers.43.norm2.weight', 'vision_model.base_model.model.encoder.layers.44.attn.k_norm.weight', 'vision_model.base_model.model.encoder.layers.44.attn.proj.base_layer.bias', 'vision_model.base_model.model.encoder.layers.44.attn.proj.base_layer.weight', 'vision_model.base_model.model.encoder.layers.44.attn.proj.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.44.attn.proj.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.44.attn.q_norm.weight', 'vision_model.base_model.model.encoder.layers.44.attn.qkv.base_layer.weight', 'vision_model.base_model.model.encoder.layers.44.attn.qkv.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.44.attn.qkv.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.44.ls1', 'vision_model.base_model.model.encoder.layers.44.ls2', 'vision_model.base_model.model.encoder.layers.44.mlp.fc1.base_layer.bias', 'vision_model.base_model.model.encoder.layers.44.mlp.fc1.base_layer.weight', 'vision_model.base_model.model.encoder.layers.44.mlp.fc1.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.44.mlp.fc1.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.44.mlp.fc2.base_layer.bias', 'vision_model.base_model.model.encoder.layers.44.mlp.fc2.base_layer.weight', 'vision_model.base_model.model.encoder.layers.44.mlp.fc2.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.44.mlp.fc2.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.44.norm1.weight', 'vision_model.base_model.model.encoder.layers.44.norm2.weight', 'vision_model.base_model.model.encoder.layers.5.attn.k_norm.weight', 'vision_model.base_model.model.encoder.layers.5.attn.proj.base_layer.bias', 'vision_model.base_model.model.encoder.layers.5.attn.proj.base_layer.weight', 'vision_model.base_model.model.encoder.layers.5.attn.proj.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.5.attn.proj.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.5.attn.q_norm.weight', 'vision_model.base_model.model.encoder.layers.5.attn.qkv.base_layer.weight', 'vision_model.base_model.model.encoder.layers.5.attn.qkv.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.5.attn.qkv.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.5.ls1', 'vision_model.base_model.model.encoder.layers.5.ls2', 'vision_model.base_model.model.encoder.layers.5.mlp.fc1.base_layer.bias', 'vision_model.base_model.model.encoder.layers.5.mlp.fc1.base_layer.weight', 'vision_model.base_model.model.encoder.layers.5.mlp.fc1.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.5.mlp.fc1.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.5.mlp.fc2.base_layer.bias', 'vision_model.base_model.model.encoder.layers.5.mlp.fc2.base_layer.weight', 'vision_model.base_model.model.encoder.layers.5.mlp.fc2.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.5.mlp.fc2.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.5.norm1.weight', 'vision_model.base_model.model.encoder.layers.5.norm2.weight', 'vision_model.base_model.model.encoder.layers.6.attn.k_norm.weight', 'vision_model.base_model.model.encoder.layers.6.attn.proj.base_layer.bias', 'vision_model.base_model.model.encoder.layers.6.attn.proj.base_layer.weight', 'vision_model.base_model.model.encoder.layers.6.attn.proj.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.6.attn.proj.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.6.attn.q_norm.weight', 'vision_model.base_model.model.encoder.layers.6.attn.qkv.base_layer.weight', 'vision_model.base_model.model.encoder.layers.6.attn.qkv.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.6.attn.qkv.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.6.ls1', 'vision_model.base_model.model.encoder.layers.6.ls2', 'vision_model.base_model.model.encoder.layers.6.mlp.fc1.base_layer.bias', 'vision_model.base_model.model.encoder.layers.6.mlp.fc1.base_layer.weight', 'vision_model.base_model.model.encoder.layers.6.mlp.fc1.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.6.mlp.fc1.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.6.mlp.fc2.base_layer.bias', 'vision_model.base_model.model.encoder.layers.6.mlp.fc2.base_layer.weight', 'vision_model.base_model.model.encoder.layers.6.mlp.fc2.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.6.mlp.fc2.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.6.norm1.weight', 'vision_model.base_model.model.encoder.layers.6.norm2.weight', 'vision_model.base_model.model.encoder.layers.7.attn.k_norm.weight', 'vision_model.base_model.model.encoder.layers.7.attn.proj.base_layer.bias', 'vision_model.base_model.model.encoder.layers.7.attn.proj.base_layer.weight', 'vision_model.base_model.model.encoder.layers.7.attn.proj.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.7.attn.proj.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.7.attn.q_norm.weight', 'vision_model.base_model.model.encoder.layers.7.attn.qkv.base_layer.weight', 'vision_model.base_model.model.encoder.layers.7.attn.qkv.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.7.attn.qkv.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.7.ls1', 'vision_model.base_model.model.encoder.layers.7.ls2', 'vision_model.base_model.model.encoder.layers.7.mlp.fc1.base_layer.bias', 'vision_model.base_model.model.encoder.layers.7.mlp.fc1.base_layer.weight', 'vision_model.base_model.model.encoder.layers.7.mlp.fc1.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.7.mlp.fc1.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.7.mlp.fc2.base_layer.bias', 'vision_model.base_model.model.encoder.layers.7.mlp.fc2.base_layer.weight', 'vision_model.base_model.model.encoder.layers.7.mlp.fc2.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.7.mlp.fc2.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.7.norm1.weight', 'vision_model.base_model.model.encoder.layers.7.norm2.weight', 'vision_model.base_model.model.encoder.layers.8.attn.k_norm.weight', 'vision_model.base_model.model.encoder.layers.8.attn.proj.base_layer.bias', 'vision_model.base_model.model.encoder.layers.8.attn.proj.base_layer.weight', 'vision_model.base_model.model.encoder.layers.8.attn.proj.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.8.attn.proj.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.8.attn.q_norm.weight', 'vision_model.base_model.model.encoder.layers.8.attn.qkv.base_layer.weight', 'vision_model.base_model.model.encoder.layers.8.attn.qkv.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.8.attn.qkv.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.8.ls1', 'vision_model.base_model.model.encoder.layers.8.ls2', 'vision_model.base_model.model.encoder.layers.8.mlp.fc1.base_layer.bias', 'vision_model.base_model.model.encoder.layers.8.mlp.fc1.base_layer.weight', 'vision_model.base_model.model.encoder.layers.8.mlp.fc1.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.8.mlp.fc1.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.8.mlp.fc2.base_layer.bias', 'vision_model.base_model.model.encoder.layers.8.mlp.fc2.base_layer.weight', 'vision_model.base_model.model.encoder.layers.8.mlp.fc2.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.8.mlp.fc2.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.8.norm1.weight', 'vision_model.base_model.model.encoder.layers.8.norm2.weight', 'vision_model.base_model.model.encoder.layers.9.attn.k_norm.weight', 'vision_model.base_model.model.encoder.layers.9.attn.proj.base_layer.bias', 'vision_model.base_model.model.encoder.layers.9.attn.proj.base_layer.weight', 'vision_model.base_model.model.encoder.layers.9.attn.proj.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.9.attn.proj.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.9.attn.q_norm.weight', 'vision_model.base_model.model.encoder.layers.9.attn.qkv.base_layer.weight', 'vision_model.base_model.model.encoder.layers.9.attn.qkv.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.9.attn.qkv.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.9.ls1', 'vision_model.base_model.model.encoder.layers.9.ls2', 'vision_model.base_model.model.encoder.layers.9.mlp.fc1.base_layer.bias', 'vision_model.base_model.model.encoder.layers.9.mlp.fc1.base_layer.weight', 'vision_model.base_model.model.encoder.layers.9.mlp.fc1.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.9.mlp.fc1.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.9.mlp.fc2.base_layer.bias', 'vision_model.base_model.model.encoder.layers.9.mlp.fc2.base_layer.weight', 'vision_model.base_model.model.encoder.layers.9.mlp.fc2.lora_A.default.weight', 'vision_model.base_model.model.encoder.layers.9.mlp.fc2.lora_B.default.weight', 'vision_model.base_model.model.encoder.layers.9.norm1.weight', 'vision_model.base_model.model.encoder.layers.9.norm2.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2024-06-24 17:24:18 | INFO | model_worker | Register to controller
2024-06-24 17:24:19 | ERROR | stderr | INFO:     Started server process [179251]
2024-06-24 17:24:19 | ERROR | stderr | INFO:     Waiting for application startup.
2024-06-24 17:24:19 | ERROR | stderr | INFO:     Application startup complete.
2024-06-24 17:24:19 | ERROR | stderr | INFO:     Uvicorn running on http://0.0.0.0:10061 (Press CTRL+C to quit)
2024-06-24 17:24:33 | INFO | model_worker | Send heart beat. Models: ['chemvlm']. Semaphore: None. global_counter: 0
2024-06-24 17:24:48 | INFO | model_worker | Send heart beat. Models: ['chemvlm']. Semaphore: None. global_counter: 0
2024-06-24 17:25:04 | INFO | model_worker | Send heart beat. Models: ['chemvlm']. Semaphore: None. global_counter: 0
2024-06-24 17:25:19 | INFO | model_worker | Send heart beat. Models: ['chemvlm']. Semaphore: None. global_counter: 0
2024-06-24 17:25:34 | INFO | model_worker | Send heart beat. Models: ['chemvlm']. Semaphore: None. global_counter: 0
2024-06-24 17:25:49 | INFO | model_worker | Send heart beat. Models: ['chemvlm']. Semaphore: None. global_counter: 0
2024-06-24 17:26:04 | INFO | model_worker | Send heart beat. Models: ['chemvlm']. Semaphore: None. global_counter: 0
2024-06-24 17:26:19 | INFO | model_worker | Send heart beat. Models: ['chemvlm']. Semaphore: None. global_counter: 0
2024-06-24 17:26:34 | INFO | model_worker | Send heart beat. Models: ['chemvlm']. Semaphore: None. global_counter: 0
2024-06-24 17:26:49 | INFO | model_worker | Send heart beat. Models: ['chemvlm']. Semaphore: None. global_counter: 0
2024-06-24 17:27:04 | INFO | model_worker | Send heart beat. Models: ['chemvlm']. Semaphore: None. global_counter: 0
2024-06-24 17:27:19 | INFO | model_worker | Send heart beat. Models: ['chemvlm']. Semaphore: None. global_counter: 0
2024-06-24 17:27:34 | INFO | model_worker | Send heart beat. Models: ['chemvlm']. Semaphore: None. global_counter: 0
2024-06-24 17:27:49 | INFO | model_worker | Send heart beat. Models: ['chemvlm']. Semaphore: None. global_counter: 0
2024-06-24 17:28:04 | INFO | model_worker | Send heart beat. Models: ['chemvlm']. Semaphore: None. global_counter: 0
2024-06-24 17:28:19 | INFO | model_worker | Send heart beat. Models: ['chemvlm']. Semaphore: None. global_counter: 0
2024-06-24 17:28:34 | INFO | model_worker | Send heart beat. Models: ['chemvlm']. Semaphore: None. global_counter: 0
2024-06-24 17:28:49 | INFO | model_worker | Send heart beat. Models: ['chemvlm']. Semaphore: None. global_counter: 0
2024-06-24 17:29:04 | INFO | model_worker | Send heart beat. Models: ['chemvlm']. Semaphore: None. global_counter: 0
2024-06-24 17:29:19 | INFO | model_worker | Send heart beat. Models: ['chemvlm']. Semaphore: None. global_counter: 0
2024-06-24 17:29:34 | INFO | model_worker | Send heart beat. Models: ['chemvlm']. Semaphore: None. global_counter: 0
2024-06-24 17:29:49 | INFO | model_worker | Send heart beat. Models: ['chemvlm']. Semaphore: None. global_counter: 0
2024-06-24 17:30:04 | INFO | model_worker | Send heart beat. Models: ['chemvlm']. Semaphore: None. global_counter: 0
2024-06-24 17:30:19 | INFO | model_worker | Send heart beat. Models: ['chemvlm']. Semaphore: None. global_counter: 0
2024-06-24 17:30:34 | INFO | model_worker | Send heart beat. Models: ['chemvlm']. Semaphore: None. global_counter: 0
2024-06-24 17:30:43 | INFO | stdout | INFO:     10.140.24.69:43444 - "POST /worker_get_status HTTP/1.1" 200 OK
2024-06-24 17:30:49 | INFO | model_worker | Send heart beat. Models: ['chemvlm']. Semaphore: None. global_counter: 0
2024-06-24 17:30:50 | INFO | model_worker | Send heart beat. Models: ['chemvlm']. Semaphore: Semaphore(value=4, locked=False). global_counter: 1
2024-06-24 17:30:50 | INFO | stdout | INFO:     10.140.24.69:43486 - "POST /worker_generate_stream HTTP/1.1" 200 OK
2024-06-24 17:30:50 | INFO | model_worker | max_input_tiles: 12
2024-06-24 17:30:50 | INFO | model_worker | dynamic_image_size: False
2024-06-24 17:30:50 | INFO | model_worker | use_thumbnail: False
2024-06-24 17:30:51 | INFO | model_worker | Resize images to 448x448
2024-06-24 17:30:51 | INFO | model_worker | Split images to torch.Size([1, 3, 448, 448])
2024-06-24 17:30:51 | INFO | model_worker | A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: <img><image></img>
What does this image mean ASSISTANT:
2024-06-24 17:30:51 | INFO | model_worker | num_image_tokens: 256
2024-06-24 17:30:51 | INFO | model_worker | max_new_tokens: 1024
2024-06-24 17:30:58 | ERROR | stderr | Exception in thread Thread-3:
2024-06-24 17:30:58 | ERROR | stderr | Traceback (most recent call last):
2024-06-24 17:30:58 | ERROR | stderr |   File "/mnt/petrelfs/zhangdi1/miniforge3/envs/internvl/lib/python3.9/threading.py", line 980, in _bootstrap_inner
2024-06-24 17:30:58 | ERROR | stderr |     self.run()
2024-06-24 17:30:58 | ERROR | stderr |   File "/mnt/petrelfs/zhangdi1/miniforge3/envs/internvl/lib/python3.9/threading.py", line 917, in run
2024-06-24 17:30:58 | ERROR | stderr |     self._target(*self._args, **self._kwargs)
2024-06-24 17:30:58 | ERROR | stderr |   File "/mnt/petrelfs/zhangdi1/miniforge3/envs/internvl/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
2024-06-24 17:30:58 | ERROR | stderr |     return func(*args, **kwargs)
2024-06-24 17:30:58 | ERROR | stderr |   File "/mnt/petrelfs/zhangdi1/lijunxian/InternVL/internvl_chat/internvl/model/internvl_chat/modeling_internvl_chat.py", line 383, in generate
2024-06-24 17:30:58 | ERROR | stderr |     outputs = self.language_model.generate(
2024-06-24 17:30:58 | ERROR | stderr |   File "/mnt/petrelfs/zhangdi1/miniforge3/envs/internvl/lib/python3.9/site-packages/peft/peft_model.py", line 1491, in generate
2024-06-24 17:30:58 | ERROR | stderr |     outputs = self.base_model.generate(*args, **kwargs)
2024-06-24 17:30:58 | ERROR | stderr |   File "/mnt/petrelfs/zhangdi1/miniforge3/envs/internvl/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
2024-06-24 17:30:58 | ERROR | stderr |     return func(*args, **kwargs)
2024-06-24 17:30:58 | ERROR | stderr |   File "/mnt/petrelfs/zhangdi1/miniforge3/envs/internvl/lib/python3.9/site-packages/transformers/generation/utils.py", line 1525, in generate
2024-06-24 17:30:58 | ERROR | stderr |     return self.sample(
2024-06-24 17:30:58 | ERROR | stderr |   File "/mnt/petrelfs/zhangdi1/miniforge3/envs/internvl/lib/python3.9/site-packages/transformers/generation/utils.py", line 2658, in sample
2024-06-24 17:30:59 | ERROR | stderr |     next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)
2024-06-24 17:30:59 | ERROR | stderr | RuntimeError: probability tensor contains either `inf`, `nan` or element < 0
2024-06-24 17:31:04 | INFO | model_worker | Send heart beat. Models: ['chemvlm']. Semaphore: Semaphore(value=4, locked=False). global_counter: 1
2024-06-24 17:31:06 | INFO | stdout | Caught Unknown Error
2024-06-24 17:31:06 | INFO | model_worker | Send heart beat. Models: ['chemvlm']. Semaphore: Semaphore(value=5, locked=False). global_counter: 1
2024-06-24 17:31:19 | INFO | model_worker | Send heart beat. Models: ['chemvlm']. Semaphore: Semaphore(value=5, locked=False). global_counter: 1
2024-06-24 17:31:21 | INFO | stdout | INFO:     10.140.24.69:43672 - "POST /worker_get_status HTTP/1.1" 200 OK
2024-06-24 17:31:30 | INFO | model_worker | Send heart beat. Models: ['chemvlm']. Semaphore: Semaphore(value=4, locked=False). global_counter: 2
2024-06-24 17:31:30 | INFO | stdout | INFO:     10.140.24.69:43726 - "POST /worker_generate_stream HTTP/1.1" 200 OK
2024-06-24 17:31:30 | INFO | model_worker | max_input_tiles: 12
2024-06-24 17:31:30 | INFO | model_worker | dynamic_image_size: False
2024-06-24 17:31:30 | INFO | model_worker | use_thumbnail: False
2024-06-24 17:31:30 | INFO | model_worker | Resize images to 448x448
2024-06-24 17:31:30 | INFO | model_worker | Split images to torch.Size([1, 3, 448, 448])
2024-06-24 17:31:30 | INFO | model_worker | A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: <img><image></img>
Describe this image in detail ASSISTANT:
2024-06-24 17:31:30 | INFO | model_worker | num_image_tokens: 256
2024-06-24 17:31:30 | INFO | model_worker | max_new_tokens: 1024
2024-06-24 17:31:30 | ERROR | stderr | Exception in thread Thread-4:
2024-06-24 17:31:30 | ERROR | stderr | Traceback (most recent call last):
2024-06-24 17:31:30 | ERROR | stderr |   File "/mnt/petrelfs/zhangdi1/miniforge3/envs/internvl/lib/python3.9/threading.py", line 980, in _bootstrap_inner
2024-06-24 17:31:30 | ERROR | stderr |     self.run()
2024-06-24 17:31:30 | ERROR | stderr |   File "/mnt/petrelfs/zhangdi1/miniforge3/envs/internvl/lib/python3.9/threading.py", line 917, in run
2024-06-24 17:31:30 | ERROR | stderr |     self._target(*self._args, **self._kwargs)
2024-06-24 17:31:30 | ERROR | stderr |   File "/mnt/petrelfs/zhangdi1/miniforge3/envs/internvl/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
2024-06-24 17:31:30 | ERROR | stderr |     return func(*args, **kwargs)
2024-06-24 17:31:30 | ERROR | stderr |   File "/mnt/petrelfs/zhangdi1/lijunxian/InternVL/internvl_chat/internvl/model/internvl_chat/modeling_internvl_chat.py", line 383, in generate
2024-06-24 17:31:30 | ERROR | stderr |     outputs = self.language_model.generate(
2024-06-24 17:31:30 | ERROR | stderr |   File "/mnt/petrelfs/zhangdi1/miniforge3/envs/internvl/lib/python3.9/site-packages/peft/peft_model.py", line 1491, in generate
2024-06-24 17:31:30 | ERROR | stderr |     outputs = self.base_model.generate(*args, **kwargs)
2024-06-24 17:31:30 | ERROR | stderr |   File "/mnt/petrelfs/zhangdi1/miniforge3/envs/internvl/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
2024-06-24 17:31:30 | ERROR | stderr |     return func(*args, **kwargs)
2024-06-24 17:31:30 | ERROR | stderr |   File "/mnt/petrelfs/zhangdi1/miniforge3/envs/internvl/lib/python3.9/site-packages/transformers/generation/utils.py", line 1525, in generate
2024-06-24 17:31:30 | ERROR | stderr |     return self.sample(
2024-06-24 17:31:30 | ERROR | stderr |   File "/mnt/petrelfs/zhangdi1/miniforge3/envs/internvl/lib/python3.9/site-packages/transformers/generation/utils.py", line 2658, in sample
2024-06-24 17:31:30 | ERROR | stderr |     next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)
2024-06-24 17:31:30 | ERROR | stderr | RuntimeError: probability tensor contains either `inf`, `nan` or element < 0
2024-06-24 17:31:34 | INFO | model_worker | Send heart beat. Models: ['chemvlm']. Semaphore: Semaphore(value=4, locked=False). global_counter: 2
2024-06-24 17:31:45 | INFO | stdout | Caught Unknown Error
2024-06-24 17:31:45 | INFO | model_worker | Send heart beat. Models: ['chemvlm']. Semaphore: Semaphore(value=5, locked=False). global_counter: 2
2024-06-24 17:31:49 | INFO | model_worker | Send heart beat. Models: ['chemvlm']. Semaphore: Semaphore(value=5, locked=False). global_counter: 2
2024-06-24 17:32:04 | INFO | model_worker | Send heart beat. Models: ['chemvlm']. Semaphore: Semaphore(value=5, locked=False). global_counter: 2
2024-06-24 17:32:19 | INFO | model_worker | Send heart beat. Models: ['chemvlm']. Semaphore: Semaphore(value=5, locked=False). global_counter: 2
2024-06-24 17:32:34 | INFO | model_worker | Send heart beat. Models: ['chemvlm']. Semaphore: Semaphore(value=5, locked=False). global_counter: 2
2024-06-24 17:32:49 | INFO | model_worker | Send heart beat. Models: ['chemvlm']. Semaphore: Semaphore(value=5, locked=False). global_counter: 2
2024-06-24 17:33:04 | INFO | model_worker | Send heart beat. Models: ['chemvlm']. Semaphore: Semaphore(value=5, locked=False). global_counter: 2
2024-06-24 17:33:20 | INFO | model_worker | Send heart beat. Models: ['chemvlm']. Semaphore: Semaphore(value=5, locked=False). global_counter: 2
2024-06-24 17:33:35 | INFO | model_worker | Send heart beat. Models: ['chemvlm']. Semaphore: Semaphore(value=5, locked=False). global_counter: 2
2024-06-24 17:33:50 | INFO | model_worker | Send heart beat. Models: ['chemvlm']. Semaphore: Semaphore(value=5, locked=False). global_counter: 2
2024-06-24 17:34:05 | INFO | model_worker | Send heart beat. Models: ['chemvlm']. Semaphore: Semaphore(value=5, locked=False). global_counter: 2
2024-06-24 17:34:20 | INFO | model_worker | Send heart beat. Models: ['chemvlm']. Semaphore: Semaphore(value=5, locked=False). global_counter: 2
2024-06-24 17:34:35 | INFO | model_worker | Send heart beat. Models: ['chemvlm']. Semaphore: Semaphore(value=5, locked=False). global_counter: 2
2024-06-24 17:34:50 | INFO | model_worker | Send heart beat. Models: ['chemvlm']. Semaphore: Semaphore(value=5, locked=False). global_counter: 2
2024-06-24 17:35:05 | INFO | model_worker | Send heart beat. Models: ['chemvlm']. Semaphore: Semaphore(value=5, locked=False). global_counter: 2
slurmstepd: error: *** JOB 3586729 ON SH-IDC1-10-140-24-69 CANCELLED AT 2024-06-24T17:35:16 ***
2024-06-24 17:35:16 | ERROR | stderr | INFO:     Shutting down
